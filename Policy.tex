\chapter{Studio e sperimentazione della gestione di policy in Teraflow}
\label{cap:policy}
%Lo scopo della tesi è quello di stabilire una connessione tra due end points nella rete che rispetti determinate caratteristiche
%a tempo di esecuzione utilizzando il controller SDN Teraflow.
In questo Capitolo verranno esposti i vari esperimenti eseguiti utilizzando il controller SDN TeraFlow per quanto riguarda la gestione degli intenti.
Nella fase iniziale si è seguito l'Hackfest 3 \cite{hackfest} dove è stato fornito un ambiente pre-configurato per testare TeraFlow SDN. 
Il lavoro è iniziato con l'installazione della macchina virtuale \cite{VM} creata appositamente per il congresso.
%Gli esperimenti sono stati svolti all'interno di un'infrastruttura virtualizzata configurata per eseguire sia TeraFlow che Mininet, permettendo di simulare e analizzare vari scenari di rete.
%La macchina virtuale (VM) utilizzata inizialmente per gli esperimenti è stata quella utilizzata durante l'Hackfest 3 \cite{hackfest}, dove è stato fornito un ambiente pre-configurato per testare TeraFlow SDN. 
\\Di seguito vengono riportate le specifiche della VM utilizzata:
\begin{itemize}
    \item IP Address: 10.0.2.X/24 (DHCP)
    \item Gateway: 10.0.2.1
    \item DNS: 8.8.8.8, 8.8.4.4
    \item Creata con VirtualBox 6.1 ma compatibile con versioni successive
    \item Requisiti minimi: 4 vCPU, 6 GB di RAM, 50 GB di spazio sul disco, Virtual Disk Image (VDI)
    \item Connessione di rete: NAT Network con porte 22 per SSH e 80 per HTTP esposte
    \item Sistema operativo senza interfaccia grafica per ridurre il consumo di risorse.
\end{itemize}
La VM ha al suo interno preinstallati MicroK8s con le componenti richieste e Mininet in formato docker.
La versione di TeraFlow utilizzata è la 2.1 con adattamenti specifici per l'Hackfest.
\\Successivamente è stata installata la VM con la versione aggiornata di TeraFlow (3.0) a causa di un'incompleta implementazione della componente di Policy che non era in grado di gestire e riconoscere le KPI.
Le caratteristiche richieste sono sostanzialmente simili, con un aumento di memoria RAM a 8 GB e dello spazio su disco a 60 GB.
Il sistema operativo utilizzato è Ubuntu Server 22.04.4 LTS, compatibile anche con la versione successiva 22.04.6 LTS. 
Inoltre è richiesta l'installazione di MicroK8 v1.24.17 cone le componenti necessarie, Docker e la versione 3.9.18 di Python.
\\I vari esperimenti si sono svolti seguendo il seguente schema:
attraverso il Service Level è stato introdotto un servizio nella rete sotto forma di intento per stabilire la connessione e il percorso. 
Successivamente, tramite il Management Level, è stata inserita una politica basata sugli eventi che consente di associare un Service Level Agreement (SLA) a un servizio specifico.
Questo SLA include condizioni che devono essere monitorate e rispettate durante l'esecuzione del servizio.
%modifica file (NodePort..)
%modifica per abilitare più porte 
%end-to-end
%tutte queste info sono persistite in un database logicamente centralizzato, fisicamente distribuito e scalabile, dato dalla componente di Context
\\Per iniziare il lavoro si è partiti da una demo già preesistente, apportando in seguito le modifiche necessarie.
\section{Strumenti per la sperimentazione}
INTRO
\\li descrivo perche verranno usati switch p4 emulati in mininet
\subsection{Mininet}
\label{ch:Mininet}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{mininet.png}
    \caption{Rete mininet}
    %https://telcomaglobal.com/p/what-is-mininet  fonte foto mininet
    \label{fig:mininet}
\end{figure}
Mininet \cite{mininet} è un sistema open source di orchestrazione per l'emulazione di reti su un unico ambiente Linux che permette di emulare un'intera rete su un singolo computer.
E' ampiamente utilizzato in ambiti di ricerca e sviluppo per creare e testare reti virtuali in modo realistico.
A differenza di altri emulatori che utilizzano macchine virtuali per ogni dispositivo, si distingue per la sua capacità di avviare rapidamente reti virtuali complesse e di eseguire test su scenari vari e personalizzati.
Consente anche di configurare l'inoltro dei pacchetti per testare diverse funzionalità, facilitando la condivisione e la replica del codice.
%Rispetto ad altri emulatori presenti in circolazione che emulano ogni dispositivo su una macchina virtuale, Mininet offre una serie di vantaggi. 
%Inanzitutto, permette l'avviamento rapido di una rete, la capacità di eseguire test e programmi con tipologie ampie e personalizzate. 
%Inoltre consente di personalizzare l'inoltro dei pacchetti per testare diverse funzionalità con la possibilità di condividere e replicare il codice.
\\Mininet offre delle API e un interprete Python che consentono di definire e gestire facilmente delle topologie di rete.
%Ciò è possibile anche tramite interfaccia a riga di comando (CLI). In entambi i casi possono essere sia predefinite che personalizzate con la possibilità di 
%aggiungere e rimuovere switch, router, host, controller e link, tutti eseguiti su un unico computer.
%Mininet utilizza infatti una virtualizzazione leggera per creare nodi di rete ognuno con la propria pila di rete in modo tale che possano comunicare con gli 
%altri nodi come farebbero in una rete fisica. 
È inoltre possibile utilizzare un'interfaccia a riga di comando (CLI) per la stessa funzione.
In entrambi i casi, si possono configurare topologie predefinite o personalizzate, aggiungendo e rimuovendo switch, router, host, controller e link, tutti eseguiti su un unico computer.
\\Mininet è in grado di gestire un insieme di terminali di rete (host), 
%switch, router e collegamenti all'interno di un singolo ambiente Linux,
utilizzando la virtualizzazione leggera attraverso tecnologie implementate nel kernel Linux, come i network namespaces.
Questi permettono di creare istanze separate di interfacce di rete, tabelle di routing e tabelle ARP, che operano in modo indipendente \cite{tesiMininet}. 
Questo approccio consente di avviare numerosi host e switch (fino a 4096) su un singolo kernel del sistema operativo, simulando una rete completa su un'unica macchina \cite{MininetOv}.
Ciò consente di testare nuove applicazioni, protocolli e algoritmi in un ambiente controllato e modificabile prima di implementarli su reti reali.
\\Mininet mette a disposizione tre livelli differenti di API \cite{introMin}:
\begin{itemize}
\item \textbf{Low-level}: consiste nelle classi dei nodi e dei link istanziati individualmente e usati per creare una rete.
\item \textbf{Mid-level}: aggiunge un containter per nodi e link, l'oggetto Mininet, e fornisce metodi per la configurazione di rete.
\item \textbf{High-level}: aggiunge l'astrazione della topologia di rete, la classe Topo. Offre la possibilità di creare modelli di topologia riusabili passandoli al comando mn da linea di comando.
\end{itemize}
Si possono configurare i link come up o down e inserire metriche specifiche 
come quelle di banda, ritardo, perdita o massima lunghezza della coda di recezione per rendere la rete più realistica e adatta a esperimenti di test.
\\Gli host su Mininet condividono il filesystem root del server sottostante. 
Ciò significa che non è necessario trasferire file tra gli host virtuali perché tutti accedono agli stessi file direttamente.
Tuttavia, questa condivisione del filesystem può creare problemi se un programma ha bisogno di file di configurazione specifici per ogni host. 
In tal caso, è necessario creare un file di configurazione separato per ogni host e specificare quale file utilizzare quando si avvia il programma.
Un'altra limitazione riguarda la condivisione delle risorse del sistema su cui è in esecuzione che dovranno essere bilanciate tra tutti gli host della rete.
%Inoltre ci possono essere collisioni tra file se si prova a creare lo stesso file nella stessa directory di più hosts.
%Mininet mette a disposizione una GUI (miniedit) utile per visualizzare lo stato della rete durante gli esperimenti svolti.
\\Mininet è stato progettato per essere facilmente integrabile con altri software e sistemi di rete.
Consente anche di connettere un controller SDN remoto, quindi esterno alla rete, agli switch, indipendentemente dal PC su cui è installato, in modo da fornire un ambiente adatto allo sviluppo e al test.


\subsubsection{Alcuni comandi fondamentali}
\textbf{Linea di comando}
\\Inanzitutto è fondamentale creare una topologia di rete con il seguente comando\cite{walkmin}:
\\\textit{\$ sudo mn}
\\Di default viene inizializzata la topologia minimale (--topo=minimal) che consiste in uno switch connesso a due host e un controller OpenFlow.
All'interno di Mininet si possono trovare altre topologie disponibili e visualizzabili con il comando \\\textit{\$sudo mn -h} \\che si possono specificare tramite l'opzione $--topo$.
%scrivi le varie topologie
%--topo single, 3 uno switch con 3 host
\\Per avviare la topologia esistono diverse opzioni da poter applicare.Ad esempio, l'opzione $--controller$ seguito dall'indirizzo IP specifica il controller al quale gli switch dovranno collegarsi al posto 
del predefinito offerto da Mininet.
%riguarda
\\Una volta creata la topologia per avere informazioni su di essa esistono diversi comandi:
%mettere output/immagini
\begin{itemize}
    \item \textit{ nodes}: per visualizzare i nodi presenti.
    \item \textit{ net}: per visualizzare i nodi e i link presenti.
    \item \textit{ dump}: per visualizzare tutte le informazioni di dump dei nodi.
    \item \textit{h1 ifconfig}: per visualizzare le interfacce del nodo h1.
\end{itemize}
Alcuni comandi per interagire con la rete e fare dei test minimali sono:
\begin{itemize}
    \item \textit{ h1 ping -c 1 h2 }: verifica il corretto funzionamento del percorso tra h1 e h2.
    \item \textit{ pingall}: esegue il ping tra tutti gli host connessi alla rete.
    \item \textit{ iperf}: esegue un test di banda tra 2 degli host della rete.
    \item \textit{xterm h1}: permette di avviare il terminale relativo al nodo h1.
    \item \textit{exit}: esce dalla rete.
\end{itemize}
Per manipolare le metriche relative ai link invece vengono messi a disposizione i seguenti comandi:
\begin{itemize}
    \item \textit{ link s1 h1 down}: disabilita un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{ link s1 h1 up}: attiva un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem loss 50\% }: aggiunge una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem delay 200ms}: aggiunge un ritardo di 200ms sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem loss 50\% }: elimina una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem delay 200ms}: elimina un ritardo di 200ms sulla porta eth2 dello switch s2.
\end{itemize} 
\textbf{API Python}
\\Le API Python di Mininet permettono di creare e gestire topologie di rete in modo più flessibile e programmabile. 
Di seguito esponiamo alcune classi e comandi della Mid-level API:
\begin{itemize}
    \item \textit{Mininet}: classe per creare e gestire la rete. Il costruttore prende in input diversi parametri la topologia, gli host, gli switch,i controller, i link e ritorna un oggetto di rete.
    \item \textit{addSwitch()}: aggiunge uno switch alla topologia.
    \item \textit{addHost()}: aggiunge un host alla topologia.
    \item \textit{addLink()}: aggiunge un link alla topologia. Si possono specificare paramentri come la banda espressa in Mbit (bw=10 ), il ritardo (delay='5ms'), massima dimensione della coda espressa in numero di pacchetti (max\_queue\_size=1000), la loss espressa in percentuale (loss=10)
    \item \textit{start}: avvia la rete
    \item \textit{stop}: esce dalla rete
    \item \textit{pingall}: esegue il ping tra tutti gli host connessi alla rete
    \item \textit{h1.cmd('comando da eseguire')}: esegue un comando su h1 da CLI e prende l'output
\end{itemize}
Con le API in Python si può anche estendere il comando \textit{mn} usando l'opzione \textit{--custom} per invocare la topologia ricreata nello script.
\\\textit{sudo mn --your\_script.py --topo your\_topo}

\subsection{P4}
Programming Protocol-indipendent Packet Processor (P4 \cite{p4}), è un linguaggio di programmazione flessibile
che permette di descrivere il comportamento degli elementi di rete, consentendo di personalizzare come i dispositivi elaborano i pacchetti.
%\\P4 è nato quindi con l'obiettivo di definire nuove astrazioni per programmare il piano dati e gestire l'inoltro senza dover andare incontro alle limitazioni riscontrate in precedenza.
\\I principali obiettivi sono:
\begin{itemize}
    \item \textbf{Indipendenza dal protocollo}: P4 non è vincolato a nessun protocollo specifico consentendo di definire nuovi protocolli o modificare quelli esistenti
    \item \textbf{Indipendenza dal target}: Il codice può essere compilato per funzionare su diversi dispositivi, sia hardware che software, rendendolo versatile
    \item \textbf{Riprogrammabilità}: Il comportamento del piano dati può essere aggiornato dinamicamente, consentendo di rispondere rapidamente ai cambiamenti delle esigenze di rete.
\end{itemize}
P4 è stato introdotto per superare le limitazioni di OpenFlow per poter fornire una soluzione più flessibile.
OpenFlow, pur separando il piando di controllo e il piano dati, si basa su un insieme fisso di funzionalità, definendo regole di elaborazione dei pacchetti attraverso l'astrazione di tabelle che mappano i campi degli header (indirizzi IP, MAC, porte...). 
Negli anni le specifiche dei pacchetti sono diventate sempre più complesse e dipendenti dalle singole aziende, che hanno iniziato a sviluppare l'hardware indipendentemente, rendendo necessari continui aggiornamenti per supportare nuove funzionalità\cite{p4art}.
D'altra parte, gli switch non aggiornati, o prodotti da aziende diverse, non riescono a supportare tutte le nuove caratteristiche.
Inoltre, OpenFLow non fornisce delle interfacce operative o amministrative standard, quindi rende complicato aggiungere supporto per nuovi protocolli.
Tutte queste problematiche hanno portato all'introduzione di dispositivi programmabili che utilizzano P4.
\\P4 consente di definire intestazioni e tabelle personalizzate e programmare esplicitamente il flusso di controllo dello switch, permettendo di adattarsi rapidamente ai cambiamenti e alle innovazioni.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{p4.png}
    \caption{Workflow di P4 sul piano dati \cite{p4Article}}
    \label{fig:p4}
\end{figure}
%Permette di definire la tabella, le azioni e the counters per poi essere applicate a elementi hardware o software switch allo stesso modo.
%P4 ha un interfaccia per analizzare pacchetti e match campi nell'header. In questo modo abbiamo un accoppiamento tra hardware e software.
\\Nella Figura \ref{fig:p4} è illustrato il Workflow del modello di P4.
\\Un programma P4 (P4 Program) definisce il comportamento del piano dati desiderato ed è suddiviso in varie sezioni \cite{p4Article}, ognuna delle quali descrive un aspetto specifico del trattamento dei pacchetti.
Il processo inizia con la dichiarazione degli header, dove vengono specificati i protocolli da analizzare o si possono definire nuovi protocolli per scopi di ricerca o sperimentazione.
Questi header contengono campi come indirizzi IP, numeri di porta e tutti i vari dati di protocollo.
\\Successivamente, il programma viene inviato al compilatore P4 (P4 Compiler) che genera due tipi di output. 
Il primo è un file eseguibile che specifica i formati degli header e le relative operazioni da implementare all'interno del dispositivo target (hardware come ASICs, FPGA..).
Il secondo è un file indipendente dal target che 
contiene le informazioni necessarie per far comunicare il piano di controllo e il piano dati tramite l'API P4Runtime.
\\P4Runtime permette al controller di connettersi ai dispositivi e interagire con la pipeline 
per poter inviare le configurazioni rilevanti nella relativa tabella. 
%Permette inoltre di definire il piano dati in modo dinamico collegandolo al piano di controllo. 
%Per il piano di controllo, P4Runtime protegge i dettagli hardware del piano dati ed è indipendente dalle funzionalità e dal protocollo supportati.
\\I dispositivi programmabili, ottenibili tramite software o hardware, sono definiti P4 target.
Essi hanno una pipeline per l'elaborazione dei pacchetti la cui struttura è specifica per il target ed è descritta da un determinato modello di architettura.
Il Parser all'interno dei dispositivi specifica come estrarre e interpretare i vari header dai pacchetti seguendo uno schema prestabilito.
\\La Pipeline di Elaborazione (Match-Action Pipeline) include tabelle e azioni che determinano come i pacchetti vengono processati
mentre
%guarda se mettere separato tabelle e azioni
i Controlli del flusso coordinano parser e pipeline per garantire il corretto funzionamento dell'intero processo.
Il Deparser ricompone i pacchetti con le eventuali modifiche agli header e successivamente li invia.
%E' lo schema del pipeline descritto al controller in modo da nascondere il tipo di dispositivi presenti nella rete.
P4 rappresenta un passo significativo verso reti più flessibili e programmabili, consentendo agli sviluppatori di adattarsi rapidamente ai cambiamenti dei requisiti di rete.
Si propone come una soluzione innovativa e versatile per superare le limitazioni degli attuali protocolli e dispositivi di rete fornendo un linguaggio dinamico e indipendente dall'hardware.

\section{Sperimentazione}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{componenti.png}
    \caption{Interazione tra alcune componenti in TeraFlow}
    \label{fig:componenti}
\end{figure}
\textbf{Servizio end-to-end}
\\Come già detto in precedenza il Device Level sfrutta una South-Bound Interface (SBI) per interagire con i device tramite l'API P4Runtime. 
Inizialmente, il codice p4 compilato, ossia i vari artefatti, viene copiato nel pod SBI per poter inserire le giuste configurazioni nelle tabelle dei dispositivi.
Il passo successivo è registrare i dispositivi e i link al controller SDN per permettere una corretta comunicazione tra di essi.
A questo punto, siamo in grado di richiedere una connessione tra due end points specificando solamente i dispositivi finali tramite un servizio.
\\La creazione del servizio è realizzata in due passi. 
Inizialmente, tramite la funzione \textit{CreateService}, si crea una servizio di connettività vuoto nel quale viene specificato solo il tipo associato per poi ritornare l'identificativo a cui è stato correlato.
Successivamente viene aggiornato il servizio popolando i campi richiesti come endpoints, vincoli e configurazioni di servizio. Di questo si occupa la funzione \textit{UpdateService} \cite{D32}.
La componente di Service a questo punto si rivolge prima alla componente di Context per recuperare la versione più aggiornata del servizio e settare lo stato a Planned (pianificato), poi alla PathComp per calcolare un percorso.
\\Per eseguire questa operazione la PathComp utilizza delle informazioni della rete che risiedono nel database logicamente centralizzato della componente di Context.
La PathComp è in grado di soddisfare anche richieste di servizio che attraversano più livelli, in questo caso, la risposta includerà uno o più sottoservizi con le relative sottoconnessioni che li supportano.
\\Una volta ricevute le connessioni ed eventuali sottoservizi la componente Service istanzia un \textit{Task Scheduler} con le relative funzioni, %che li correla e li schedula.
quest'ultimo è responsabile dell'esecuzione delle attività di installazione e smantellamento dei servizi e collegamenti nell'ordine appropriato.
Infine viene eseguito il metodo \textit{Execute} del \textit{Task Scheduler} per realizzare tutte le operazioni di configurazione richieste per i dispositivi lungo il percorso tramite l'SBI; inoltre viene modificato lo stato del servizio.
Al termine del processo, la componente Context viene aggiornata con le nuove informazioni e l’identificatore del servizio viene restituito all’entità chiamante.
\\Per mantenere questo processo agnostico rispetto ai dettagli della tecnologia, la componente di Service 
sfrutta una definizione minima permettendo agli utenti di esprimere cosa vogliono connettere, lasciando che sia il sistema sottostante a decidere come realizzare la connessione reale.
La componente traduce automaticamente questa definizione minimale del servizio in modelli di configurazioni astratte dei dispositivi. 
Queste vengono a loro volta tradotte in regole P4 dal driver del dispositivo P4 della SBI.
\\\textbf{Vincoli e configurazioni di servizio}
\\È possibile richiedere azioni supplementari, come l'aggiunta di vincoli o configurazioni di servizio specifiche. 
%In questo modo il componente Policy offre SLA basati sugli eventi per servizi di connettività end-to-end tramite pipeline P4
Queste vengono specificate inizialmente alla creazione di un servizio e devono essere rispettate finchè quest'ultimo non verrà eliminato. 
Ciò semplifica la gestione dei servizi, in quanto la dichiarazione di queste informazioni aggiuntive può sostituire l'associazione di una politica.
\\Nella demo presa in considerazione i vincoli non erano specificati ma sono stati aggiunti per quanto riguarda la latenza e la capacità del percorso \ref{fig:constraints}.
Questa aggiunta non ha prodotto cambiamenti nella definizione del percorso poiché le funzioni relative all'effettivo funzionamento di queste funzionalità non 
sono ancora state implementate nel controller ma sono solo pianificate per release future.
Altri esempi di vincoli possono essere rappresentati dalla posizione di un dispositivo, che può essere sia terminale che non, dal tempo o dal numero di passi massimo di un determinato percorso.

%spiegare cosa sono i constraits, guarda i deliverable
%dire che non sono implementati
%run-time-->loss+latenza
\textbf{Politica}
\\Una parte fondamentale nei sistemi moderni è la gestione a run-time del servizio stabilito\cite{demo}.
A tale scopo si sfrutta la componente di Monitoring che permette di associare il monitoraggio delle metriche nel proprio database
con condizioni che devono essere rispettate.
Quando questi requisiti non vengono soddisfatti, la componente di Monitoring solleva un allarme che fa scattare l'azione prestabilita.
\\Nella sperimentazione abbiamo introdotto una politica per il servizio stabilito con tre condizioni: una per la latenza (che non deve superare i 100ms), una per la loss (che non deve superare il 5\%) 
e infine una per la capacità\ref{fig:policy}.
Queste tre condizioni sono legate tra loro tramite un "OR", quindi appena una di esse non è più rispettata viene invocata l'azione che in questo caso consiste nel ricalcolo del percorso.
%MODIFICA O LA FOTO O LA DESCRIZIONE

\section{Demo}
RISCRIVERE DESCRIVENDO SOLO LA DEMO DELL'HACKFEST3 (descrivi modifica al file per eseguire la componente di policy)
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{policy sw2.png}
    \caption{Servizio 4 sw iniziale}
    \label{fig:sw4}
\end{figure}

\textit{FIGURA POLICY SOLO LATENZA}
\\descrivi il comando mininet usato per modificare la latenza
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{policy con sw3.png}
    \caption{Servizio 4 sw cambio percorso}
    \label{fig:sw4 dopo}
\end{figure}


Per verificare il comportamento del controller e assicurarsi che le condizioni di latenza, di loss e di capacità vengano rispettate, 
non avendo a disposizione una rete reale, abbiamo utilizzato una topologia di rete su Mininet di switch P4 basati su bmv2\cite{bmv2}.
La topologia iniziale era composta da 4 switch e due possibili percorsi ed è stata poi ampliata a 8 switch con 5 percorsi per renderla più complessa e poter fare una sperimentazione più realistica.
L'ultima topologia utilizzata è stata Abilene \cite{abilene}; una rete di trasporto creata da Internet2.
%mettere foto topologia abilene
\\I dispositivi delle varie topologie sono stati connessi alla componente SBI, come per i dispositivi reali, in modo tale che potessero comunicare con il controller.
Successivamente per dimostrare la politica basata sul servizio e il ricalcolo delle configurazioni, abbiamo aggiunto artificialmente delle condizioni di ritardo e packet loss all’interno di uno switch presente nel percorso. 
Ad esempio, abbiamo utilizzato il comando \textit{switch2 tc qdisc add dev switch2-eth2 root netem delay 200ms} per aggiungere un ritardo di 200ms oppure il comando \textit{switch2 tc qdisc add dev switch2-eth2 root netem loss 10\%}
%aggiungere il comando per la capacità
 per simulare una perdita di pacchetti del 10\%. Inoltre, abbiamo disattivato un link tra due switch con il comando \textit{link switch6 switch7 down}.
\\Questi cambiamenti sono stati monitorati grazie a un probe in Python, una funzionalità che rileva lo stato di integrità delle istanze dell'applicazione.
Inizialmente, si avvia l'agent che, ascoltando gli eventi della componente di Context, crea tre Kpi Id differenti per loss, latenza e capacità ogni volta che viene creato un servizio \ref{fig:agent}.
\begin{figure}[h]
    \centering
   \includegraphics[width=0.5\textwidth]{create new event ag.png}
    \caption{Creazione delle due KPI id}
    \label{fig:agent}
\end{figure}
Successivamente si attiva un secondo script che riesce a monitorare le metriche di loss e latenza attraverso il comando ping del terminale e la capacità tramite iperf per poi inviarle attraverso una socket all'agent.
Prima di avviare lo script è necessario attivare il server iperf sul dispositivo finale del servizio.
\\L'agent, ottenuti i relativi valori, li inoltra alla componente di monitoring tramite una KPI composta dal valore, il kpi id corretto e un timestamp.
\\La componente, ricevute le metriche, riconosce che è avvenuto un cambiamento nella rete e solleva un allarme per una potenziale violazione della politica che sarà poi validata dalla componente di Policy.
Questo evento causa l'esecuzione dell'azione predefinita nella politica, ovvero l'aggiornamento del percorso.
Quando il nuovo tragittto è calcolato dalla Path Comp, la componente di Service compila una lista di configurazioni di dispositivi per validare gli aggiornamenti 
seguita da un'altra lista per la cancellazione delle vecchie configurazioni. Queste istruzioni sono tradotte in regole P4 dalla componente SBI prima di essere imposte al piano dati 
tramite il P4Runtime.
\\In tutte e tre le tipologie di alterazione della rete, si è riscontrato un cambiamento del percorso volto a rispettare le condizioni desiderate. 
%Inoltre, si è notato che una volta rimosso il ritardo, la perdita di pacchetti, o riattivato un link, il controller percepisce questo cambiamento e modifica automaticamente il percorso precedente, poiché è memorizzato nel database della Path Comp come percorso preferito per le sue caratteristiche.
\section{Esperimento 2}
8 sw script presi da ofc22
\\politica con latenza e pkt loss
\\descrizione comando mininet per modificare pkt loss
\\cambio tolopogia, aggiunta link, esteso probe, aggiunta porte per mininet
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{service sw8.png}
    \caption{Servizio 8 sw iniziale}
    \label{fig:sw8}
\end{figure}

\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{policy 5 100.png}
    \caption{Politica latenza e pkt loss}
    \label{fig:policy}
\end{figure}

\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{servizion constraints.png}
    \caption{Servizio 8 sw cambio percorso}
    \label{fig:sw8 dopo}
\end{figure}

\section{Esperimento 3}
Cambio topologia, nuovo script mininet (preso da internet con cambio bw)
\\ aggiunte porte,
\\aggiunta metrica di capacità alla politica
\\descrizione comando mininet per modificare capacità
\\aggiunto iperf al probe
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{service abilene (10).png}
    \caption{abilene iniziale}
    \label{fig:abilene}
\end{figure}

\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{policy cap.png}
    \caption{Servizio 8 sw cambio percorso}
    \label{fig:policy cap}
\end{figure}

\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{service abilene (11).png}
    \caption{Servizio 8 sw cambio percorso}
    \label{fig:abilene dopo}
\end{figure}

\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{service 3 10 11.png}
    \caption{abilene cambio }
    \label{fig:abilene post}
\end{figure}


