\chapter{Studio e sperimentazione della gestione di policy in Teraflow}
\label{cap:policy}
%Lo scopo della tesi è quello di stabilire una connessione tra due end points nella rete che rispetti determinate caratteristiche
%a tempo di esecuzione utilizzando il controller SDN Teraflow.
In questo Capitolo verranno esposti i vari esperimenti eseguiti utilizzando il controller SDN TeraFlow per quanto riguarda la gestione degli intenti.
Nella fase iniziale è stato replicato l'esperimento seguito nell'Hackfest 3 \cite{hackfest} organizzato da ETSI presso la sede del CTTC a Barcellona.
L'Hackfest è un evento con scopi formativi e di sperimentazione in cui viene fornito un ambiente di test pre-configurato, in questa edizione 
per il controller TeraFlow, con l'obiettivo di eseguire esperimenti e dimostrazioni pratiche delle sue funzionalità. 
\newline Il lavoro è iniziato con l'installazione della macchina virtuale \cite{VM} su VirtualBox 7.0.
%Gli esperimenti sono stati svolti all'interno di un'infrastruttura virtualizzata configurata per eseguire sia TeraFlow che Mininet, permettendo di simulare e analizzare vari scenari di rete.
%La macchina virtuale (VM) utilizzata inizialmente per gli esperimenti è stata quella utilizzata durante l'Hackfest 3 \cite{hackfest}, dove è stato fornito un ambiente pre-configurato per testare TeraFlow SDN. 
Di seguito vengono riportate le specifiche della VM utilizzata:
\begin{itemize}
    \item IP Address: 10.0.2.X/24 (DHCP)
    \item Gateway: 10.0.2.1
    \item DNS: 8.8.8.8, 8.8.4.4
    \item Requisiti minimi: 4 vCPU, 8 GB di RAM, 60 GB di spazio sul disco, Virtual Disk Image (VDI)
    \item Connessione di rete: NAT Network con porte esposte 22 per SSH e 80 per HTTP
    \item Sistema operativo Ubuntu Server 22.04.4 LST (compatibile anche con la versione successiva 22.04.6 LTS), senza interfaccia grafica per ridurre il consumo di risorse.
\end{itemize}
Per configurare l'ambiente necessario viene installato Docker CE (Community Edition) insieme al plugin Docker BuildX. Succcessivamente 
viene installato MicroK8 con Kubernetes v1.24.17 assicurandosi di disabilitare il firewall ufw. 
A questo punto viene installata la versione di TeraFlow 3.0 in una nuova cartella tfs-ctrl.
Successivamente si eseguono i comandi \textit{source my\_deploy.sh} e \textit{./deploy/all.sh} per distribuire il controller su MicroK8.
TeraFlow ha preinstallato al suo interno Mininet containerizzato.
Per avere a disposizione più topologie emulate su Mininet e poter riuscire a connetterle correttamente alle componenti del controller SDN è stata integrata
la cartella ngsdn-tutorial \cite{ngsdn}. 
Al suo interno contiene esercizi pratici e topologie per l'utilizzo di P4 e ONOS, e in questo caso è stata adattata per funzionare con TeraFlow, 
con l'obiettivo di agevolare le sperimentazioni e testare le capacità del controller SDN. 
Infine viene configurata la versione di Python 3.9.18.
\newline La scelta di configurare la VM è il frutto di varie prove, tra cui l'installazione della macchina virtuale creata appositamente per l'Hackfest 3
su VirtualBox 6.1 (con 6 GB di RAM e 50 GB di spazio sul disco) con la versione di TeraFlow 2.1 con degli adattamenti specifici per i test.
In questa prima VM è stata riscontrata un'incompleta implementazione della componente di Policy, che non era in grado di riconoscere le KPI e di conseguenza di gestire le politiche di rete richieste.
%Inoltre è richiesta l'installazione di MicroK8 v1.24.17 con le componenti necessarie, Docker e la versione 3.9.18 di Python.
\newline I vari esperimenti si sono svolti seguendo il seguente schema:
attraverso il Service Level è stato introdotto un servizio nella rete sotto forma di intento per stabilire la connessione, e quindi il percorso, tra due endpoint creando delle KPI 
specifiche per latenza, packet loss e capacità. 
Successivamente, tramite il Management Level, è stata inserita una politica basata sugli eventi che ha consentito di associare un Service Level Agreement (SLA) al servizio,
specificando le condizioni da monitorate e i requisiti che da rispettare durante l'esecuzione.
%modifica file (NodePort..)
%modifica per abilitare più porte 
%end-to-end
%tutte queste info sono persistite in un database logicamente centralizzato, fisicamente distribuito e scalabile, dato dalla componente di Context
\section{Strumenti per la sperimentazione}
Prima di passare alla sperimentazione verranno descritti due strumenti che sono stati fondamentali per questa fase.
L'uso di Mininet è stato necessario per emulare una rete reale costituita da switch P4.
Questi hanno permesso, attraverso la loro programmazione, di portare a termine i vari esperimenti sui 
servizi e sulle politiche.
\subsection{Mininet}
\label{ch:Mininet}
Mininet \cite{mininet} è un sistema open source per l'emulazione di reti, su un unico ambiente Linux, che permette di emulare un'intera rete su un singolo computer, come rappresentato in Figura \ref{fig:mininet}.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{mininet.png}
    \caption{Rete mininet}
    %https://telcomaglobal.com/p/what-is-mininet  fonte foto mininet
    \label{fig:mininet}
\end{figure}
E' ampiamente utilizzato in ambiti di ricerca e sviluppo per creare reti virtuali in modo realistico e testare nuove applicazioni, o protocolli, in un ambiente controllato prima di implementarli su reti reali.
\newline Mininet, a differenza di altri emulatori che utilizzano macchine virtuali per ogni dispositivo, è in grado di gestire un insieme di terminali di rete
utilizzando la virtualizzazione leggera e consentendo di avviare numerosi host e switch (fino a 4096) \cite{MininetOv}.
Questo è reso possibile da tecnologie implementate nel kernel, come i network namespaces, %KERNEL LINUX
che permettono di creare istanze separate di interfacce di rete, tabelle di routing e tabelle ARP che operano in modo indipendente \cite{tesiMininet}. 
%Rispetto ad altri emulatori presenti in circolazione che emulano ogni dispositivo su una macchina virtuale, Mininet offre una serie di vantaggi. 
%Inanzitutto, permette l'avviamento rapido di una rete, la capacità di eseguire test e programmi con tipologie ampie e personalizzate. 
%Inoltre consente di personalizzare l'inoltro dei pacchetti per testare diverse funzionalità con la possibilità di condividere e replicare il codice.
\newline Mininet offre delle API e un interprete Python, o un interfaccia a riga di comando (CLI), che consentono di definire e gestire facilmente le topologie di rete.
In entrambi i casi, si possono scegliere topologie predefinite o personalizzarne di nuove aggiungendo e rimuovendo switch, router, host, controller e link.
Mininet consente anche di configurare regole per l'inoltro dei pacchetti per testare diverse funzionalità, come NAT (Network Address Translation) o ECMP (Equal-Cost Multi-Path), facilitando la replica degli scenari di test in ambienti con caratteristiche differenti.
%switch, router e collegamenti all'interno di un singolo ambiente Linux,
\newline Mininet mette a disposizione tre livelli differenti di API \cite{introMin}:
\begin{itemize}
\item \textbf{Low-level}: consiste nelle classi dei nodi e dei link istanziati individualmente e usati per creare una rete.
\item \textbf{Mid-level}: aggiunge l'oggetto Mininet, un contenitore di nodi e link e fornisce metodi per la configurazione di rete.
\item \textbf{High-level}: aggiunge l'astrazione della topologia di rete, la classe Topo. Offre la possibilità di creare modelli di topologia riusabili passandoli al comando mn da linea di comando.
\end{itemize}
Si possono anche impostare i link come up o down e inserire metriche specifiche 
come quelle di banda, di ritardo, di perdita o di massima lunghezza della coda di recezione per rendere la rete più realistica e adatta a esperimenti.
%\newline Gli host su Mininet condividono il filesystem root del server sottostante. 
%Ciò significa che non è necessario trasferire file tra gli host virtuali perché tutti accedono agli stessi file direttamente.
%Inoltre ci possono essere collisioni tra file se si prova a creare lo stesso file nella stessa directory di più hosts.
%Mininet mette a disposizione una GUI (miniedit) utile per visualizzare lo stato della rete durante gli esperimenti svolti.
\newline Mininet è stato progettato per essere facilmente integrabile con altri software e strumenti di rete,
infatti anche se è fornito un controller di default, consente di connetterne uno remoto agli switch, indipendentemente dal PC su cui è installato.
Questa funzionalià ci ha permesso di poter effettuare la simulazione.
\subsubsection{Alcuni comandi fondamentali}
Di seguito sono stati elencati i comandi fondamentali di Mininet usati per la sperimentazione tramite linea di comando.
La documentazione completa si può trovare al sito ufficiale \cite{walkmin}.
\newline Innanzitutto è fondamentale creare una topologia di rete con il seguente comando\cite{walkmin}:
\newline \textit{\$ sudo mn}
\newline Di default è stata inizializzata la topologia minimale (--topo=minimal) che consiste in uno switch connesso a due host e un controller OpenFlow.
All'interno di Mininet si possono trovare altre topologie disponibili e visualizzabili con il comando \newline \textit{\$sudo mn -h} \\che si possono specificare tramite l'opzione $--topo$.
%scrivi le varie topologie
%--topo single, 3 uno switch con 3 host
\newline Per avviare la topologia esistono diverse opzioni da poter applicare.Ad esempio, l'opzione $--controller$ seguito dall'indirizzo IP specifica il controller al quale gli switch dovranno collegarsi al posto 
del predefinito offerto da Mininet.
%riguarda
\newline Una volta creata la topologia per avere informazioni su di essa esistono diversi comandi:
%mettere output/immagini
\begin{itemize}
    \item \textit{ nodes}: per visualizzare i nodi presenti.
    \item \textit{ net}: per visualizzare i nodi e i link presenti.
    \item \textit{ dump}: per visualizzare tutte le informazioni di dump dei nodi.
    \item \textit{h1 ifconfig}: per visualizzare le interfacce del nodo h1.
\end{itemize}
Alcuni comandi per interagire con la rete e fare dei test minimali sono:
\begin{itemize}
    \item \textit{ h1 ping -c 1 h2 }: verifica il corretto funzionamento del percorso tra h1 e h2.
    \item \textit{ pingall}: esegue il ping tra tutti gli host connessi alla rete.
    \item \textit{ iperf}: esegue un test di banda tra 2 degli host della rete.
    \item \textit{exit}: esce dalla rete.
\end{itemize}
Per manipolare le metriche relative ai link invece vengono messi a disposizione i seguenti comandi:
\begin{itemize}
    \item \textit{ link s1 h1 down}: disabilita un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{ link s1 h1 up}: attiva un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem loss 50\% }: aggiunge una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem delay 200ms}: aggiunge un ritardo di 200ms sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem loss 50\% }: elimina una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem delay 200ms}: elimina un ritardo di 200ms sulla porta eth2 dello switch s2.
\end{itemize} 
\textbf{API Python}
\newline Le API Python di Mininet permettono di creare e gestire topologie di rete in modo più flessibile e programmabile. 
Di seguito esponiamo alcune classi e comandi della Mid-level API utilizzate negli script:
\begin{itemize}
    \item \textit{Mininet}: classe per creare e gestire la rete. Il costruttore prende in input diversi parametri la topologia, gli host, gli switch,i controller, i link e ritorna un oggetto di rete.
    \item \textit{addSwitch()}: aggiunge uno switch alla topologia.
    \item \textit{addHost()}: aggiunge un host alla topologia.
    \item \textit{addLink()}: aggiunge un link alla topologia. Si possono specificare paramentri come la banda espressa in Mbit (bw=10 ), il ritardo (delay='5ms'), massima dimensione della coda espressa in numero di pacchetti (max\_queue\_size=1000), la loss espressa in percentuale (loss=10)
    \item \textit{start}: avvia la rete
    \item \textit{stop}: esce dalla rete
    \item \textit{pingall}: esegue il ping tra tutti gli host connessi alla rete
    \item \textit{h1.cmd('comando da eseguire')}: esegue un comando su h1 da CLI e prende l'output
\end{itemize}
Con le API in Python si può anche estendere il comando \textit{mn} usando l'opzione \textit{--custom} per invocare la topologia ricreata nello script.
\newline \textit{sudo mn --your\_script.py --topo your\_topo}

\subsection{P4}
Programming Protocol-indipendent Packet Processor (P4 \cite{p4}), è un linguaggio di programmazione flessibile
che permette di descrivere il comportamento degli elementi di rete, consentendo di personalizzare come i dispositivi elaborano i pacchetti.
%\newline P4 è nato quindi con l'obiettivo di definire nuove astrazioni per programmare il piano dati e gestire l'inoltro senza dover andare incontro alle limitazioni riscontrate in precedenza.
P4 è stato introdotto per superare le limitazioni di OpenFlow al fine di fornire una soluzione più flessibile.
\newline OpenFlow, pur separando il piano di controllo e il piano dati, si basa su regole di elaborazione dei pacchetti attraverso tabelle che mappano i campi degli header (indirizzi IP, MAC, porte...) in un insieme fisso di funzionalità.
\newline Negli anni le specifiche dei pacchetti sono diventate sempre più complesse, nonchè dipendenti dalle singole aziende che hanno iniziato a sviluppare l'hardware indipendentemente, rendendo necessari continui aggiornamenti per supportare le varie esigenze\cite{p4art}.
D'altra parte, gli switch non aggiornati, o prodotti da aziende diverse, non riescono a supportare tutte le nuove caratteristiche a causa delle limitazioni hardware.
\newline Inoltre, OpenFLow non fornisce delle interfacce operative o amministrative standard, quindi rende complicato aggiungere supporto per nuovi protocolli.
Tutte queste problematiche hanno portato all'introduzione di dispositivi programmabili che utilizzano P4.
\newline I principali obiettivi sono:
\begin{itemize}
    \item \textbf{Indipendenza dal protocollo}: P4 non è vincolato a nessun protocollo specifico consentendo anche di definirne di nuovi o modificare quelli esistenti
    \item \textbf{Indipendenza dal target}: Il codice può essere compilato per funzionare su diversi dispositivi, sia hardware che software, rendendolo versatile
    \item \textbf{Riprogrammabilità}: Il comportamento del piano dati può essere aggiornato dinamicamente consentendo di rispondere rapidamente ai cambiamenti delle esigenze di rete.
\end{itemize}
P4 consente di definire intestazioni e tabelle personalizzate, e programmare esplicitamente il flusso di controllo dello switch, permettendo di adattarsi rapidamente ai cambiamenti e alle innovazioni.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{p4.png}
    \caption{Workflow di P4 sul piano dati \cite{p4Article}}
    \label{fig:p4}
\end{figure}
%Permette di definire la tabella, le azioni e the counters per poi essere applicate a elementi hardware o software switch allo stesso modo.
%P4 ha un interfaccia per analizzare pacchetti e match campi nell'header. In questo modo abbiamo un accoppiamento tra hardware e software.
\newline Nella Figura \ref{fig:p4} è illustrato il Workflow del modello di P4.
\newline Un programma P4 (P4 Program) definisce il comportamento del piano dati desiderato ed è suddiviso in varie sezioni, ognuna delle quali descrive un aspetto specifico del trattamento dei pacchetti.
Il processo inizia con la dichiarazione degli header del pacchetto da analizzare, successivamente si
definisce il comportamento del parser, le tabelle, le azioni e il flusso di controllo.
%Questi header contengono campi come indirizzi IP, numeri di porta .
\newline Il programma è stato poi inviato al compilatore P4 (P4 Compiler) che genera due tipi di output. 
Il primo è un file eseguibile che descrive le varie operazioni da implementare all'interno del dispositivo target.
I target P4 sono dispositivi programmabili ottenibili tramite hardware, come ASICs o FPGA, o software, che realizzano il comportamento desiderato mettendo in atto le specifiche descritte nel programma grazie al file eseguibile.
%Permette inoltre di definire il piano dati in modo dinamico collegandolo al piano di controllo. 
Il parser all'interno dei dispositivi specifica come estrarre e interpretare i vari header dei pacchetti seguendo uno schema prestabilito.
\newline La pipeline di elaborazione (Match-Action Pipeline) include tabelle e azioni che determinano come processare i pacchetti. %guarda se mettere separato tabelle e azioni
La struttura della pipeline è distinta per ciascun dispositivo ed è descritta da un determinato modello di architettura.
Il flusso di controllo invece coordona parser e pipeline per garantire il corretto funzionamento dell'intero processo.
Infine, il deparser ricompone i pacchetti con le eventuali modifiche degli header e successivamente li reintroduce nella rete.
\newline Il secondo file generato dal compilatore è indipendente dal target e 
contiene le informazioni necessarie per far comunicare il piano di controllo e il piano dati tramite l'API P4Runtime.
\newline P4Runtime permette al controller di connettersi ai dispositivi e interagire con la pipeline 
per poter inviare le configurazioni nella relativa tabella \cite{p4Article}. 
I dettagli hardware del piano dati sono nascosti al piano di controllo rendendolo indipendente dalle funzionalità e dai protocolli supportati.
%E' lo schema del pipeline descritto al controller in modo da nascondere il tipo di dispositivi presenti nella rete.
P4 rappresenta un passo significativo verso reti più flessibili e programmabili, consentendo agli sviluppatori di adattarsi rapidamente ai cambiamenti dei requisiti di rete.
Si propone come una soluzione innovativa e versatile per superare le limitazioni degli attuali protocolli e dispositivi di rete fornendo un linguaggio dinamico e indipendente dall'hardware.

\section{Flusso di lavoro degli esperimenti}
L'ambiente di sperimentazione utilizzato è costituito da due componenti principali eseguite all'interno della macchina virtuale: il controller e la rete emulata P4.
Il controller rappresenta l'elemento centrale che gestisce e orchestra la rete consentendo di monitorare lo stato dei collegamenti
e di applicare le politiche.
La rete è costituita da dispositivi P4 emulati su Mininet che consentono la programmazione dinamica del piano dati attraverso il linguaggio P4.
Quesi due elementi riescono a comunicare tramite P4 Runtime e la componente SBI del controller.
\newline Nei paragrafi successivi verrà descritto
il flusso di lavoro della sperimentazione e l'interazione tra le componenti che ne fanno parte.
La Figura \ref{fig:componenti} rappresenta una visione complessiva della configurazione.
In seguito verranno forniti i dettagli più pratici relativi a un'analisi approfondita del codice e della documentazione delle funzioni del controller utilizzate relative ai servizi e alle politiche.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{Componenti teraflow.png}
    \caption{Interazione tra le componenti in TeraFlow per la creazione di un intento}
    \label{fig:componenti}
\end{figure}
\subsection{Servizio end-to-end}
\label{cap:service}
Come già detto in precedenza il controller sfrutta una South-Bound Interface (SBI) per interagire con i device, in questo caso, tramite l'API P4Runtime. 
Inizialmente, il codice P4 compilato, ossia la rappresentazione del programma P4 dopo il processo di compilazione (nella sperimentazione è rappresentato dai file p4info.txt e bmv2.json),
viene copiato nel pod SBI.
Il file \textit{p4info.txt} contiene le specifiche relative alle tabelle e alle azioni definite nel programma P4, come i campi di confronto e i parametri delle azioni,
mentre il file \textit{bmv2.json} descrive i tipi di header, il parser e altre componenti necessarie per la configurazione dei dispositivi.
Questo primo passo permette di poter configurare dinamicamente i dispositivi di rete per instradare il traffico in base alle specifiche del programma P4.
\newline Successivamente vengono registrati i dispositivi e i link nel Context database del controller SDN tramite lo script \textit{run\_test\_01\_bootstrap.sh}.
Lo script richiama il file \textit{test\_functional \_bootstrap.py} che ha più funzionalità: verifica che il contesto sia pronto; crea i dispositivi all'interno del controller aggiungendo a ciascuno le regole di connessione e configurazione; verifica che i dispositivi siano stati registrati nel database.
A questo punto, siamo in grado di richiedere la prima parte dell'intento che sarà implementato: una connessione tra due end points alla componente di Service tramite un servizio.
\newline Per mantenere il processo agnostico rispetto ai dettagli della tecnologia, la componente  
sfrutta una definizione minima permettendo agli utenti di specificare solo quali sono i dispositivi finali.
La richiesta del servizio è composta quindi solo dagli indirizzi IP e dalle porte che si vogliono connettere.
Questi vengono tradotti 
in regole P4 specifiche dal driver del dispositivo, come la creazione delle tabella di instradamento nei dispositivi lungo il percorso che determinano l'uscita dei pacchetti sulla 
base delle regole specificate.
\newline La creazione del servizio è realizzata grazie alle funzioni messe a disposizione dalla componente di Service all'interno del controller descritte in seguito, 
la cui implementazione è presente nella repository pubblica \cite{servserv}.
\newline Inizialmente, la funzione \textit{CreateService} viene utilizzata per creare un servizio di connettività vuoto, specificandone solamente il tipo. 
Questa funzione salva l'identificativo del servizio creato nel database del Context e lo restituisce.
%Nella Figura \ref{fig:componenti} il servizio è stato richiesto dalla componente Web UI, mentre nella nostra sperimentazione si è usato uno script interno al controller.
\newline Successivamente, la funzione \textit{UpdateService} aggiorna il servizio, popolando i campi richiesti come gli endpoint, i vincoli e le configurazioni di servizio, utilizzando i parametri passati \cite{D32}. 
La componente di Service si interfaccia con la componente Context per recuperare la versione più aggiornata del servizio e impostarne lo stato a "Planned" (pianificato).
Infine, si rivolge alla PathComp per calcolare il percorso di rete.
\newline Per eseguire questa operazione la PathComp utilizza informazioni di rete memorizzate nel Context database come i nodi e i collegamenti presenti.
%La PathComp è in grado di soddisfare anche richieste di servizio che attraversano più livelli, in questo caso, la risposta includerà uno o più sottoservizi con le relative sottoconnessioni che li supportano. \textbf{CAPISCILO SENNO LO TOGLI}
\newline Una volta ottenuti i percorsi di rete con i relativi collegamenti calcolati dalla PathComp, la componente di Service crea un \textit{Task Scheduler}, 
responsabile dell'esecuzione delle attività di installazione e smantellamento dei collegamenti, garantendo che queste vengano effettuate nell'ordine appropriato.
Come ultima operazione, il \textit{Task Scheduler} , esegue il metodo \textit{Execute} per applicare le configurazioni necessarie ai dispositivi lungo il percorso stabilito tramite l'SBI. 
Dopo l'esecuzione, lo stato del servizio viene impostato su "Active" (attivo).
\newline Parallelamente, il database della componente Context viene aggiornato con le nuove informazioni, e l'identificativo del servizio viene restituito all'entità chiamante.
\subsection{Vincoli e configurazioni di servizio}
Oltre alla specifica degli endpoints è possibile richiedere azioni supplementari quali l'aggiunta di vincoli o configurazioni di servizio specifiche, 
come la posizione di un endpoint o il numero di giorni per cui un servizio deve rimanere attivo. 
Anche se nella documentazione ufficiale non sono menzionati, tutti i tipi di vincoli supportati 
si possono ritrovare nella repository pubblica \cite{vincoli}.
%In questo modo il componente Policy offre SLA basati sugli eventi per servizi di connettività end-to-end tramite pipeline P4
\newline Queste specifiche vengono inizializzate nella fase di creazione di un servizio e devono essere rispettate finchè quest'ultimo non verrà eliminato. 
Ciò semplifica la gestione dei servizi, in quanto la dichiarazione di informazioni aggiuntive può sostituire l'associazione di una politica.
\newline Nella demo presa in considerazione i vincoli non erano specificati ma sono stati aggiunti per quanto riguarda la latenza e la capacità del percorso, 
come si può vedere nella Figura \ref{fig:abilene}.
Questa aggiunta non ha prodotto cambiamenti nella gestione del servizio,
poiché le funzionalità che consentirebbero di monitorare e adeguare automaticamente il servizio 
in base alle condizioni della rete non sono ancora state implementate nel controller, ma sono previste per release future.
Al momento questa funzionalità può essere implementata andando ad eseguire un passo successivo di specifica e immissione di una politica.
\subsection{Politica}
Per finire l'implementazione dell'intento viene aggiunta una politica al fine di far rispettare al servizio specifici vincoli 
a tempo di esecuzione. 
\newline Inizialmente alla creazione del servizio si associano le differenti KPI tramite i KpiDescriptor.
In ogni KpiDescriptor  si devono specificare i valori numerici da rispettare insieme al tipo di KPI (KpiSampleType).
Quest'ultimo può essere predefinito,
come i KpiSampleType per la latenza (KPISAMPLETYPE\_SERVICE\_LATENCY\_MS) o la capacità (KPISAMPLETYPE\_LINK\_TOTAL\_CAPACITY\_GBPS,  KPISAMPLETYPE\_LINK\_USED\_CAPA CITY\_GBPS) (illustrati nel File pubblico \cite{kpi}),
oppure, utilizzando il tipo UNKNOWN, si può personalizzare tramite una descrizione.
\newline Successivamente viene richiamata la funzione della componente di Monitoring \textit{SetKpi} per ogni regola, passando il KpiDescriptor come parametro, così da
associare il monitoraggio delle metriche richieste a una KPI nel Monitoring database e restituire il relativo identificatore.
Se a uno stesso servizio vengono associate più KPI ognuna deve avere un KpiSampleType diverso, altrimenti, anche se con una descrizione differente,
verrà associato lo stesso identificativo per monitorare metriche diverse creando conflitti. 
Infine al servizio è stata associata una politica specificando le diverse regole collegate tra loro da operatori booleani come AND/OR tramite file JSON.
Per definire una politica va specificato l'id del contesto a cui si vuole associare e l'id del relativo servizio, successivamente vengono specificate le regole.
Ogni regola è composta dall'identificatore della KPI, l'operatore numerico (maggiore, minore o uguale \cite{op}) insieme al valore limite 
per definire l'intervallo di valori non ammessi e infine l'azione da eseguire (le possibili azioni sono riportare nel file \cite{az}).
Appena creata la politica si trova nello stato di VALIDATED. Quando le metriche delle KPI monitorate
specificate nelle regole vengono sottoscritte dalla componente di Monitoring al Metrics database, la politica passa allo stato PROVISIONED.
Se i requisiti richiesti non vengono più soddisfatti, la componente di Monitoring solleva un allarme che invia alla componente di Policy
e la politica passa allo stato ACTIVE.
A questo punto, la componente di Policy recupera il servizio interessato dalla componente Context e applica le azioni correttive specificate.
Nel caso di ricalcolo del percorso viene richiamata la funzione \textit{UpdateService} il cui funzionamento è stato descritto precedentemente \ref{cap:service}.
Durante questa fase lo stato della politica passa a ENFORCED e la politica viene monitorata per un certo periodo per verificare l'efficacia effettiva delle azioni.
Se le metriche continuano a non rispettare i requisiti, la politica passa allo stato INEFFECTIVE, segnalando la necessità di ulteriori interventi.
In caso contrario, se l'azione risolve il problema, la politica viene contrassegnata come EFFECTIVE \cite{D32}.
Il flusso dello stato della politica è descritto anche nella macchina a stati in Figura \ref{fig:diagramma}.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{diagramma policy.png}
    \caption{Macchina a stati interna alla componente di policy \cite{D32}}
    \label{fig:diagramma}
\end{figure}
%\newline Nella sperimentazione abbiamo introdotto una politica per il servizio stabilito tramite un file JSON
%con tre condizioni: una per la latenza (che non deve superare i 100ms), una per la loss (che non deve superare il 5\%) 
%e infine una per la capacità\ref{fig:policy}.
%Queste tre condizioni sono legate tra loro tramite un "OR", quindi appena una di esse non è più rispettata viene invocata l'azione che in questo caso consiste nel ricalcolo del percorso.
%MODIFICA O LA FOTO O LA DESCRIZIONE

\section{Sperimentazione}
In questa sezione verranno descritti gli esperimenti svolti esponendo i codici, le topologie e i comandi usati 
in modo tale da permetterne la riproduzione. In alcuni casi è stata necessaria la modifica dei codici originali; i file sono riportati in Appendice.
\newline Per verificare il comportamento del controller, non avendo a disposizione una rete reale, sono state utilizzate delle topologie
di rete riprodotte tramite Mininet basate su BMv2\cite{bmv2}.
BMv2 (Behavioral Model version 2) è un software open-source che implementa uno switch virtuale programmabile utilizzato principalmente
per sviluppare e testare reti gestite dal paradigma SDN basate su protocolli programmabili, in particolare P4.
\newline Per iniziare il lavoro si è partiti da una demo già preesistente, apportando in seguito le modifiche necessarie.
\subsection{Esperimento 1}
Inizialmente si è riprodotta la demo dell'Hackfest 3.
Lo scopo di questa sperimentazione consiste nel creare un intento di rete, attraverso la definizione di un servizio, 
che connette due endpoint, client e server, tramite un collegamento che mantiene i valori di latenza sotto 
una determinata soglia grazie a una politica ad esso associata.
I dettagli si possono ritrovare nella pagina del sito \cite{hackfest}.
\newline Dopo aver verificato che le componenti del controller siano in stato di running (\textit{Kubectl get pods -n=tfs}),
si istanzia la topologia \cite{topo4} costituita da 4 nodi e 4 link, illustrata in Figura \ref{fig:top4}, sul container di Mininet collegato al controller
tramite il comando \textit{make start}.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{topologiasw4.png}
    \caption{Topologia 4 switch e 2 percorsi}
    \label{fig:top4}
\end{figure}
\newline Il file Objects.py mantiene le informazioni sulla topologia, sui dispositivi e sui servizi
che saranno utilizzate dagli script (per il bootstrap, la creazione, l'eliminazione del servizio e per la pulizia dell'ambiente alla fine dell'esperimento) 
per comunicare le configurazioni al controller.
\newline Il primo comando da eseguire sul terminale del controller (\textit{./setup.sh}) configura il pod Kubernetes dell'SBI
inserendo le configurazioni di P4 necessarie al server per comunicare con gli switch.
Questo comando crea l'ambiente di esecuzione e imposta i parametri di configurazione di P4, come le tabelle di instradamento e le regole di forwarding, e 
inizializza i driver degli switch per supportare la gestione dei dispositivi all'interno della rete. 
\newline Successivamente viene eseguito il comando \textit{./run\_test\_01\_bootstrap.sh} che richiama il file \textit{test\_functional\_bootstrap.py}.
Questo comando registra la topologia collegata a Mininet al controller, inserendo i dispositivi e i collegamenti nel Context database, così da poter proseguire con l'installazione del servizio.
\newline Alla URL http://localhost:8080/webui si può accedere all'interfaccia grafica del controller e, selezionando il contesto \textit{Context:(admin):Topology(admin)},
si può verificare la corretta registrazione nel controller della topologia di rete e delle configurazioni ad essa associate.
Dal terminale Mininet (connettendosi tramite \textit{./connect-to-mininet.sh}) e
si attiva lo script \textit{tfsagent} (\textit{./tfsagent.sh}) che permette di ascoltare gli eventi dalla componente di Context e, 
quando un servizio viene registrato, crea la KpiDescriptor relativa alla latenza e la registra nel Monitoring database.
In questo script il KpiSampleType specificato è UNKNOWN e il relativo commento è "\textit{Latency value for service \{\}}".
Successivamente si mette in attesa dei dati dal tfsping per ricevere le metriche relative alla latenza del collegamento e crea delle KPI da mandare alla componente di Monitoring.
\newline A questo punto si può creare il servizio tra i due endpoints (\textit{./run\_test\_02\_ create\_service.sh}) e visualizzarlo sull'interfaccia web, come si può vedere nelle Figure \ref{fig:ser} \ref{fig:sw4}.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{service active.png}
    \caption{Servizio}
    \label{fig:ser}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Copia di 4sw_2.png}
    \caption{Servizio iniziale con topologia a 4 switch}
    \label{fig:sw4}
\end{figure}
Gli endpoints relativi a questa demo sono “SW1-port3” e “SW4-port3”, 
gli switch della topologia collegati rispettivamente al client e al server instanziati su Mininet.
A questo punto se si esegue il comando \textit{client ping server} sul terminale di Mininet il server risponde come atteso.
Per implementare correttamente la politica è necessario modificare il file \textit{manifests/policyservice.yaml}, come rappresentato in Figura \ref{fig:pol}.
Per farlo, apriamo il file dal terminale con il comando \textit{kubectl edit svc policyservice -n=tfs} per accedere alla configurazione del policyservice nel namespace tfs.
In particolare si deve aggiungere \textit{NodePort: 30060} così da consentire di esporre il servizio sulla porta 30060 che sarà utilizzata dallo script \textit{addPolicy.sh} in seguito.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{modificafile.png}
    \caption{File policyservice modificato}
    \label{fig:pol}
\end{figure}
Adesso è possibile installare la regola di politica.
Questa viene definita utilizzando un file JSON (\textit{policyAddService.json}) che specifica gli identificativi 
del contesto e del servizio a cui si vuole associare la politica, oltre alle regole che devono essere rispettate. 
In questo caso 
alla politica viene associata una sola regola rappresentata dall'identificativo KpiId, registrato in precedenza nel Monitoring database dall'agent, 
specificando la condizione che il valore non deve superare 10000.
Se tale richiesta viene violata, la regola indicata è il ricalcolo del percorso.
Lo script è richiamato eseguendo \textit{./addPolicy.sh}.
Dalla Web UI si può constatare che la politica è stata correttamente implementata, come rappresentato in Figura \ref{fig:lat}.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{politica lat.png}
    \caption{Politica associata al servizio}
    \label{fig:lat}
\end{figure}
Infine, per verificare l'efficacia della politica inserita, si aggiunge un ritardo su un'interfaccia di uno switch interno al percorso prestabilito.
Ciò viene realizzato tramite il comando
\textit{switch2 tc qdisc add dev switch2-eth2 root netem delay 2000ms}. 
In questo caso, si applica un ritardo di 2000 ms sull'interfaccia eth2 dello switch switch2 con lo scopo di aumentare la latenza del percorso di rete
e provocare una violazione dei vincoli imposti dalla regola della politica definita.
Questo dovrebbe attivare l'azione corrispondente di ricalcolo del percorso.
A questo punto, per verificare che ciò avvenga, viene eseguito \textit{client ./tfsping}, uno script che utilizza il comando ping per calcolare la latenza del percorso e mandarla al \textit{tfsagent}.
Quest'ultimo trasmette la metrica alla componente di Monitoring, che ha il compito di verificare i valori e notificare la violazione della soglia imposta. 
In questo caso, il superamento del limite di latenza ha attivato correttamente l'azione prevista come si può vedere dal nuovo percorso in Figura \ref{fig:sw4 dopo}.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Copia di topologiasw4.png}
    \caption{Servizio con cambiamento di percorso dovuto all'introduzione di un aumento della latenza}
    \label{fig:sw4 dopo}
\end{figure}
Si è fatta anche un'ulteriore prova, invece di aumentare la latenza si è direttamente disattivato un link tra due switch, 
in questo caso switch2 e switch4, tramite \textit{link switch2 switch4 down} e si è potuto constatare che anche in questo caso è stata attivata l’azione della politica.
\newline Prima di spengere la macchina virtuale del controller bisogna disattivare correttamente le risorse istanziate.
Per prima cosa va rimossa la politica dalla quale dipende il servizio \textit{./removePolicy.sh}. Dopo aver eseguito il comando 
è opportuno verificare tramite l'interfaccia web che la politica non è più disponibile; vista l'instabilità del controller potrebbe essere necessario rieseguirlo una seconda volta prima di continuare.
Successivamente si disattiva il servizio (\textit{./run\_test\_03\_ delete\_service.sh}) e si rimuovono i dispositivi e l'ambiente allocati (\textit{./run\_test\_04\_ cleanup.sh}).
\newline Questo primo esperimento ha permesso di acquisire familiarità con l'ambiente del controller, 
comprendere l'interazione con esso e configurare correttamente i file necessari per l'implementazione dell'ambiente di test, confermando così la corretta installazione del sistema.
Sono state verificate le funzionalità di interazione con la rete attraverso il programma P4 e l'interfaccia SBI per la
gestione degli intenti grazie all'utilizzo delle componenti di Service e di Policy.
E' stato implementato un servizio nel sistema al quale è stata applicata una politica di routing 
verificata tramite il monitoraggio della relativa KPI.
Infine, sono stati simulati eventi di aumento di latenza e disattivazione di link, che hanno permesso di 
verificare il comportamento del sistema in risposta a tali cambiamenti.
%Inoltre, si è notato che una volta rimosso il ritardo, la perdita di pacchetti, o riattivato un link, il controller percepisce questo cambiamento e modifica automaticamente il percorso precedente, poiché è memorizzato nel database della Path Comp come percorso preferito per le sue caratteristiche.
\subsection{Esperimento 2}
In questo esperimento la topologia di base da cui ha avuto inizio è stata quella descritta nel file del controller \textit{8switch3path.py} \cite{8sw}.
Dopo lo script è stato modificato al fine di aggiungere due link.
Il primo tra gli switch sw2 e sw5, l'altro tra sw4 e sw3, al fine di creare 5 possibili percorsi tra client e server al posto di 3. 
La topologia finale, con 8 nodi e 11 link, è illustrata nella Figura \ref{fig:top8}. 
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{sw4.drawio.png}
    \caption{Topologia 8 switch e 5 percorsi}
    \label{fig:top8}
\end{figure}
\newline Prima di inizializzare la topologia è stato necessario modificare il file \textit{docker-compose.yaml}
per abilitare ulteriori porte di rete (50005,50006,50007,50008) al fine di stabilire le connessioni per la comunicazione 
tra Mininet containerizzato e il controller.
Queste porte vengono esposte per consentire di comunicare con i dispositivi P4 emulati in Mininet, in modo che i comandi possano essere inviati e ricevuti correttamente.
\newline Per questa sperimentazione sono stati utilizzati gli script presenti nella cartella P4 del controller \cite{ofc}.
E' stato necessario il cambiamento del file \textit{Objects.py} per una corretta sincronizzazione con la topologia istanziata.
Le modifiche consistono nell'inserimento di una interfaccia agli switch interessati (sw2, sw3, sw4, sw5) per poter 
supportare un nuovo collegamento e la specifica di quest'ultimi in modo unidirezionale come riportato in Appendice \ref{cap:link}.
Nello stesso file sono stati aggiunti dei vincoli di configurazione relativi alla latenza e alla capacità nella definizione del servizio, con la conseguente modifica anche del file \textit{test\_functional\_create\_service.py}.
I cambiamenti sono evidenziati in Appendice \ref{cap:obj}.
\newline Per quanto riguarda la parte iniziale i comandi per la configurazione dei dispositivi sono gli stessi della sezione precedente.
\newline Successivamente viene definito un probe, necessario per il monitoraggio della rete e la raccolta di informazioni.
Il probe in questione è costituito da due componenti principali: l'agent e il ping.
L'agent è responsabile della creazione delle KpiId, della raccolta dei dati e dell'invio delle metriche al sistema di Monitoring del controller.
Il ping, invece, viene utilizzato per testare la latenza del percorso di rete e la packet loss.
\newline L'implementazione utilizzata è disponibile nella cartella del controller \cite{probe} e sono gli script  
utilizzati nella seconda parte della demo dell'Hackfest 3.
\newline I file sono stati modificati per aggiungere la metrica della packet loss. 
Per quanto riguarda l'agent si è aggiunta la relativa KpiDescriptor da sottoscrivere alla componente di Monitoring ed
è stata necessaria la modifica del KpiSampleType per il KpiDescriptor della latenza da UNKNOWN a SERVICE\_LATENCY\_MS. 
Pur essendo uno dei tipi predefiniti, non tutti sono implementati completamente, infatti
i KpiSampleType in stato finale sono: UNKNOWN, PACKETS\_TRANSMITTED, PACKETS\_RECEIVED, BYTES\_TRANSMITTED, BYTES\_RECEIVED e LINK\_TOTAL \_CAPACITY\_GBPS.
Per questo motivo all'inizio la KPI relativa alla latenza non aveva l'effetto desiderato; per sistemare questo problema è stata necessaria la 
modifica di alcuni file di configurazione del controller relativi alla gestione dei kpiSampleType.
Utilizzare un KpiSampleType specifico non è stato possibile per la packet loss in quanto non era tra i tipi, anche solo parzialmente, implementati.  
La seconda parte dello script dell'agent è stata adattata per ricevere la metrica aggiunta e inviare entrambi i valori di latenza e packet loss alla componente di Monitoring.
Per quanto riguarda il file \textit{ping2.py} viene calcolata la packet loss grazie ai risultati ottenuti dall'esecuzione
del comando ping in Mininet.
Per effettuare questo calcolo, vengono mantenute le informazioni sul numero totale di ping inviati e sul numero di quelli riusciti.
La percentuale di pacchetti persi viene quindi calcolata sottraendo i ping riusciti dal totale e dividendo il risultato per il totale dei ping, moltiplicando poi per 100. 
Una volta calcolata la percentuale, questa metrica viene inviata all'agent tramite una socket.
Per un problema di comunicazione si è modificato il percorso della socket, infatti inizialmente era \textit{/home/teraflow/ngsdn-tutorial/tmp/sock} ed
è stata cambiata in \textit{"/home/teraflow/ngsdn-tutorial/tmp/tfsping"}.
\newline In un terminale si eseguono i seguenti comandi per avviare l'agent del probe: \textit{source $\sim$/tfs-ctrl/tfs\_ runtime\_env\_vars.sh} per inserire le variabili di TeraFlow
necessarie, se l'ambiente Python è disattivato \textit{pyenv activate 3.9.18/envs/tfs}, e infine \textit{python agent.py}.
Quando si esegue il comando relativo alla creazione del servizio (\textit{./run\_test\_02\_create\_service.sh}),
sul terminale in cui è in esecuzione l'agent viene visualizzato l'output in Figura \ref{fig:ag} che indica la corretta creazione e la sottoscrizione al
Metrics database dei KPI richiesti al servizio creato.
\newline Il servizio ha come endpoints "SW1-port4” e “SW8-port4”, come si può vedere in Figura \ref{fig:sw8}.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{Copia di sw8_dopo.png}
    \caption{Servizio iniziale con topologia a 8 switch}
    \label{fig:sw8}
\end{figure}
\begin{figure}[h]
    \centering
   \includegraphics[width=0.5\textwidth]{create new event ag.png}
    \caption{Creazione KPI dallo script agent.py}
    \label{fig:ag}
\end{figure}
\newline Per creare la politica si riutilizzano gli script precedenti assicurandosi di cambiare nel file \textit{policyAddService.json}
gli id relativi al contesto e al servizio e di aggiungere alle regole quella relativa alla packet loss. Lo script è riportato in Appendice \ref{cap:pktl} e
la politica risultante è illustrata in Figura \ref{fig:policy}.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{policy 5 100.png}
    \caption{Politica associata al servizio}
    \label{fig:policy}
\end{figure}
\newline Sul terminale Mininet, oltre alle prove fatte aggiungendo dei ritardi e disattivando dei link, 
per verificare il funzionamento della seconda regola inserita ,si simula una packet loss.
Questo avviene tramite il comando
\textit{switch4 tc qdisc add dev switch6-eth2 root netem loss 7\%}
che applica una perdita del 7\% dei pacchetti sull'interfaccia eth2 dello switch 6.
Al fine di verificare la politica viene attivato lo script \textit{ping2.py} tramite Mininet
(\textit{client python ping2.py 10.0.0.2}).
Anche in questo caso, dopo aver alterato lo stato della rete in modo tale che le regole della politica non fossero più rispettate,
si è riscontato un cambiamento nel persorso come mostrato in Figura \ref{fig:sw8 dopo}.
Infine tramite i comandi esposti in precendenza, cambiando nello script \textit{removePolicy.json} l'identificativo della politica, si sono disattivati in ordine la politica, il servizio e i dispositivi.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{Copia di sw8_dopo1.png}
    \caption{Servizio con cambiamento di percorso dovuto all'introduzione di una perdita di pacchetti}
    \label{fig:sw8 dopo}
\end{figure}
Lo scopo principale di questo secondo esperimento è stato di consolidare le conoscenze acquisite nel primo esperimento,
consentendo di apportare modifiche autonome agli script forniti e di esplorare come il controller gestisce una politica con più di una regola
in un ambiente di rete dinamico e con una topologia più ampia. 
Simulando la perdita di pacchetti, è stato verificato il comportamento del sistema nella gestione,
e nell'applicazione, di entrambe le regole.
\subsection{Esperimento 3}
L'ultima topologia utilizzata è stata Abilene \cite{abilene}, raffigurata in Figura \ref{fig:ab}; una rete di trasporto creata da Internet2 con 11 nodi e 14 link.
La configurazione della rete emulata in Mininet per questa topologia è disponibile in \cite{topab}. 
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{abilene.png}
    \caption{Topologia Abilene}
    \label{fig:ab}
\end{figure}
Partendo da questo script, è stato modificato al fine di garantire la connessione tra il container e il controller e 
la corretta configurazione del servizio da istanziare successivamente per l'intento.
Una delle modifiche apportate è stata la configurazione manuale delle associazioni ARP nei due endpoints, tramite il comando setARP,
per garantire il corretto instradamento dei pacchetti e evitare ritardi dovuti alla risoluzione ARP.
Inoltre la larghezza di banda è stata impostata a 30 Mbps per simulare l'ambiente.
Lo script è riportato in Appendice \ref{cap:ab} e la relativa topologia è rappresentata in Figura \ref{fig:d}. 
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{ab_topologia.png}
    \caption{Topologia Abilene in Teraflow}
    \label{fig:d}
\end{figure}
Per poter utilizzare la topologia sono state abilitate ulteriori porte di rete nel file \textit{docker-compose.yaml} (5009, 5010, 5011).
\newline Per quanto riguarda gli script sono stati riutilizzati quelli della sperimentazione precedente nella cartella P4 \cite{ofc} con le 
modifiche al file \textit{Objects.py} per rappresentare la topologia; l'aggiunta dei nuovi dispositivi con le relative interfacce 
e il cambiamento dei link descritti. 
I comandi iniziali per la configurazione dei dispositivi sono i medesimi degli esperimenti precedenti.
\newline In questa sperimentazione è stata introdotta una nuova regola di politica relativa alla capacità.
A tal fine è stato modificato nuovamente il file \textit{agent.py} con l'aggiunta di una nuova KpiDescriptor, con il relativo KpiSampleType LINK\_TOTAL\_CAPACITY\_GBPS, e si è avviato.
Il file è riportato in Appendice \ref{cap:agcap}.
Successivamente viene creato il servizio, illustrato in Figura \ref{fig:abilene}, con endpoints "SW1-port3” e “SW8-port4”, che nella topologia corrispondono alle città di New York e Kansas City,
poi si istanzia la politica, raffigurata in Figura \ref{fig:policy cap}, cambiando nel file JSON i campi relativi
al contesto, al servizio e alle regole (in Appendice \ref{cap:polcap}).
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{ab_def_11.png}
    \caption{Servizio iniziale con Abilene}
    \label{fig:abilene}
\end{figure}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{policy cap.png}
    \caption{Politica associata al servizio}
    \label{fig:policy cap}
\end{figure}
Infine per verificarla , oltre ai test effettuati anche negli altri esperimenti, 
si è modificata la capacità tramite due differenti comandi: \textit{switch2 tc qdisc add dev switch2-eth2 root tbf rate 1mbit burst 10kb latency 50ms} che 
limita la velocità di trasmissione a 1Mbps e imposta un ritardo massimo di 50ms per i pacchetti nella coda nell'interfaccia eth2 dello switch2; 
\textit{switch1 tc qdisc add dev switch1-eth1 root tbf rate 1mbit burst 10kb limit 10000} che
limita la velocità di trasmissione a 1Mbps e specifica un limite sulla dimensione massima della coda dell'interfaccia eth1 dello switch1 a 10.000 byte.
\newline Prima di attivare lo script del ping vanno effettuate delle modifiche per poter monitorare anche la capacità e inviare la metrica all'agent.
Mentre la latenza e la packet loss potevano essere calcolate tramite il comando ping, per la capacità questo non è possibile.
Viene introdotto nel codice la chiamata al comando iperf,
questo serve per misurare attivamente l'ampiezza di banda disponibile nel percorso tra due endpoints.
Il codice è riportato in Appendice \ref{cap:pingcap}.
\newline Prima di eseguire il comando su Mininet per avviare lo script (\textit{h0 python ping2.py 10.0.0.8}), è necessario attivare iperf sul dispositivo finale \textit{h7} tramite il comando \textit{iperf -s \&},.
L'opzione \& permette di avviare iperf in background e riutilizzare lo stesso terminale per altre operazioni.
Se gli script sono attivi dopo aver modificato la velocità di trasmissione, o altre metriche nello stato della rete
in modo tale che le regole della politica non siano più rispettare, si verifica un cambiamento del percorso,
come si può vedere dalla Figura \ref{fig:abilene post}.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{ab_def_10.png}
    \caption{Servizio con cambiamento di percorso dovuto a un'introduzione di limitata capacità}
    \label{fig:abilene post}
\end{figure}
Come negli altri esperimenti, con le solite modifiche ai file, si sono disattivati politica, servizio e dispositivi ripristinando l'ambiente alle condizioni iniziali.
\newline Questo esperimento si è distinto per l'utilizzo della topologia Abilene, che ha permesso di lavorare su una rete più ampia e realistica.
Alla politica è stata aggiunta una nuova regola introducendo un vincolo sulla capacità.
Per monitorare questo nuovo valore, è stato integrato l'uso di iperf nel probe per misurare la larghezza di banda effettiva.
Attraverso comandi eseguiti tramite Mininet, è stata limitata la velocità di trasmissione dei pacchetti. 
Ciò ha permesso di simulare le limitazioni sulla capacità, garantendo una verifica accurata della corretta attivazione di tutte le regole di politica e 
dimostrando l'abilità del sistema di ricalcolare dinamicamente i percorsi in base ai vincoli definiti.
