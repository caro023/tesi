\chapter{Studio e sperimentazione della gestione di policy in Teraflow}
\label{cap:policy}
%Lo scopo della tesi è quello di stabilire una connessione tra due end points nella rete che rispetti determinate caratteristiche
%a tempo di esecuzione utilizzando il controller SDN Teraflow.
In questo Capitolo verranno esposti i vari esperimenti eseguiti utilizzando il controller SDN TeraFlow per quanto riguarda la gestione degli intenti.
Nella fase iniziale si è seguito l'Hackfest 3 \cite{hackfest} dove è stato fornito un ambiente pre-configurato per testare TeraFlow SDN. 
Il lavoro è iniziato con l'installazione della macchina virtuale \cite{VM} creata appositamente per il congresso.
%Gli esperimenti sono stati svolti all'interno di un'infrastruttura virtualizzata configurata per eseguire sia TeraFlow che Mininet, permettendo di simulare e analizzare vari scenari di rete.
%La macchina virtuale (VM) utilizzata inizialmente per gli esperimenti è stata quella utilizzata durante l'Hackfest 3 \cite{hackfest}, dove è stato fornito un ambiente pre-configurato per testare TeraFlow SDN. 
\\Di seguito vengono riportate le specifiche della VM utilizzata:
\begin{itemize}
    \item IP Address: 10.0.2.X/24 (DHCP)
    \item Gateway: 10.0.2.1
    \item DNS: 8.8.8.8, 8.8.4.4
    \item Creata con VirtualBox 6.1 ma compatibile con versioni successive
    \item Requisiti minimi: 4 vCPU, 6 GB di RAM, 50 GB di spazio sul disco, Virtual Disk Image (VDI)
    \item Connessione di rete: NAT Network con porte 22 per SSH e 80 per HTTP esposte
    \item Sistema operativo senza interfaccia grafica per ridurre il consumo di risorse.
\end{itemize}
La VM ha al suo interno preinstallati MicroK8s con le componenti richieste e Mininet in formato docker.
La versione di TeraFlow utilizzata è la 2.1 con adattamenti specifici per l'Hackfest.
\\Successivamente è stata installata la VM con la versione aggiornata di TeraFlow (3.0) a causa di un'incompleta implementazione della componente di Policy che non era in grado di gestire e riconoscere le KPI.
Le caratteristiche richieste sono sostanzialmente simili, con un aumento di memoria RAM a 8 GB e dello spazio su disco a 60 GB.
Il sistema operativo utilizzato è Ubuntu Server 22.04.4 LTS, compatibile anche con la versione successiva 22.04.6 LTS. 
Inoltre è richiesta l'installazione di MicroK8 v1.24.17 cone le componenti necessarie, Docker e la versione 3.9.18 di Python.
\\I vari esperimenti si sono svolti seguendo il seguente schema:
attraverso il Service Level è stato introdotto un servizio nella rete sotto forma di intento per stabilire la connessione e il percorso. 
Successivamente, tramite il Management Level, è stata inserita una politica basata sugli eventi che consente di associare un Service Level Agreement (SLA) a un servizio specifico.
Questo SLA include condizioni che devono essere monitorate e rispettate durante l'esecuzione del servizio.
%modifica file (NodePort..)
%modifica per abilitare più porte 
%end-to-end
%tutte queste info sono persistite in un database logicamente centralizzato, fisicamente distribuito e scalabile, dato dalla componente di Context
\\Per iniziare il lavoro si è partiti da una demo già preesistente, apportando in seguito le modifiche necessarie.
\section{Strumenti per la sperimentazione}
INTRO
\\li descrivo perche verranno usati switch p4 emulati in mininet
\subsection{Mininet}
\label{ch:Mininet}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{mininet.png}
    \caption{Rete mininet}
    %https://telcomaglobal.com/p/what-is-mininet  fonte foto mininet
    \label{fig:mininet}
\end{figure}
Mininet \cite{mininet} è un sistema open source di orchestrazione per l'emulazione di reti su un unico ambiente Linux che permette di emulare un'intera rete su un singolo computer.
E' ampiamente utilizzato in ambiti di ricerca e sviluppo per creare e testare reti virtuali in modo realistico.
A differenza di altri emulatori che utilizzano macchine virtuali per ogni dispositivo, si distingue per la sua capacità di avviare rapidamente reti virtuali complesse e di eseguire test su scenari vari e personalizzati.
Consente anche di configurare l'inoltro dei pacchetti per testare diverse funzionalità, facilitando la condivisione e la replica del codice.
%Rispetto ad altri emulatori presenti in circolazione che emulano ogni dispositivo su una macchina virtuale, Mininet offre una serie di vantaggi. 
%Inanzitutto, permette l'avviamento rapido di una rete, la capacità di eseguire test e programmi con tipologie ampie e personalizzate. 
%Inoltre consente di personalizzare l'inoltro dei pacchetti per testare diverse funzionalità con la possibilità di condividere e replicare il codice.
\\Mininet offre delle API e un interprete Python che consentono di definire e gestire facilmente delle topologie di rete.
%Ciò è possibile anche tramite interfaccia a riga di comando (CLI). In entambi i casi possono essere sia predefinite che personalizzate con la possibilità di 
%aggiungere e rimuovere switch, router, host, controller e link, tutti eseguiti su un unico computer.
%Mininet utilizza infatti una virtualizzazione leggera per creare nodi di rete ognuno con la propria pila di rete in modo tale che possano comunicare con gli 
%altri nodi come farebbero in una rete fisica. 
È inoltre possibile utilizzare un'interfaccia a riga di comando (CLI) per la stessa funzione.
In entrambi i casi, si possono configurare topologie predefinite o personalizzate, aggiungendo e rimuovendo switch, router, host, controller e link, tutti eseguiti su un unico computer.
\\Mininet è in grado di gestire un insieme di terminali di rete (host), 
%switch, router e collegamenti all'interno di un singolo ambiente Linux,
utilizzando la virtualizzazione leggera attraverso tecnologie implementate nel kernel Linux, come i network namespaces.
Questi permettono di creare istanze separate di interfacce di rete, tabelle di routing e tabelle ARP, che operano in modo indipendente \cite{tesiMininet}. 
Questo approccio consente di avviare numerosi host e switch (fino a 4096) su un singolo kernel del sistema operativo, simulando una rete completa su un'unica macchina \cite{MininetOv}.
Ciò consente di testare nuove applicazioni, protocolli e algoritmi in un ambiente controllato e modificabile prima di implementarli su reti reali.
\\Mininet mette a disposizione tre livelli differenti di API \cite{introMin}:
\begin{itemize}
\item \textbf{Low-level}: consiste nelle classi dei nodi e dei link istanziati individualmente e usati per creare una rete.
\item \textbf{Mid-level}: aggiunge un containter per nodi e link, l'oggetto Mininet, e fornisce metodi per la configurazione di rete.
\item \textbf{High-level}: aggiunge l'astrazione della topologia di rete, la classe Topo. Offre la possibilità di creare modelli di topologia riusabili passandoli al comando mn da linea di comando.
\end{itemize}
Si possono configurare i link come up o down e inserire metriche specifiche 
come quelle di banda, ritardo, perdita o massima lunghezza della coda di recezione per rendere la rete più realistica e adatta a esperimenti di test.
\\Gli host su Mininet condividono il filesystem root del server sottostante. 
Ciò significa che non è necessario trasferire file tra gli host virtuali perché tutti accedono agli stessi file direttamente.
Tuttavia, questa condivisione del filesystem può creare problemi se un programma ha bisogno di file di configurazione specifici per ogni host. 
In tal caso, è necessario creare un file di configurazione separato per ogni host e specificare quale file utilizzare quando si avvia il programma.
Un'altra limitazione riguarda la condivisione delle risorse del sistema su cui è in esecuzione che dovranno essere bilanciate tra tutti gli host della rete.
%Inoltre ci possono essere collisioni tra file se si prova a creare lo stesso file nella stessa directory di più hosts.
%Mininet mette a disposizione una GUI (miniedit) utile per visualizzare lo stato della rete durante gli esperimenti svolti.
\\Mininet è stato progettato per essere facilmente integrabile con altri software e sistemi di rete.
Consente anche di connettere un controller SDN remoto, quindi esterno alla rete, agli switch, indipendentemente dal PC su cui è installato, in modo da fornire un ambiente adatto allo sviluppo e al test.


\subsubsection{Alcuni comandi fondamentali}
\textbf{Linea di comando}
\\Inanzitutto è fondamentale creare una topologia di rete con il seguente comando\cite{walkmin}:
\\\textit{\$ sudo mn}
\\Di default viene inizializzata la topologia minimale (--topo=minimal) che consiste in uno switch connesso a due host e un controller OpenFlow.
All'interno di Mininet si possono trovare altre topologie disponibili e visualizzabili con il comando \\\textit{\$sudo mn -h} \\che si possono specificare tramite l'opzione $--topo$.
%scrivi le varie topologie
%--topo single, 3 uno switch con 3 host
\\Per avviare la topologia esistono diverse opzioni da poter applicare.Ad esempio, l'opzione $--controller$ seguito dall'indirizzo IP specifica il controller al quale gli switch dovranno collegarsi al posto 
del predefinito offerto da Mininet.
%riguarda
\\Una volta creata la topologia per avere informazioni su di essa esistono diversi comandi:
%mettere output/immagini
\begin{itemize}
    \item \textit{ nodes}: per visualizzare i nodi presenti.
    \item \textit{ net}: per visualizzare i nodi e i link presenti.
    \item \textit{ dump}: per visualizzare tutte le informazioni di dump dei nodi.
    \item \textit{h1 ifconfig}: per visualizzare le interfacce del nodo h1.
\end{itemize}
Alcuni comandi per interagire con la rete e fare dei test minimali sono:
\begin{itemize}
    \item \textit{ h1 ping -c 1 h2 }: verifica il corretto funzionamento del percorso tra h1 e h2.
    \item \textit{ pingall}: esegue il ping tra tutti gli host connessi alla rete.
    \item \textit{ iperf}: esegue un test di banda tra 2 degli host della rete.
    \item \textit{xterm h1}: permette di avviare il terminale relativo al nodo h1.
    \item \textit{exit}: esce dalla rete.
\end{itemize}
Per manipolare le metriche relative ai link invece vengono messi a disposizione i seguenti comandi:
\begin{itemize}
    \item \textit{ link s1 h1 down}: disabilita un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{ link s1 h1 up}: attiva un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem loss 50\% }: aggiunge una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem delay 200ms}: aggiunge un ritardo di 200ms sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem loss 50\% }: elimina una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem delay 200ms}: elimina un ritardo di 200ms sulla porta eth2 dello switch s2.
\end{itemize} 
\textbf{API Python}
\\Le API Python di Mininet permettono di creare e gestire topologie di rete in modo più flessibile e programmabile. 
Di seguito esponiamo alcune classi e comandi della Mid-level API:
\begin{itemize}
    \item \textit{Mininet}: classe per creare e gestire la rete. Il costruttore prende in input diversi parametri la topologia, gli host, gli switch,i controller, i link e ritorna un oggetto di rete.
    \item \textit{addSwitch()}: aggiunge uno switch alla topologia.
    \item \textit{addHost()}: aggiunge un host alla topologia.
    \item \textit{addLink()}: aggiunge un link alla topologia. Si possono specificare paramentri come la banda espressa in Mbit (bw=10 ), il ritardo (delay='5ms'), massima dimensione della coda espressa in numero di pacchetti (max\_queue\_size=1000), la loss espressa in percentuale (loss=10)
    \item \textit{start}: avvia la rete
    \item \textit{stop}: esce dalla rete
    \item \textit{pingall}: esegue il ping tra tutti gli host connessi alla rete
    \item \textit{h1.cmd('comando da eseguire')}: esegue un comando su h1 da CLI e prende l'output
\end{itemize}
Con le API in Python si può anche estendere il comando \textit{mn} usando l'opzione \textit{--custom} per invocare la topologia ricreata nello script.
\\\textit{sudo mn --your\_script.py --topo your\_topo}

\subsection{P4}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{p4.png}
    \caption{Workflow di P4 sul piano dati \cite{p4Article}}
    \label{fig:p4}
\end{figure}
P4 \cite{p4}, che sta per "Programming Protocol-indipendent Packet Processor",  è un linguaggio di programmazione ad alto livello che consente di descrivere il comportamento di un elemento di rete e del piano dati in modo flessibile.
Il linguaggio permette di personalizzare il modo in cui i dispositivi di rete elaborano i pacchetti, senza essere vincolati a un set predefinito di protocolli.
\\P4 è stato introdotto per l'esigenza di superare le limitazioni con cui si stava interfacciando OpenFlow.
Per quest'ultimo protocollo l'hardware e il software non erano più sincronizzati. In alcuni casi 
gli switch non riuscivano a supportare tutte le funzionalità che OpenFlow poteva offrire a causa delle limitazioni hardware in quanto progettati per supportare un insieme fisso di protocolli e funzionalità. 
Per introdurre un cambiamento dell'hardware sono richiesti vari sforzi per lo sviluppo e il finanziamento, inoltre richiederebbe mesi se non anni per essere portato a termine.
\\P4 è nato quindi con l'obiettivo di definire nuove astrazioni per programmare il piano dati senza dover andare incontro alle limitazioni riscontrate.
Offre un linguaggio di programmazione che permette di definire regole per cui i pacchetti vengono processati direttamente nei dispositivi di rete.
Di seguito vengono indicati i vari passi che si eseguono durante un flusso di lavoro di un programma P4 che si possono ritrovare anche nell'immagine \ref{fig:p4}.
%Permette di definire la tabella, le azioni e the counters per poi essere applicate a elementi hardware o software switch allo stesso modo.
%P4 ha un interfaccia per analizzare pacchetti e match campi nell'header. In questo modo abbiamo un accoppiamento tra hardware e software.
\\Un programma P4 è costituito da diverse sezioni \cite{p4Article}, ognuna delle quali descrive un aspetto specifico del trattamento dei pacchetti.
La dichiarazione degli header permette di specificare i protocolli che si vogliono analizzare e riconoscere o inventarne di nuovi per scopi di ricerca o esperimenti.
Questo include campi come indirizzi IP, numeri di porta e tutti i dati di protocollo.
Il Parser specifica come estrarre e interpretare i vari header dai pacchetti. Esso definisce uno stato macchina che determina, attraverso le informazioni in arrivo, come passare da uno stato all'altro. 
\\La Pipeline di Elaborazione comprende tabelle e azioni che definiscono come i pacchetti vengono processati dopo aver effettuato il parsing.
%guarda se mettere separato tabelle e azioni
I Controlli del flusso coordinano il parser, le tabelle di elaborazione e le azioni. Definiscono quindi i controlli necessari per il flusso corretto di un pacchetto.
\\Dopo aver definito il programma esso viene mandato al compilatore P4 che genera due tipi di output. 
Il primo è un file binario P4 ed è ciò che viene installato all'interno del dispositivo target.
Questo file binario è specifico per un target e quindi per uno specifico hardware (ad esempio Asics, fpga..).
Il secondo output è indipendente dal target ed è diretto verso la parte NorthBound del controller. Questo è chiamato P4 info e genera i metadati necessari per consentire al piano di controllo e al piano
dati di comunicare attraverso P4 Runtime.
\\P4Runtime è un API messa a disposizione da P4 che permette al controller di connettersi ai dispositivi, vedere cosa c'è attualmente nel pipeline 
e poter mandare le configurazioni rilevanti nella relativa tabella. Permette inoltre di definire il piano dati in modo dinamico collegandolo al piano di controllo. 
Per il piano di controllo, P4Runtime protegge i dettagli hardware del piano dati ed è indipendente dalle funzionalità e dal protocollo supportati.
P4Runtime riesce quindi a raggiunge l'indipendenza dal target, dal pipeline e dal protocollo.
\\I nodi programmabili che possono essere ottenuti tramite software o hardware sono definiti P4 target.
Essi hanno una pipeline di elaborazione dei pacchetti la cui struttura è specifica per il target ed è descritta da un determinato modello di architettura.

%E' lo schema del pipeline descritto al controller in modo da nascondere il tipo di dispositivi presenti nella rete.
I principali obiettivi quindi includono:
\begin{itemize}
    \item \textbf{Protocollo-indipendenza}: P4 non è vincolato a nessun protocollo specifico consentendo addirittura di definire nuovi protocolli o modificare quelli esistenti
    \item \textbf{Target-indipendenza}: Il codice può essere compilato per una varietà di target sia hardware che software
    \item \textbf{Riprogrammabilità}: permette di aggiornare e modificare il comportamento del dataplane in tempo reale rispondendo ai cambiamenti nei requisiti di rete.
\end{itemize}
P4 rappresenta un passo significativo verso reti più flessibili e programmabili, consentendo agli sviluppatori di adattare e innovare rapidamente in risposta ai cambiamenti nei requisiti di rete.
Si propone come una soluzione innovativa e versatile per superare le limitazioni degli attuali protocolli e dispositivi di rete offrendo un linguaggio dinamico e indipendente dall'hardware.


\textbf{Servizio end-to-end}
\\Come già detto in precedenza il Device Level sfrutta una South-Bound Interface (SBI) per interagire con i device tramite l'API P4Runtime. 
Inizialmente, il codice p4 compilato, ossia i vari artefatti, viene copiato nel pod SBI per poter inserire le giuste configurazioni nelle tabelle dei dispositivi.
Il passo successivo è registrare i dispositivi e i link al controller SDN per permettere una corretta comunicazione tra di essi.
A questo punto, siamo in grado di richiedere una connessione tra due end points specificando solamente i dispositivi finali tramite un servizio.
\\La creazione del servizio è realizzata in due passi. 
Inizialmente, tramite la funzione \textit{CreateService}, si crea una servizio di connettività vuoto nel quale viene specificato solo il tipo associato per poi ritornare l'identificativo a cui è stato correlato.
Successivamente viene aggiornato il servizio popolando i campi richiesti come endpoints, vincoli e configurazioni di servizio. Di questo si occupa la funzione \textit{UpdateService} \cite{D32}.
La componente di Service a questo punto si rivolge prima alla componente di Context per recuperare la versione più aggiornata del servizio e settare lo stato a Planned (pianificato), poi alla PathComp per calcolare un percorso.
\\Per eseguire questa operazione la PathComp utilizza delle informazioni della rete che risiedono nel database logicamente centralizzato della componente di Context.
La PathComp è in grado di soddisfare anche richieste di servizio che attraversano più livelli, in questo caso, la risposta includerà uno o più sottoservizi con le relative sottoconnessioni che li supportano.
\\Una volta ricevute le connessioni ed eventuali sottoservizi la componente Service istanzia un \textit{Task Scheduler} con le relative funzioni, %che li correla e li schedula.
quest'ultimo è responsabile dell'esecuzione delle attività di installazione e smantellamento dei servizi e collegamenti nell'ordine appropriato.
Infine viene eseguito il metodo \textit{Execute} del \textit{Task Scheduler} per realizzare tutte le operazioni di configurazione richieste per i dispositivi lungo il percorso tramite l'SBI; inoltre viene modificato lo stato del servizio.
Al termine del processo, la componente Context viene aggiornata con le nuove informazioni e l’identificatore del servizio viene restituito all’entità chiamante.
\\Per mantenere questo processo agnostico rispetto ai dettagli della tecnologia, la componente di Service 
sfrutta una definizione minima permettendo agli utenti di esprimere cosa vogliono connettere, lasciando che sia il sistema sottostante a decidere come realizzare la connessione reale.
La componente traduce automaticamente questa definizione minimale del servizio in modelli di configurazioni astratte dei dispositivi. 
Queste vengono a loro volta tradotte in regole P4 dal driver del dispositivo P4 della SBI.
\\\textbf{Vincoli e configurazioni di servizio}
\\È possibile richiedere azioni supplementari, come l'aggiunta di vincoli o configurazioni di servizio specifiche. 
%In questo modo il componente Policy offre SLA basati sugli eventi per servizi di connettività end-to-end tramite pipeline P4
Queste vengono specificate inizialmente alla creazione di un servizio e devono essere rispettate finchè quest'ultimo non verrà eliminato. 
Ciò semplifica la gestione dei servizi, in quanto la dichiarazione di queste informazioni aggiuntive può sostituire l'associazione di una politica.
\\Nella demo presa in considerazione i vincoli non erano specificati ma sono stati aggiunti per quanto riguarda la latenza e la capacità del percorso \ref{fig:constraints}.
Questa aggiunta non ha prodotto cambiamenti nella definizione del percorso poiché le funzioni relative all'effettivo funzionamento di queste funzionalità non 
sono ancora state implementate nel controller ma sono solo pianificate per release future.
Altri esempi di vincoli possono essere rappresentati dalla posizione di un dispositivo, che può essere sia terminale che non, dal tempo o dal numero di passi massimo di un determinato percorso.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{servizion constraints.png}
    \caption{Servizio creato in fase di sperimentazione}
    \label{fig:constraints}
\end{figure}
%spiegare cosa sono i constraits, guarda i deliverable
%dire che non sono implementati
%run-time-->loss+latenza
\\\textbf{Politica}
\\Una parte fondamentale nei sistemi moderni è la gestione a run-time del servizio stabilito\cite{demo}.
A tale scopo si sfrutta la componente di Monitoring che permette di associare il monitoraggio delle metriche nel proprio database
con condizioni che devono essere rispettate.
Quando questi requisiti non vengono soddisfatti, la componente di Monitoring solleva un allarme che fa scattare l'azione prestabilita.
\\Nella sperimentazione abbiamo introdotto una politica per il servizio stabilito con tre condizioni: una per la latenza (che non deve superare i 100ms), una per la loss (che non deve superare il 5\%) 
e infine una per la capacità\ref{fig:policy}.
Queste tre condizioni sono legate tra loro tramite un "OR", quindi appena una di esse non è più rispettata viene invocata l'azione che in questo caso consiste nel ricalcolo del percorso.
%MODIFICA O LA FOTO O LA DESCRIZIONE
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{policy 5 100.png}
    \caption{Politica creata in fase di sperimentazione}
    \label{fig:policy}
\end{figure}

\section{Demo}
Per verificare il comportamento del controller e assicurarsi che le condizioni di latenza, di loss e di capacità vengano rispettate, 
non avendo a disposizione una rete reale, abbiamo utilizzato una topologia di rete su Mininet di switch P4 basati su bmv2\cite{bmv2}.
La topologia iniziale era composta da 4 switch e due possibili percorsi ed è stata poi ampliata a 8 switch con 5 percorsi per renderla più complessa e poter fare una sperimentazione più realistica.
L'ultima topologia utilizzata è stata Abilene \cite{abilene}; una rete di trasporto creata da Internet2.
%mettere foto topologia abilene
\\I dispositivi delle varie topologie sono stati connessi alla componente SBI, come per i dispositivi reali, in modo tale che potessero comunicare con il controller.
Successivamente per dimostrare la politica basata sul servizio e il ricalcolo delle configurazioni, abbiamo aggiunto artificialmente delle condizioni di ritardo e packet loss all’interno di uno switch presente nel percorso. 
Ad esempio, abbiamo utilizzato il comando \textit{switch2 tc qdisc add dev switch2-eth2 root netem delay 200ms} per aggiungere un ritardo di 200ms oppure il comando \textit{switch2 tc qdisc add dev switch2-eth2 root netem loss 10\%}
%aggiungere il comando per la capacità
 per simulare una perdita di pacchetti del 10\%. Inoltre, abbiamo disattivato un link tra due switch con il comando \textit{link switch6 switch7 down}.
\\Questi cambiamenti sono stati monitorati grazie a un probe in Python, una funzionalità che rileva lo stato di integrità delle istanze dell'applicazione.
Inizialmente, si avvia l'agent che, ascoltando gli eventi della componente di Context, crea tre Kpi Id differenti per loss, latenza e capacità ogni volta che viene creato un servizio \ref{fig:agent}.
\begin{figure}[h]
    \centering
   \includegraphics[width=0.5\textwidth]{create new event ag.png}
    \caption{Creazione delle due KPI id}
    \label{fig:agent}
\end{figure}
Successivamente si attiva un secondo script che riesce a monitorare le metriche di loss e latenza attraverso il comando ping del terminale e la capacità tramite iperf per poi inviarle attraverso una socket all'agent.
Prima di avviare lo script è necessario attivare il server iperf sul dispositivo finale del servizio.
\\L'agent, ottenuti i relativi valori, li inoltra alla componente di monitoring tramite una KPI composta dal valore, il kpi id corretto e un timestamp.
\\La componente, ricevute le metriche, riconosce che è avvenuto un cambiamento nella rete e solleva un allarme per una potenziale violazione della politica che sarà poi validata dalla componente di Policy.
Questo evento causa l'esecuzione dell'azione predefinita nella politica, ovvero l'aggiornamento del percorso.
Quando il nuovo tragittto è calcolato dalla Path Comp, la componente di Service compila una lista di configurazioni di dispositivi per validare gli aggiornamenti 
seguita da un'altra lista per la cancellazione delle vecchie configurazioni. Queste istruzioni sono tradotte in regole P4 dalla componente SBI prima di essere imposte al piano dati 
tramite il P4Runtime.
\\In tutte e tre le tipologie di alterazione della rete, si è riscontrato un cambiamento del percorso volto a rispettare le condizioni desiderate. 
%Inoltre, si è notato che una volta rimosso il ritardo, la perdita di pacchetti, o riattivato un link, il controller percepisce questo cambiamento e modifica automaticamente il percorso precedente, poiché è memorizzato nel database della Path Comp come percorso preferito per le sue caratteristiche.
\section{Esperimento 2}
8 sw, latenza pkt loss
cambio tolopogia, aggiunta link, esteso probe, aggiunta porte per mininet
\section{Esperimento 3}
cambio topologia, nuovo script mininet (preso da internet con cambio bw), aggiunte porte,
aggiunto iperf, aggiunta capacità 

