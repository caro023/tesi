\chapter{TeraFlow: architettura e gestione degli intenti}
\label{cap:teraflow}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{architetturatf.png}
    \caption{Architettura di TeraFlow \cite{archtfs}}
    \label{fig:tfs}
\end{figure}
Il controller SDN su cui ci focalizzeremo è TeraFlow \cite{TeraFlow}, una piattaforma innovativa e recentemente proposta, sviluppata da un'ampia comunità open source nell'ambito di un progetto europeo. 
\\Infatti, è stato finanziato dall'unione europea per il programma di ricerca e innovazione Horizon 2020 \cite{Horizon} e supportato dal 5G PPP \cite{5GPPP}, un'iniziativa congiunta tra la Commissione Europea e l'industria europea delle telecomunicazioni.
%Nonostante la quantità di controller SDN i fondatori di TeraFlow hanno riscontrato che molte delle soluzioni attuali sono costituite da un software monolitoco
%in grado di sincronizzarsi con altri controller SDN distribuiti tramite protocolli specifici.
%Il problema principale dei controller SDN `e il basso numero di contribuzioni che negli ul-timi anni o mesi si stanno sempre di pi`u diradando
Nonostante la quantità di controller SDN i fondatori di TeraFlow hanno riscontrato come problema comune il calo delle contribuzioni ai progetti negli ultimi anni. 
Questo declino mette a rischio il continuo sviluppo e supporto per i nuovi bisogni e requisiti delle reti moderne lasciando molte soluzioni esistenti inadeguate per affrontare le sfide emergenti.
\\L'obiettivo di TeraFlow è implementare un controller che soddisfi i requisiti attuali ed eventualmente futuri, sia architetturali che infrastrutturali, per le reti.
Il controller mira a migliorare le capacità di elaborazione dei flussi permettendo di gestire un volume di traffico equivalente a un terabit al secondo. 
Questa capacità è cruciale per supportare le elevate esigenze di connettività delle reti B5G.
%TeraFlow è un progetto OpenSource 
%al quale tutti i membri della comunità ETSI (European Telecommunications Standard Institute) \cite{etsi} possono contribuire. 
\\Un ulteriore obiettivo è ridurre il divario tra le esigenze delle industrie e le capacità offerte dagli standard SDN.
Questo controller, attualmente in fase di sviluppo, sarà progettato per integrarsi con gli attuali framework NFV e MEC.
Inoltre, si prevede che supporti l'integrazione delle 
apparecchiature di rete ottica e a microonde, e che sarà compatibile con altri controller come ONOS, ma anche con istanze multiple di TeraFlow che gestiscono diversi domini, al fine di sfruttare funzionalità avanzate e facilitare l'interoperabilità con altre reti.
\\A differenza dei controller presentati precedentemente TeraFlow è stato progettato con un'architettura cloud-native, pensata per sfruttare appieno gli ambienti cloud.
Questo approccio consente una maggiore flessibilità e scalabilità rispetto ai sistemi tradizionali grazie alla suddivisione delle applicazioni in microservizi che possono essere gestiti, distribuiti e aggiornati in modo indipendente.
Ciò differenzia TeraFlow dai controller modulari tradizionali che non sono stati progettati con la stessa capacità di adattamento alle moderne richieste delle reti.
\\L'uso di container, una tecninca di virtualizzazione leggera, permette di isolare ogni microservizio utilizzando una minore quantità di risorse rispetto alle macchine virtuali (VM).
Ciascun microservizio interagisce con gli altri attraverso la connessione di rete rendendo il controller disaggregato.
L'ambiente creato da un container include sia il codice di esecuzione che le sue dipendenze. %codice del microservizio
Questi container sono gestiti tramite Kubernetes, un orchestratore responsabile dell'allocazione delle risorse in termini di capacità di calcolo, memoria e archiviazione\cite{arttfs}.
Kubernetes offre diverse funzionalità che garantiscono dinamicità, autoriparazione, integrità e bilanciamento del carico \cite{D53} \cite{D14}.
%\\Ogni container, o componente, ha delle responsabilità specifiche e definisce un microservizio che 
Le componenti principali del sistema sono implementate in Java (solo quelle di Automation e Policy) e Python e l'ambiente è sviluppato presso la sede del CTTC a Barcellona. 
%\\TeraFlow si propone di affrontare le sfide delle reti moderne attraverso un'architettura innovativa e la collaborazione attiva della comunità. 
\\L'obiettivo di TeraFlow è sviluppare un controller SDN Carrier Grade (reti o infrastrutture ben testate con livelli estremamente elevati di affidabilità, ridondanza e sicurezza) per le reti B5G che automatizzi la gestione della rete e sia in grado di scalare per gestire miliardi di dispositivi.
\\TeraFlow mira a ottimizzare l'uso delle risorse di rete per migliorare l'efficienza energetica e ridurre i costi operativi.
\\Per gestire la configurazione di rete, TeraFlow utilizza una componente chiamata Context che memorizza la configurazione di rete, inclusi topologie, dispositivi, 
collegamenti e servizi, in un database No-SQL\cite{D31}. Questa componente garantisce la coerenza dei dati gestiti dai vari componenti del controller SDN.
\\La componente di Monitoring gestisce le diverse metriche configurate per le apparecchiature e i servizi di rete memorizzando i dati di monitoraggio relativi alle Key Performance Indicators (KPI) selezionate.
I servizi sono progettati per essere semplici e dettagliati, ciò è reso possibile grazie all'uso di protocolli leggeri.
Dal punto di vista della sicurezza il sistema utilizza un approccio basato sul Machine Learning per la prevenzione e la mitigazione degli attacchi.
\\Alcuni dei requisiti funzionali del controller sono rappresentati da \cite{D22}:
\begin{itemize} 
    \item \textbf{usabilità}: realizzata grazie a un'interfaccia utente web (web UI) che consente la configurazione di servizi predefiniti e visualizzazione personalizzabile delle metriche.
    \item \textbf{scalabilità}: è intrinseca nel design del controller con la replicazione automatica dei miscroservizi per gestire elevati volumi di richieste in ingresso.
    \item \textbf{affidabilità}: garantita attraverso robusti meccanismi di monitoraggio che supervisionano lo stato dei miscroservizi e dei flussi, attivando automaticamente dei processi di ripristino se necessari.
\end{itemize}
Questo progetto riveste un ruolo chiave nel panorama delle tecnologie 5G, contribuendo a unire diverse università e istituti di ricerca per sviluppare soluzioni all'avanguardia
lavorando con organismi di standardizzazione per garantire l'adozione su scala globale.
%L'architettura è basata su microservizi interconnessi da un bus grpc.
\section{Componenti architetturali}
Le componenti di TeraFlow sono classificate in due categorie; le componenti principali del sistema operativo e le netapp sovrapposte \cite{Component}. 
Le componenti del sistema operativo di rete (Network Operating System - NOS) formano la base dell'infrastruttura di TeraFlow, offrendo servizi di connettività per infrastrutture di rete programmabili avanzate \cite{Component}. %sistema operativo si riferisce al controller e le sue componenti principali-> gestisce e coordina le funzioni di rete
Queste infrastrutture sono basate su tecnologie come P4 (Programming Protocol-independent Packet Processors), OpenConfig e TAPI (Transport API), che forniscono modalità flessibili e programmabili per gestire il traffico di rete e le configurazioni.
\\Le netapp sovrapposte sono applicazioni che operano sopra il livello del NOS, sfruttando le interfacce di comunicazione, indipendentemente dal linguaggio di programmazione, per interagire con le infrastrutture di rete.
Queste applicazioni possono includere strumenti per il monitoraggio, la gestione della qualità del servizio, e altre funzionalità.
\\Per garantire l'interoperabilità tra le diverse componenti, TeraFlow utilizza un bus gRPC (Google Remote Procedure Call) come protocollo interno, 
un framework che consente la comunicazione tra servizi in modo efficiente e preciso. 
%Questo protocollo è stato preferito a REST per la sua capacità più concreta di descrivere le interazioni tra entità in modo efficiente e preciso.
gRPC sfrutta i Protocol Buffers per definire in modo preciso gli schemi dei messaggi condivisi e scambiati tra le componenti. %,consentendo ai vari servizi di implementare le adeguate funzioni di comunicazione indipendentemente dalla piattaforma e dal linguaggio.
I Protocol Buffers utilizzati sono dettagliati nella pagina dedicata del sito \cite{ProtoBuf}.
\\TeraFlow fornisce, inoltre, funzionalità di automazione SDN e IBN avanzate basate su policy e parti interessate. Nello specifico, TeraFlow Automation sfrutta 
importanti eventi di sistema per realizzare la (ri)configurazione di servizi e dispositivi in modo zero-touch, ossia minimizzando l'intervento umano attraverso l'automazione completa della configurazione e della gestione dei servizi e dei dispositivi.
Questo approccio contribuisce significativamente alla riduzione delle spese operative, poiché riduce la necessità di interventi manuali e accelera i processi di configurazione e manutenzione.\cite{Component}.
\\Le componenti di TeraFlow, come si può vedere dalla figura \ref{fig:componenti}, sono suddivise in diversi livelli di astrazione per facilitare la gestione della rete:
\begin{itemize}
\item Device Level Abstraction: fornisce un'astrazione dei dispositivi fisici presenti, consentendo al controller di interagire con diverse tipologie di hardware.
\item Service Level Abstraction: focalizzato sulla gestione e configurazione dei servizi, questo livello gestisce le interazioni con l'SBI (Southbound Interface) per fornire un'interfaccia unificata.
\item Management Level Abstraction: realizza la gestione complessiva della rete, che include il monitoraggio, il controllo e la manutenzione dei servizi e dei dispositivi gestiti tramite SBI, garantendo un controllo centralizzato dell'infrastruttura.
\end{itemize}




\subsection{Device Level Abstraction}
Il Device Level Abstraction permette l'interazione con i dispositivi presenti all'interno della rete e corrisponde all'Infrastructure layer descritto nell'architettura SDN.
\\Una componente fondamentale per questo livello di astrazione è la componente SBI.
Per far comunicare più tipi di device possibili con il controller, la componente presenta eterogeneità offrendo supporto a differenti protocolli.
Il suo compito principale è stabilire una connessione con i dispositivi per integrarli nell'ecosistema del controller
e essere in grado di configurarli dinamicamente a tempo di esecuzione \cite{D32}. 
Dopo aver stabilito la connessione con un determinato dispositivo e aver verificato la disponibilità,
la componente offre una API che consente di inviare le configurazioni scelte dal controller al dispositivo sotto forma di file JSON.
In questo modo, il dispositivo riceve istruzioni per configurare i suoi parametri operativi in linea con le politiche definite a livello di rete. 
%Il controller provvederà a inserire le giuste entry nelle tabelle degli switch o router pertinenti.
%Per questo viene introdotta la componente di monitoring che servirà da supporto per il Management-level Abstraction.
%che interagisce con la il network equipment attraverso pluggable drivers.
%OpenFlowOpenConfig-based routers
\\Un'altra componente che fa parte di questo livello è quella di Monitoring il cui compito è offrire supporto al Management-level.
E' essenziale per l'automazione dei servizi e per prendere le decisioni in tempo reale sulla base di eventi.
Questa componente interagisce con i dispositivi per catturare lo stato della rete attraverso delle KPI (Key Performance Indicators) persistendo le informazioni all'interno di un database (Metrics Database) così da poter fornire dati dimensionali 
con serie temporali visualizzabili su Grafana \cite{grafana}. %necessari per l'esposizione e l'utilizzo da parte delle altre componenti.
Quando un valore KPI registrato supera determinate soglie predefinite, la componente di Monitoring utilizza il sistema di gestione degli eventi del controller, un'API dedicata alla gestione e distribuzione di notifiche riguardanti eventi di rete
per generare e notificare un allarme. Questo allarme viene inviato alla componente responsabile della gestione di quel servizio specifico, ossia quella che ha
originariamente richiesto il monitoraggio della specifica metrica all'interno del controller.
Questa API permette al controller di notificare in tempo reale le componenti del sistema (ad esempio, quelle di gestione o sicurezza) sugli eventi critici, consentendo interventi tempestivi e automatizzati \cite{D32}.
\\Ogni evento è composto da una KPI che identifica la regola a cui si riferisce, un timestamp e un KPIValue, che rappresenta il valore della metrica monitorata in tempo reale richiesta dalla KPI al momento specificato dal timestamp.
Supponendo che la componente di Monitoring stia monitorando la latenza di un collegamento di rete tramite una KPI denominata "Latency" e che la latenza massima accettabile sia 100ms,
se la latenza supera questa soglia, ad esempio 120ms, viene generato un evento così composto: 
\begin{itemize}
    \item KPI: "Latency"
    \item Timestamp: "06-09-2024 14:35:20" (la data e l'ora a cui si è verificato l'evento)
    \item KPIValue: 120 ms
\end{itemize}
Per il corretto funzionamento la componente di Monitoring deve essere in grado di recuperare le metriche da tutti i diversi dispositivi monitorati.
Questi implementano spesso protocolli diversi per notificare le KPI, per questo motivo sono stati inclusi una serie di sottomoduli che si connettono 
agli elementi monitorati utilizzando i protocolli SBI necessari. %I dati estratti vengono memorizzati nel Metrics Database
Tuttavia quanto si tratta di gestire topologie più complesse che coinvolgono molti dispositivi è necessario un livello di astrazione superiore per specificare la connessione tra vari end-points. A tal fine si introduce il Service-level. 

\subsection{Service Level Abstraction}
Il Service Level è responsabile della creazione e dell'aggiornamento dei servizi di rete.
Questo livello di astrazione permette agli utenti di definire intenti specifici per la connessione tra gli end-points attraverso la componente di Service.
Per svolgere questo compito, la componente si avvale di diverse funzionalità offerte dalle altre parti del sistema.
In particolare, per il calcolo dei percorsi di rete, si affida alla componente PathComp (Path Computation).
Ad esempio, se un intento richiede la creazione di una connessione con una latenza minima tra due nodi specifici, 
la componente invia una richiesta a PathComp, che calcola il percorso ottimale basato su parametri come la larghezza di banda o la latenza.
Una volta ricevuta la risposta, la componente di Service utilizza uno scheduler per configurare i dispositivi di rete lungo il percorso selezionato, utilizzando le connessioni restituite dalla PathComp \cite{D32}.
Queste regole vengono poi propagate all'interfaccia Southbound (SBI) attraverso dei file JSON, consentendo una configurazione automatizzata della rete e permettendo di astrarre la complessità del livello sottostante all'utente.
%Un utente può selezionare un path scelto o chiedere al controller di determinarne uno che rispetti determinati requisiti. 
\\La componente di Service supporta diversi tipi di servizi ed è in grado di utilizzare vari protocolli per configurare i dispositivi di rete.
Implementa inoltre una Service Handler API che consente agli operatori di rete di definire i comportamenti necessari per ciascun tipo di servizio \cite{D32}.
\begin{itemize}
    \item \textbf{L2-VPN}: servizio per dispositivi OpenConfig
    \item \textbf{L3-VPN}: servizio per dispositivi emulati o OpenConfig con supporto per ACLs
    \item \textbf{Connectivity}: servizio per dispositivi TAPI
    \item \textbf{L2 service Handler}: servizio per dispositivi P4
    \item \textbf{Microwave service Handler}
\end{itemize}
%La componente descritta ha al suo interno un blocco gRPC che espone una NBI per consentire l'interazione con le altre componenti del controller.  
%Gestisce i vari servizi inviando richieste di calcolo del percorso alla PathComp e successivamente, grazie a uno scheduler, si occupa di
%configurare i dispositivi in base alle connessioni restituite. Questo permette di astrarre la complessità del livello sottostante all'utente
%in quanto la componente è in grado di tradurre l'intento in un insieme di regole che vengono propagate all'SBI attraverso dei file JSON.
Un'altra componente che fa parte di questo livello è la PathComp. Questa, come accennato in precedenza, si occupa di gestire la selezione del percorso tra gli end-points per i servizi di connettività di rete. 
Riceve richieste dalla componente di Service e, interagendo con la componente di Context, recupera le informazioni sulle topologie sottostanti al fine di creare
percorsi che soddisfino i requisiti del servizio di rete richiesto.
La PathComp rappresenta un'entità singola e specializzata dove possono essere ospitati diversi algoritmi. Questo permette che qualsiasi nuovo algoritmo utilizzato non impatti su altre componenti del controller.
Per confrontare i percorsi viene utilizzato inizialmente un algoritmo K-SP dove i k percorsi sono ordinati per numero di passi (hop), ritardo end-to-end e larghezza di banda disponibile sul link più congestionato \cite{D53}. 
%La specifica "regolare" sottolinea che l'algoritmo non tiene conto di ottimizzazioni aggiuntive, come ad esempio la minimizzazione del consumo energetico lungo il percorso, che invece sarebbe un obiettivo nell'approccio comparativo descritto nel documento citato.
%Una volta scelto il percorso viene calcolato il risultato della power path con l'approccio EAR.
%Per astrarre la complessità del livello sottostante all'utente la componente di service è in grado di tradurre l'intento in un insieme di regole che propaga all'SBI (sempre attraverso file JSON)
%cosicchè venga configurato ogni device all'interno del percorso per stabilire la connessione.
\\Il service layer permette al controller di tradurre gli intenti in regole concrete che vengono poi propagate al livello sottostante tramite l'SBI.
Tuttavia, da solo non ha la capacità di rispondere dinamicamente agli eventi che si verificano nella rete in tempo reale, come il cambiamento dello stato di un collegamento o di una risorsa di rete. %congestionamento o guasto
Per essere in grado di creare, aggiornare o cancellare i servizi di rete in base a tali eventi è necessario introdurre un ulteriore livello di astrazione che automatizzi queste operazioni: il Management Level Abstraction.

\subsection{Management Level Abstraction}
Questo livello di astrazione è stato introdotto per consentire l'interazione dinamica con la componente di Service permettendo la creazione, l'aggiornamento o l'eliminazione automatica di un servizio in risposta agli eventi provenienti dalla rete.
In altre parole, il sistema non si limita a configurare i dispositivi solo su input manuali o su richiesta esplicita, ma è in grado di adattarsi dinamicamente ai cambiamenti dello stato della rete in tempo reale. 
Questo livello consente dunque al controller di reagire rapidamente a eventi come congestioni, guasti o modifiche nella topologia, garantendo che la rete mantenga un alto livello di efficienza e affidabilità.
%\\Una delle componenti principali è la componente di Monitoring. Essa ha il compito di raccogliere metriche da diverse componenti
%monitorate attraverso moduli che si interfacciano con i relativi protocolli.
%I dati estratti vengono memorizzati in un Metrics Database che supporta meccanismi di aggregazione per analizzare le informazioni e consente di monitorare l'evoluzione dei dati nel tempo.
%Quando una KPI supera una soglia definita precedentemente, il sistema genera un allarme.
%Questo allarme viene inviato alla componente responsabile della gestione di quel servizio specifico, ossia alla componente che ha originariamente richiesto il monitoraggio della specifica metrica all'interno del controller.
\\Una delle componenti fondamentali è la componente di Policy, che interagisce strettamente con la componente Monitoring descritta precedentemente.
\\Si occupa di definire condizioni di politica che possono essere applicate sia a livello di singoli dispositivi che a livello di dominio della rete.
Le politiche possono includere più regole collegate tra loro tramite condizioni logiche di AND/OR \cite{D32}, ognuna delle quali genera una KPI specifica da far monitorare alla componente di Monitoring.
\\Ogni regola è composta da una KPI che la identifica, un operatore numerico di confronto, e un KPIValue.
Quest'ultimo, in combinazione con l'operatore numerico, definisce l'intervallo di valori accettabili o non accettabili per quella specifica metrica.
\\Se le regole definite nella politica vengono soddisfatte la componente di Monitoring genera un allarme che viene inviato, in questo caso, alla componente di Policy, che reagisce attivando le azioni predefinite.
Ad esempio, se si verifica un eccesso di latenza, la componente di Policy può attivare il ricalcolo del percorso tramite la componente di Service o altre azioni correttive.
%Tutte queste regole vengono archiviate nel Metrics Database che fornisce anche l'evoluzione dei dati nel tempo.
%Un'ulteriore componente di questo livello è quella di Monitoring che quando rileva che una condizione impostata in una politica viene soddisfatta (ad esempio, un valore di latenza supera una soglia prestabilita), essa genera un allarme.
%Questo allarme viene inviato alla componente responsabile della gestione di quel servizio o flusso specifico all'interno del controller, che è in grado di eseguire l'azione predefinita specificata nella politica.
%Ad esempio, nel caso di un eccesso di latenza, l'allarme viene inviato alla componente di Service, la quale può poi attivare il ricalcolo del percorso o altre azioni correttive.
\\Questo meccanismo consente al controller di reagire in modo efficace agli eventi e di ripristinare uno stato desiderato per i dispositivi.
\\La componente Policy utilizza il Context Database per identificare quali dispositivi o servizi sono coinvolti nella politica.
Se viene fornito l'ID del servizio, la componente recupera i dispositivi associati a esso, in caso contrario la componente scansiona una lista di dispositivi per individuare quelli che devono rispettare le regole impostate. 
\\Una regola di politica può avere vari stati:
\begin{itemize}
    \item \textbf{inserted} (inserita)
    \item \textbf{validated} (convalidata)
    \item \textbf{provisioned} (provvista)
    \item \textbf{actively enforced} (attivamente applicata)
    \item \textbf{failed} (fallita)
    \item \textbf{updated} (aggiornata)
    \item \textbf{removed} (rimossa)
\end{itemize}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{diagramma policy.png}
    \caption{Macchina a stati interna alla componente di policy \cite{D32}}
    \label{fig:diagramma}
\end{figure}
%La componente di policy utilizza inoltre il paradigma publish-subscribe per associare dinamicamente condizioni di politica agli eventi rilevati
%che richiedono azioni immediate.
Grazie all'integrazione tra la componente di Policy e Monitoring, TeraFlow ha reso possibile l'associazione delle condizioni di politica con gli allarmi del sistema di Monitoring.
In questo modo può implementare una gestione dinamica della rete, in grado di rispondere in tempo reale agli eventi che possono degradare la qualità del servizio, come l'aumento della latenza o la perdita di pacchetti. 
Questa reattività permette di mantenere gli SLA (Service Level Agreements) senza necessitare di interventi manuali continui da parte degli operatori aumentando l'efficienza delle varie operazioni e riducendo la complessità operativa.
In ambienti su larga scala, come le reti distribuite composte da microservizi, questa capacità di adattamento consente non solo di reagire agli eventi ma anche di prevenire potenziali criticità, ottimizzando continuamente le risorse di rete.
%Ad esempio, una politica potrebbe stabilire che, se la latenza di rete supera i 50 ms per più di 10 minuti, il sistema deve ricalcolare il percorso per ridurre la congestione. 
%In questo caso, la componente Monitoring rileva l’aumento della latenza, genera un allarme e lo invia alla componente Policy. Quest'ultima, verificando che le condizioni della regola 
%siano soddisfatte, attiva la componente Service per ricalcolare automaticamente il percorso e riconfigurare i dispositivi coinvolti.

%Si occupa di creare un service level agreement (SLA) per uno specifico servizio identificato tramite un id.
%A questo punto verrà creato un evento e si eseguiranno delle azioni prestabilite dal controller fino ai dispositivi per gestire i problemi o cambiare il servizio. 
%Solitamente le azioni contemplano l'interazione con altre componenti al livello inferiore.
%Un'esempio può essere una notifica alla componente di Service che si occuperà di modificare il percorso identificato precedentemente per far rispettare nuovamente i requisiti.

\section{gRPC} \label{grpc}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{grpc.png}
    \caption{Funzionamento del protocollo gRPC \cite{librogrpc}}
    \label{fig:grpc}
\end{figure}
RPC (Remote Procedure Call) era un popolare protocollo inter-processo\cite{librogrpc}.
Esso utilizza uno scambio di messaggi con le caratteristiche di basso sovraccarico, semplicità e trasparenza.
Per questi motivi è stato ampiamente usato per diverso tempo in sistemi applicativi distribuiti.
Con RPC un client può invocare da remoto una funzione di un metodo.
Un utente di RPC non può distinguere dove viene eseguita la procedura chiamata, infatti viene chiamata come una procedura locale \cite{grpcArt2}.
Tuttavia, la maggior parte delle convenzionali implementazioni (un esempio è RMI) sono complesse dato che richiedono una gestione diretta dei protocolli di trasporto, come TCP, che ostacolano l'interoperabilità e richiedono specifiche eccessive \cite{librogrpc}.
\\Google Remote Procedure Calls (gRPC\cite{grpc}) è un framework OpenSource sviluppato da Google per facilitare la comunicazione tra applicazioni distribuite. Permette di
connettere, invocare, operare e fare debug di programmi eterogenei in modo semplice.
\\gRPC è basato sul protocollo di trasporto HTTP/2 che supporta la comunicazione bidirezionale. 
Sebbene anche gRPC utilizzi TCP la differenza con i tradizionali protocolli RPC risiede nelle caratteristiche avanzate di HTTP/2, che includono funzionalità come il multiplexing delle connessioni, il flusso bidirezionale 
e una migliore gestione delle prestazioni. 
\\gRPC include supporto per il bilanciamento del carico, il tracciamento, il controllo dello stato e l'autentificazione\cite{grpcArt3} \cite{grpcArt1}.
Inoltre, ha la capacità di supportare alte performance di streaming in modalità push da parte del server, consentendo l'indipendenza basata sul set di dati e definizioni, grazie ai metodi YANG,
e la compressione di dati binari \cite{grpcArt3}.
\\Consente di definire i servizi, i loro metodi di comunicazione e trasportare messaggi attraverso dei file di descrizione dell'interfaccia detti Protocol Buffer, o più semplicemente file proto.
\\I file proto sono un meccanismo indipendente dal linguaggio e dalla piattaforma per la serializzazione delle strutture dati. Questo sistema di serializzazione è più efficiente rispetto a formati come JSON o XML
in termini sia di dimensione dei messaggi che di velocità di serializzazione e deserializzazione.
\\Per sviluppare un'applicazione gRPC è necessario definire un'interfaccia dei servizi. Questa contiene informazioni su come il servizio deve essere usato e quali sono i metodi, i parametri e 
il formato da utilizzare per i messaggi. Essa viene definita in un file proto, che a prima vista può sembrare un file di testo ordinario, come si può notare in figura \ref{fig:proto}, in realtà specifica i metodi con i parametri di input e i valori di ritorno.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{file proto.png}
    \caption{Esempio di Protocol Buffer file in TeraFlow \cite{ProtoBuf}}
    \label{fig:proto}
\end{figure}
Tutti i metodi nella definizione di questa interfaccia possono essere invocati dal client da remoto.
Una volta definita si può generare il codice lato server (server skeleton) e il codice lato client (client stub) nel linguaggio desiderato usando il compilatore Protobuf \textit{protoc}\cite{librogrpc}.
\\Negli esperimenti svolti in \cite{espgrpc} si è dimostrato che gRPC, grazie ai file proto, in scenari che coinvolgono ambienti più complessi con molte componenti (come server, DNS, firewall..)
può portare a una riduzione di quasi il 27\% nel tempo di creazione del server rispetto a REST (Representational state transfer), uno tra i più moderni e utilizzati sistemi di trasmissione dati, riducendo il tempo di esecuzione complessivo.
Ciò che è stato provato rende gRPC più adatto per architetture composte da microservizi, come TeraFlow, in cui le prestazioni e la scalabilità del sistema sono elementi critici.
\\Quando il client invoca un servizio gRPC, il lato client utilizza i protocol buffer per serializzare la chiamata di procedura remota nel formato appropriato. 
Successivamente, la richiesta viene mandata tramite HTTP/2. Sul lato server, viene deserializzata e viene invocata la relativa procedura usando i protocol buffers.
La risposta segue il flusso inverso da server a client\cite{librogrpc}.
\\Il framework gRPC sottostante gestisce tutta le complessità che normalmente sono associate all'imposizione di vincoli di servizio, serializzazione dati, comunicazioni di rete, autentificazione e molto altro.
gRPC è progettato per trasportare messaggi peer-to-peer in modo distribuito e non durevole, consentendo a più servizi di scambiarsi informazioni attraverso un bus condiviso.
Grazie all'uso di HTTP/2 e alla codifica orientata ai byte, gRPC riesce a introdurre bassa latenza, rendendolo adatto a sistemi altamente distribuiti e scalabili. 
Un'altra caratteristica importante è la sicurezza, infatti supporta nativamente il protocollo TLS (Transport Layer Security), garantendo che le comunicazioni tra i servizi siano cifrate e sicure.


