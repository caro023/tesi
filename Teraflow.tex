\chapter{Gestione degli intenti in Teraflow}
\label{cap:teraflow}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{architetturatf.png}
    \caption{Architettura di TeraFlow \cite{archtfs}}
    \label{fig:tfs}
\end{figure}
Il controller SDN su cui ci focalizzeremo è TeraFlow \cite{TeraFlow}, una piattaforma innovativa e recentemente proposta, sviluppata da un'ampia comunità open source nell'ambito di un progetto europeo. 
\\Infatti, è stato finanziato dall'unione europea per il programma di ricerca e innovazione Horizon 2020 \cite{Horizon} e supportato dal 5G PPP \cite{5GPPP}, un'iniziativa congiunta tra la Commissione Europea e l'industria europea delle telecomunicazioni.
%Nonostante la quantità di controller SDN i fondatori di TeraFlow hanno riscontrato che molte delle soluzioni attuali sono costituite da un software monolitoco
%in grado di sincronizzarsi con altri controller SDN distribuiti tramite protocolli specifici.
%Il problema principale dei controller SDN `e il basso numero di contribuzioni che negli ul-timi anni o mesi si stanno sempre di pi`u diradando
Nonostante la quantità di controller SDN i fondatori di TeraFlow hanno riscontrato come problema comune il calo delle contribuzioni ai progetti negli ultimi anni. 
Questo declino mette a rischio il continuo sviluppo e supporto per i nuovi bisogni e requisiti delle reti moderne lasciando molte soluzioni esistenti inadeguate per affrontare le sfide emergenti.
\\L'obiettivo di TeraFlow è implementare un controller che soddisfi i requisiti attuali ed eventualmente futuri, sia architetturali che infrastrutturali, per le reti.
Il controller mira a migliorare le capacità di elaborazione dei flussi permettendo di gestire un volume di traffico equivalente a un terabit al secondo. 
Questa capacità è cruciale per supportare le elevate esigenze di connettività delle reti B5G.
%TeraFlow è un progetto OpenSource 
%al quale tutti i membri della comunità ETSI (European Telecommunications Standard Institute) \cite{etsi} possono contribuire. 
\\Un ulteriore obiettivo è ridurre il divario tra le esigenze delle industrie e le capacità offerte dagli standard SDN.
Questo controller, attualmente in fase di sviluppo, sarà progettato per integrarsi con gli attuali framework NFV e MEC.
Inoltre, si prevede che supporti l'integrazione delle 
apparecchiature di rete ottica e a microonde, e che sarà compatibile con altri controller come ONOS, ma anche con istanze multiple di TeraFlow che gestiscono diversi domini, al fine di sfruttare funzionalità avanzate e facilitare l'interoperabilità con altre reti.
\\A differenza dei controller presentati precedentemente TeraFlow è stato progettato con un'architettura cloud-native, pensata per sfruttare appieno gli ambienti cloud.
Questo approccio consente una maggiore flessibilità e scalabilità rispetto ai sistemi tradizionali grazie alla suddivisione delle applicazioni in microservizi che possono essere gestiti, distribuiti e aggiornati in modo indipendente.
Ciò differenzia TeraFlow dai controller modulari tradizionali che non sono stati progettati con la stessa capacità di adattamento alle moderne richieste delle reti.
\\L'uso di container, una tecninca di virtualizzazione leggera, permette di isolare ogni microservizio utilizzando una minore quantità di risorse rispetto alle macchine virtuali (VM).
Ciascun microservizio interagisce con gli altri attraverso la connessione di rete rendendo il controller disaggregato.
L'ambiente creato da un container include sia il codice di esecuzione che le sue dipendenze. %codice del microservizio
Questi container sono gestiti tramite Kubernetes, un orchestratore responsabile dell'allocazione delle risorse in termini di capacità di calcolo, memoria e archiviazione\cite{arttfs}.
Kubernetes offre diverse funzionalità che garantiscono dinamicità, autoriparazione, integrità e bilanciamento del carico \cite{D53} \cite{D14}.
%\\Ogni container, o componente, ha delle responsabilità specifiche e definisce un microservizio che 
Le componenti principali del sistema sono implementate in Java (solo quelle di Automation e Policy) e Python e l'ambiente è sviluppato presso la sede del CTTC a Barcellona. 
%\\TeraFlow si propone di affrontare le sfide delle reti moderne attraverso un'architettura innovativa e la collaborazione attiva della comunità. 
\\L'obiettivo di TeraFlow è sviluppare un controller SDN Carrier Grade (reti o infrastrutture ben testate con livelli estremamente elevati di affidabilità, ridondanza e sicurezza) per le reti B5G che automatizzi la gestione della rete e sia in grado di scalare per gestire miliardi di dispositivi.
\\TeraFlow mira a ottimizzare l'uso delle risorse di rete per migliorare l'efficienza energetica e ridurre i costi operativi.
\\Per gestire la configurazione di rete, TeraFlow utilizza una componente chiamata Context che memorizza la configurazione di rete, inclusi topologie, dispositivi, 
collegamenti e servizi, in un database No-SQL\cite{D31}. Questa componente garantisce la coerenza dei dati gestiti dai vari componenti del controller SDN.
\\La componente di Monitoring gestisce le diverse metriche configurate per le apparecchiature e i servizi di rete memorizzando i dati di monitoraggio relativi alle Key Performance Indicators (KPI) selezionate.
I servizi sono progettati per essere semplici e dettagliati, ciò è reso possibile grazie all'uso di protocolli leggeri.
Dal punto di vista della sicurezza il sistema utilizza un approccio basato sul Machine Learning per la prevenzione e la mitigazione degli attacchi.
\\Alcuni dei requisiti funzionali del controller sono rappresentati da \cite{D22}:
\begin{itemize} 
    \item \textbf{usabilità}: realizzata grazie a un'interfaccia utente web (web UI) che consente la configurazione di servizi predefiniti e visualizzazione personalizzabile delle metriche.
    \item \textbf{scalabilità}: è intrinseca nel design del controller con la replicazione automatica dei miscroservizi per gestire elevati volumi di richieste in ingresso.
    \item \textbf{affidabilità}: garantita attraverso robusti meccanismi di monitoraggio che supervisionano lo stato dei miscroservizi e dei flussi, attivando automaticamente dei processi di ripristino se necessari.
\end{itemize}
Questo progetto riveste un ruolo chiave nel panorama delle tecnologie 5G, contribuendo a unire diverse università e istituti di ricerca per sviluppare soluzioni all'avanguardia
lavorando con organismi di standardizzazione per garantire l'adozione su scala globale.
%L'architettura è basata su microservizi interconnessi da un bus grpc.
\section{Componenti}
Le componenti di TeraFlow sono classificate in due categorie; le componenti principali del sistema operativo e le netapp sovrapposte \cite{Component}. 
Le componenti del sistema operativo di rete (Network Operating System - NOS) formano la base dell'infrastruttura di TeraFlow, offrendo servizi di connettività per infrastrutture di rete programmabili avanzate \cite{Component}. %sistema operativo si riferisce al controller e le sue componenti principali-> gestisce e coordina le funzioni di rete
Queste infrastrutture sono basate su tecnologie come P4 (Programming Protocol-independent Packet Processors), OpenConfig e TAPI (Transport API), che forniscono modalità flessibili e programmabili per gestire il traffico di rete e le configurazioni.
\\Le netapp sovrapposte sono applicazioni che operano sopra il livello del NOS, sfruttando le interfacce di comunicazione, indipendentemente dal linguaggio di programmazione, per interagire con le infrastrutture di rete.
Queste applicazioni possono includere strumenti per il monitoraggio, la gestione della qualità del servizio, e altre funzionalità.
\\Per garantire l'interoperabilità tra le diverse componenti, TeraFlow utilizza un bus gRPC (Google Remote Procedure Call) come protocollo interno, 
un framework che consente la comunicazione tra servizi in modo efficiente e preciso. 
%Questo protocollo è stato preferito a REST per la sua capacità più concreta di descrivere le interazioni tra entità in modo efficiente e preciso.
gRPC sfrutta i Protocol Buffers per definire in modo preciso gli schemi dei messaggi condivisi e scambiati tra le componenti. %,consentendo ai vari servizi di implementare le adeguate funzioni di comunicazione indipendentemente dalla piattaforma e dal linguaggio.
I Protocol Buffers utilizzati sono dettagliati nella pagina dedicata del sito \cite{ProtoBuf}.
\\TeraFlow fornisce, inoltre, funzionalità di automazione SDN e IBN avanzate basate su policy e parti interessate. Nello specifico, TeraFlow Automation sfrutta 
importanti eventi di sistema per realizzare la (ri)configurazione di servizi e dispositivi in modo zero-touch, ossia minimizzando l'intervento umano attraverso l'automazione completa della configurazione e della gestione dei servizi e dei dispositivi.
Questo approccio contribuisce significativamente alla riduzione delle spese operative, poiché riduce la necessità di interventi manuali e accelera i processi di configurazione e manutenzione.\cite{Component}.
\\Le componenti di TeraFlow sono suddivise in diversi livelli di astrazione per facilitare la gestione della rete:
\begin{itemize}
\item Device Level Abstraction: fornisce un'astrazione dei dispositivi fisici presenti, consentendo al controller di interagire con diverse tipologie di hardware.
\item Service Level Abstraction: focalizzato sulla gestione e configurazione dei servizi, questo livello gestisce le interazioni con l'SBI (Southbound Interface) per fornire un'interfaccia unificata.
\item Management Level Abstraction: realizza la gestione complessiva della rete, che include il monitoraggio, il controllo e la manutenzione dei servizi e dei dispositivi gestiti tramite SBI, garantendo un controllo centralizzato dell'infrastruttura.
\end{itemize}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{componenti.png}
    \caption{Interazione tra alcune componenti in TeraFlow}
    \label{fig:componenti}
\end{figure}



\subsection{Device Level Abstraction}
Il Device Level Abstraction permette l'interazione con i dispositivi presenti all'interno della rete e corrisponde all'Infrastructure layer descritto nell'architettura SDN.
Una componente fondamentale per questo livello di astrazione è la componente SBI.
Per far comunicare più tipi di device possibili con il controller, la componente presenta eterogeneità offrendo supporto a differenti protocolli.
Il suo compito principale è stabilire una connessione con i dispositivi per integrarli nell'ecosistema del controller
e essere in grado di configurarli dinamicamente a tempo di esecuzione \cite{D32}. 
Dopo aver stabilito la connessione con un determinato dispositivo e aver verificato la disponibilità,
la componente offre una API che consente di inviare le configurazioni scelte dal controller al dispositivo sotto forma di file JSON.
In questo modo, il dispositivo riceve istruzioni per configurare i suoi parametri operativi in linea con le politiche definite a livello di rete. 
%Il controller provvederà a inserire le giuste entry nelle tabelle degli switch o router pertinenti.
%Per questo viene introdotta la componente di monitoring che servirà da supporto per il Management-level Abstraction.
%che interagisce con la il network equipment attraverso pluggable drivers.
%OpenFlowOpenConfig-based routers
\\Un'altra componente che fa parte di questo livello è quella di Monitoring il cui compito è offrire supporto al Management-level.
E' essenziale per l'automazione dei servizi e per prendere le decisioni in tempo reale sulla base di eventi.
Questa componente interagisce con i dispositivi per catturare lo stato della rete attraverso delle KPI (Key Performance Indicators) persistendo le informazioni all'interno di un database (Metrics Database). %necessari per l'esposizione e l'utilizzo da parte delle altre componenti.
Quando un valore KPI registrato supera determinate soglie predefinite, la componente di Monitoring utilizza il sistema di gestione degli eventi del controller, un'API dedicata alla gestione e distribuzione di notifiche riguardanti eventi di rete
per generare e notificare un allarme. Questo allarme viene inviato alla componente responsabile della gestione di quel servizio specifico, ossia quella che ha
originariamente richiesto il monitoraggio della specifica metrica all'interno del controller.
Questa API permette al controller di notificare in tempo reale le componenti del sistema (ad esempio, quelle di gestione o sicurezza) sugli eventi critici, consentendo interventi tempestivi e automatizzati \cite{D32}.
Ogni evento è composto da una KPI che identifica la regola a cui si riferisce, un timestamp e un KPIValue, che rappresenta il valore della metrica monitorata in tempo reale richiesta dalla KPI al momento specificato dal timestamp.
Supponendo che la componente di Monitoring stia monitorando la latenza di un collegamento di rete tramite una KPI denominata "Latency" e che la latenza massima accettabile sia 100ms,
se la latenza supera questa soglia, ad esempio 120ms, viene generato un evento così composto: 
\begin{itemize}
    \item KPI: "Latency"
    \item Timestamp: "06-09-2024 14:35:20" (la data e l'ora a cui si è verificato l'evento)
    \item KPIValue: 120 ms
\end{itemize}
Per il corretto funzionamento la componente di Monitoring deve essere in grado di recuperare le metriche da tutti i diversi dispositivi monitorati.
Questi implementano spesso protocolli diversi per notificare le KPI, per questo motivo sono stati inclusi una serie di sottomoduli che si connettono 
agli elementi monitorati utilizzando i protocolli SBI necessari. I dati estratti vengono memorizzati nel Metrics Database così da poter fornire dati dimensionali 
con serie temporali visualizzabili su Grafana \cite{grafana}.
Tuttavia quanto si tratta di gestire topologie più complesse che coinvolgono molti dispositivi è necessario un livello di astrazione superiore per specificare la connessione tra vari end-points. A tal fine si introduce il Service-level. 

\subsection{Service Level Abstraction}
Il Service Level è responsabile della creazione e dell'aggiornamento dei servizi di rete.
Questo livello di astrazione permette agli utenti di definire intenti specifici per la connessione tra gli end-points attraverso la componente di Service.
Per svolgere questo compito, la componente si avvale di diverse funzionalità offerte dalle altre parti del sistema.
In particolare, per il calcolo dei percorsi di rete, si affida alla componente PathComp (Path Computation).
Ad esempio, se un intento richiede la creazione di una connessione con una latenza minima tra due nodi specifici, 
la componente invia una richiesta a PathComp, che calcola il percorso ottimale basato su parametri come la larghezza di banda o la latenza.
Una volta ricevuta la risposta, la componente di Service utilizza uno scheduler per configurare i dispositivi di rete lungo il percorso selezionato, utilizzando le connessioni restituite dalla PathComp \cite{D32}.
Queste regole vengono poi propagate all'interfaccia Southbound (SBI) attraverso dei file JSON, consentendo una configurazione automatizzata della rete e permettendo di astrarre la complessità del livello sottostante all'utente.
%Un utente può selezionare un path scelto o chiedere al controller di determinarne uno che rispetti determinati requisiti. 
\\La componente di Service supporta diversi tipi di servizi ed è in grado di utilizzare vari protocolli per configurare i dispositivi di rete.
Implementa inoltre una Service Handler API che consente agli operatori di rete di definire i comportamenti necessari per ciascun tipo di servizio \cite{D32}.
\begin{itemize}
    \item \textbf{L2-VPN}: servizio per dispositivi OpenConfig
    \item \textbf{L3-VPN}: servizio per dispositivi emulati o OpenConfig con supporto per ACLs
    \item \textbf{Connectivity}: servizio per dispositivi TAPI
    \item \textbf{L2 service Handler}: servizio per dispositivi P4
    \item \textbf{Microwave service Handler}
\end{itemize}
%La componente descritta ha al suo interno un blocco gRPC che espone una NBI per consentire l'interazione con le altre componenti del controller.  
%Gestisce i vari servizi inviando richieste di calcolo del percorso alla PathComp e successivamente, grazie a uno scheduler, si occupa di
%configurare i dispositivi in base alle connessioni restituite. Questo permette di astrarre la complessità del livello sottostante all'utente
%in quanto la componente è in grado di tradurre l'intento in un insieme di regole che vengono propagate all'SBI attraverso dei file JSON.
Un'altra componente che fa parte di questo livello è la PathComp. Questa, come accennato in precedenza, si occupa di gestire la selezione del percorso tra gli end-points per i servizi di connettività di rete. 
Riceve richieste dalla componente di Service e, interagendo con la componente di Context, recupera le informazioni sulle topologie sottostanti al fine di creare
percorsi che soddisfino i requisiti del servizio di rete richiesto.
La PathComp rappresenta un'entità singola e specializzata dove possono essere ospitati diversi algoritmi. Questo permette che qualsiasi nuovo algoritmo utilizzato non impatti su altre componenti del controller.
Per confrontare i percorsi viene utilizzato inizialmente un algoritmo K-SP dove i k percorsi sono ordinati per numero di passi (hop), ritardo end-to-end e larghezza di banda disponibile sul link più congestionato \cite{D53}. 
%La specifica "regolare" sottolinea che l'algoritmo non tiene conto di ottimizzazioni aggiuntive, come ad esempio la minimizzazione del consumo energetico lungo il percorso, che invece sarebbe un obiettivo nell'approccio comparativo descritto nel documento citato.
%Una volta scelto il percorso viene calcolato il risultato della power path con l'approccio EAR.
%Per astrarre la complessità del livello sottostante all'utente la componente di service è in grado di tradurre l'intento in un insieme di regole che propaga all'SBI (sempre attraverso file JSON)
%cosicchè venga configurato ogni device all'interno del percorso per stabilire la connessione.
\\Il service layer permette al controller di tradurre gli intenti in regole concrete che vengono poi propagate al livello sottostante tramite l'SBI.
Tuttavia, da solo non ha la capacità di rispondere dinamicamente agli eventi che si verificano nella rete in tempo reale, come il cambiamento dello stato di un collegamento o di una risorsa di rete. %congestionamento o guasto
Per essere in grado di creare, aggiornare o cancellare i servizi di rete in base a tali eventi è necessario introdurre un ulteriore livello di astrazione che automatizzi queste operazioni: il Management Level Abstraction.

\subsection{Management Level Abstraction}
Questo livello di astrazione è stato introdotto per consentire l'interazione dinamica con la componente di Service permettendo la creazione, l'aggiornamento o l'eliminazione automatica di un servizio in risposta agli eventi provenienti dalla rete.
In altre parole, il sistema non si limita a configurare i dispositivi solo su input manuali o su richiesta esplicita, ma è in grado di adattarsi dinamicamente ai cambiamenti dello stato della rete in tempo reale. 
Questo livello consente dunque al controller di reagire rapidamente a eventi come congestioni, guasti o modifiche nella topologia, garantendo che la rete mantenga un alto livello di efficienza e affidabilità.
%\\Una delle componenti principali è la componente di Monitoring. Essa ha il compito di raccogliere metriche da diverse componenti
%monitorate attraverso moduli che si interfacciano con i relativi protocolli.
%I dati estratti vengono memorizzati in un Metrics Database che supporta meccanismi di aggregazione per analizzare le informazioni e consente di monitorare l'evoluzione dei dati nel tempo.
%Quando una KPI supera una soglia definita precedentemente, il sistema genera un allarme.
%Questo allarme viene inviato alla componente responsabile della gestione di quel servizio specifico, ossia alla componente che ha originariamente richiesto il monitoraggio della specifica metrica all'interno del controller.
\\Una delle componenti fondamentali è la componente di Policy, che interagisce strettamente con la componente Monitoring descritta precedentemente.
\\Si occupa di definire condizioni di politica che possono essere applicate sia a livello di singoli dispositivi che a livello di dominio della rete.
Le politiche possono includere più regole collegate tra loro tramite condizioni logiche di AND/OR \cite{D32}, ognuna delle quali genera una KPI specifica da far monitorare alla componente di Monitoring.
\\Ogni regola è composta da una KPI che la identifica, un operatore numerico di confronto, e un KPIValue.
Quest'ultimo, in combinazione con l'operatore numerico, definisce l'intervallo di valori accettabili o non accettabili per quella specifica metrica.
\\Se le regole definite nella politica vengono soddisfatte la componente di Monitoring genera un allarme che viene inviato, in questo caso, alla componente di Policy, che reagisce attivando le azioni predefinite.
Ad esempio, se si verifica un eccesso di latenza, la componente di Policy può attivare il ricalcolo del percorso tramite la componente di Service o altre azioni correttive.
%Tutte queste regole vengono archiviate nel Metrics Database che fornisce anche l'evoluzione dei dati nel tempo.
%Un'ulteriore componente di questo livello è quella di Monitoring che quando rileva che una condizione impostata in una politica viene soddisfatta (ad esempio, un valore di latenza supera una soglia prestabilita), essa genera un allarme.
%Questo allarme viene inviato alla componente responsabile della gestione di quel servizio o flusso specifico all'interno del controller, che è in grado di eseguire l'azione predefinita specificata nella politica.
%Ad esempio, nel caso di un eccesso di latenza, l'allarme viene inviato alla componente di Service, la quale può poi attivare il ricalcolo del percorso o altre azioni correttive.
\\Questo meccanismo consente al controller di reagire in modo efficace agli eventi e di ripristinare uno stato desiderato per i dispositivi.
La componente Policy utilizza il Context Database per identificare quali dispositivi o servizi sono coinvolti nella politica.
Se viene fornito l'ID del servizio, la componente recupera i dispositivi associati a quel servizio, in caso contrario la componente scansiona una lista di dispositivi per individuare quelli che devono rispettare le regole impostate. 
\\Una regola di politica può avere vari stati:
\begin{itemize}
    \item \textbf{inserted} (inserita)
    \item \textbf{validated} (convalidata)
    \item \textbf{provisioned} (provvista)
    \item \textbf{actively enforced} (attivamente applicata)
    \item \textbf{failed} (fallita)
    \item \textbf{updated} (aggiornata)
    \item \textbf{removed} (rimossa)
\end{itemize}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{diagramma policy.png}
    \caption{Macchina a stati interna alla componente di policy \cite{D32}}
    \label{fig:diagramma}
\end{figure}
%La componente di policy utilizza inoltre il paradigma publish-subscribe per associare dinamicamente condizioni di politica agli eventi rilevati
%che richiedono azioni immediate.
L'integrazione della componente di Policy con quella di Monitoring ha reso possibile l'associazione delle condizioni di politica con gli allarmi del sistema di Monitoring.
%Ad esempio, una politica potrebbe stabilire che, se la latenza di rete supera i 50 ms per più di 10 minuti, il sistema deve ricalcolare il percorso per ridurre la congestione. 
%In questo caso, la componente Monitoring rileva l’aumento della latenza, genera un allarme e lo invia alla componente Policy. Quest'ultima, verificando che le condizioni della regola 
%siano soddisfatte, attiva la componente Service per ricalcolare automaticamente il percorso e riconfigurare i dispositivi coinvolti.

%Si occupa di creare un service level agreement (SLA) per uno specifico servizio identificato tramite un id.
%A questo punto verrà creato un evento e si eseguiranno delle azioni prestabilite dal controller fino ai dispositivi per gestire i problemi o cambiare il servizio. 
%Solitamente le azioni contemplano l'interazione con altre componenti al livello inferiore.
%Un'esempio può essere una notifica alla componente di Service che si occuperà di modificare il percorso identificato precedentemente per far rispettare nuovamente i requisiti.


\section{gRPC} \label{grpc}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{grpc.png}
    \caption{Funzionamento del protocollo gRPC \cite{librogrpc}}
    \label{fig:grpc}
\end{figure}
RPC (Remote Procedure Call) era un popolare protocollo inter-processo\cite{librogrpc}.
Esso utilizza uno scambio di messaggi con le caratteristiche di basso sovraccarico, semplicità e trasparenza.
Per questi motivi è stato ampiamente usato per diverso tempo in sistemi applicativi distribuiti.
Con RPC un client può invocare da remoto una funzione di un metodo.
Un utente di RPC non può distinguere dove viene eseguita la procedura chiamata, infatti viene chiamata come una procedura locale \cite{grpcArt2}.
Tuttavia, la maggior parte delle convenzionali implementazioni (un esempio è RMI) sono complesse dato che richiedono una gestione diretta dei protocolli di trasporto, come TCP, che ostacolano l'interoperabilità e richiedono specifiche eccessive \cite{librogrpc}.
\\Google Remote Procedure Calls (gRPC\cite{grpc}) è un framework OpenSource sviluppato da Google per facilitare la comunicazione tra applicazioni distribuite. Permette di
connettere, invocare, operare e fare debug di programmi eterogenei in modo semplice.
\\gRPC è basato sul protocollo di trasporto HTTP/2 che supporta la comunicazione bidirezionale. 
Sebbene anche gRPC utilizzi TCP attraverso HTTP/2, la differenza con i tradizionali protocolli RPC risiede nelle caratteristiche avanzate di HTTP/2, che includono funzionalità come il multiplexing delle connessioni, il flusso bidirezionale 
e una migliore gestione delle prestazioni. 
\\gRPC include supporto per il bilanciamento del carico, il tracciamento, il controllo dello stato e l'autentificazione\cite{grpcArt3} \cite{grpcArt1}.
Inoltre, ha la capacità di supportare alte performance di streaming in modalità push da parte del server, consentendo l'indipendenza basata sul set di dati e definizioni, grazie ai metodi YANG,
e la compressione di dati binari \cite{grpcArt3}.
\\Consente di definire i servizi, i loro metodi di comunicazione e trasportare messaggi attraverso dei file di descrizione dell'interfaccia detti Protocol Buffer, o più semplicemente file proto.
\\I file proto sono un meccanismo indipendente dal linguaggio e dalla piattaforma per la serializzazione delle strutture dati. Questo sistema di serializzazione è più efficiente rispetto a formati come JSON o XML
in termini sia di dimensione dei messaggi che di velocità di serializzazione e deserializzazione.
\\Per sviluppare un'applicazione gRPC è necessario definire un'interfaccia dei servizi. Questa contiene informazioni su come il servizio deve essere usato e quali sono i metodi, i parametri e 
il formato da utilizzare per i messaggi. Essa viene definita in un file proto, che a prima vista può sembrare un file di testo ordinario come si può notare in figura \ref{fig:proto}, ma in realtà specifica i metodi con i parametri di input e i valori di ritorno.
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{file proto.png}
    \caption{Esempio di Protocol Buffer file in TeraFlow \cite{ProtoBuf}}
    \label{fig:proto}
\end{figure}
Tutti i metodi nella definizione di questa interfaccia possono essere invocati dal client da remoto.
Una volta definita si può generare il codice lato server (server skeleton) e il codice lato client (client stub) nel linguaggio desiderato usando il compilatore Protobuf \textit{protoc}\cite{librogrpc}.
\\Negli esperimenti svolti in \cite{espgrpc} si è dimostrato che gRPC, grazie ai file proto, in scenari che coinvolgono ambienti più complessi con molte componenti (come server, DNS, firewall..)
può portare a una riduzione di quasi il 27\% nel tempo di creazione del server, rispetto a REST (Representational state transfer), uno tra i più moderni e utilizzati sistemi di trasmissione dati, riducendo il tempo di esecuzione complessivo.
Ciò che è stato provato rende gRPC più adatto per architetture composte da microservizi, come TeraFlow, in cui le prestazioni e la scalabilità del sistema sono elementi critici.
\\Quando il client invoca un servizio gRPC, il lato client utilizza i protocol buffer per serializzare la chiamata di procedura remota nel formato appropriato. 
Successivamente, la richiesta viene mandata tramite HTTP/2. Sul lato server, viene deserializzata e viene invocata la relativa procedura usando i protocol buffers.
La risposta segue il flusso inverso da server a client\cite{librogrpc}.
\\Il framework gRPC sottostante gestisce tutta le complessità che normalmente sono associate all'imposizione di vincoli di servizio, serializzazione dati, comunicazioni di rete, autentificazione e molto altro.
gRPC è progettato per trasportare messaggi peer-to-peer in modo distribuito e non durevole, consentendo a più servizi di scambiarsi informazioni attraverso un bus condiviso.
Grazie all'uso di HTTP/2 e alla codifica orientata ai byte, gRPC riesce a introdurre bassa latenza, rendendolo adatto a sistemi altamente distribuiti e scalabili. 
Un'altra caratteristica importante è la sicurezza, infatti supporta nativamente il protocollo TLS (Transport Layer Security), garantendo che le comunicazioni tra i servizi siano cifrate e sicure.


\section{P4}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{p4.png}
    \caption{Workflow di P4 sul piano dati \cite{p4Article}}
    \label{fig:p4}
\end{figure}
P4 \cite{p4}, che sta per "Programming Protocol-indipendent Packet Processor",  è un linguaggio di programmazione ad alto livello che consente di descrivere il comportamento di un elemento di rete e del piano dati in modo flessibile.
Il linguaggio permette di personalizzare il modo in cui i dispositivi di rete elaborano i pacchetti, senza essere vincolati a un set predefinito di protocolli.
\\P4 è stato introdotto per l'esigenza di superare le limitazioni con cui si stava interfacciando OpenFlow.
Per quest'ultimo protocollo l'hardware e il software non erano più sincronizzati. In alcuni casi 
gli switch non riuscivano a supportare tutte le funzionalità che OpenFlow poteva offrire a causa delle limitazioni hardware in quanto progettati per supportare un insieme fisso di protocolli e funzionalità. 
Per introdurre un cambiamento dell'hardware sono richiesti vari sforzi per lo sviluppo e il finanziamento, inoltre richiederebbe mesi se non anni per essere portato a termine.
\\P4 è nato quindi con l'obiettivo di definire nuove astrazioni per programmare il piano dati senza dover andare incontro alle limitazioni riscontrate.
Offre un linguaggio di programmazione che permette di definire regole per cui i pacchetti vengono processati direttamente nei dispositivi di rete.
Di seguito vengono indicati i vari passi che si eseguono durante un flusso di lavoro di un programma P4 che si possono ritrovare anche nell'immagine \ref{fig:p4}.
%Permette di definire la tabella, le azioni e the counters per poi essere applicate a elementi hardware o software switch allo stesso modo.
%P4 ha un interfaccia per analizzare pacchetti e match campi nell'header. In questo modo abbiamo un accoppiamento tra hardware e software.
\\Un programma P4 è costituito da diverse sezioni \cite{p4Article}, ognuna delle quali descrive un aspetto specifico del trattamento dei pacchetti.
La dichiarazione degli header permette di specificare i protocolli che si vogliono analizzare e riconoscere o inventarne di nuovi per scopi di ricerca o esperimenti.
Questo include campi come indirizzi IP, numeri di porta e tutti i dati di protocollo.
Il Parser specifica come estrarre e interpretare i vari header dai pacchetti. Esso definisce uno stato macchina che determina, attraverso le informazioni in arrivo, come passare da uno stato all'altro. 
\\La Pipeline di Elaborazione comprende tabelle e azioni che definiscono come i pacchetti vengono processati dopo aver effettuato il parsing.
%guarda se mettere separato tabelle e azioni
I Controlli del flusso coordinano il parser, le tabelle di elaborazione e le azioni. Definiscono quindi i controlli necessari per il flusso corretto di un pacchetto.
\\Dopo aver definito il programma esso viene mandato al compilatore P4 che genera due tipi di output. 
Il primo è un file binario P4 ed è ciò che viene installato all'interno del dispositivo target.
Questo file binario è specifico per un target e quindi per uno specifico hardware (ad esempio Asics, fpga..).
Il secondo output è indipendente dal target ed è diretto verso la parte NorthBound del controller. Questo è chiamato P4 info e genera i metadati necessari per consentire al piano di controllo e al piano
dati di comunicare attraverso P4 Runtime.
\\P4Runtime è un API messa a disposizione da P4 che permette al controller di connettersi ai dispositivi, vedere cosa c'è attualmente nel pipeline 
e poter mandare le configurazioni rilevanti nella relativa tabella. Permette inoltre di definire il piano dati in modo dinamico collegandolo al piano di controllo. 
Per il piano di controllo, P4Runtime protegge i dettagli hardware del piano dati ed è indipendente dalle funzionalità e dal protocollo supportati.
P4Runtime riesce quindi a raggiunge l'indipendenza dal target, dal pipeline e dal protocollo.
\\I nodi programmabili che possono essere ottenuti tramite software o hardware sono definiti P4 target.
Essi hanno una pipeline di elaborazione dei pacchetti la cui struttura è specifica per il target ed è descritta da un determinato modello di architettura.

%E' lo schema del pipeline descritto al controller in modo da nascondere il tipo di dispositivi presenti nella rete.
I principali obiettivi quindi includono:
\begin{itemize}
    \item \textbf{Protocollo-indipendenza}: P4 non è vincolato a nessun protocollo specifico consentendo addirittura di definire nuovi protocolli o modificare quelli esistenti
    \item \textbf{Target-indipendenza}: Il codice può essere compilato per una varietà di target sia hardware che software
    \item \textbf{Riprogrammabilità}: permette di aggiornare e modificare il comportamento del dataplane in tempo reale rispondendo ai cambiamenti nei requisiti di rete.
\end{itemize}
P4 rappresenta un passo significativo verso reti più flessibili e programmabili, consentendo agli sviluppatori di adattare e innovare rapidamente in risposta ai cambiamenti nei requisiti di rete.
Si propone come una soluzione innovativa e versatile per superare le limitazioni degli attuali protocolli e dispositivi di rete offrendo un linguaggio dinamico e indipendente dall'hardware.

\section{Mininet}
\label{ch:Mininet}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{mininet.png}
    \caption{Rete mininet}
    %https://telcomaglobal.com/p/what-is-mininet  fonte foto mininet
    \label{fig:mininet}
\end{figure}
Mininet \cite{mininet} è un sistema open source di orchestrazione per l'emulazione di reti su un unico ambiente Linux che permette di emulare un'intera rete su un singolo computer.
E' ampiamente utilizzato in ambiti di ricerca e sviluppo per creare e testare reti virtuali in modo realistico.
A differenza di altri emulatori che utilizzano macchine virtuali per ogni dispositivo, si distingue per la sua capacità di avviare rapidamente reti virtuali complesse e di eseguire test su scenari vari e personalizzati.
Consente anche di configurare l'inoltro dei pacchetti per testare diverse funzionalità, facilitando la condivisione e la replica del codice.
%Rispetto ad altri emulatori presenti in circolazione che emulano ogni dispositivo su una macchina virtuale, Mininet offre una serie di vantaggi. 
%Inanzitutto, permette l'avviamento rapido di una rete, la capacità di eseguire test e programmi con tipologie ampie e personalizzate. 
%Inoltre consente di personalizzare l'inoltro dei pacchetti per testare diverse funzionalità con la possibilità di condividere e replicare il codice.
\\Mininet offre delle API e un interprete Python che consentono di definire e gestire facilmente delle topologie di rete.
%Ciò è possibile anche tramite interfaccia a riga di comando (CLI). In entambi i casi possono essere sia predefinite che personalizzate con la possibilità di 
%aggiungere e rimuovere switch, router, host, controller e link, tutti eseguiti su un unico computer.
%Mininet utilizza infatti una virtualizzazione leggera per creare nodi di rete ognuno con la propria pila di rete in modo tale che possano comunicare con gli 
%altri nodi come farebbero in una rete fisica. 
È inoltre possibile utilizzare un'interfaccia a riga di comando (CLI) per la stessa funzione.
In entrambi i casi, è possibile configurare topologie predefinite o personalizzate, aggiungendo e rimuovendo switch, router, host, controller e link, tutti eseguiti su un unico computer.
\\Mininet è in grado di gestire un insieme di terminali di rete (host), 
%switch, router e collegamenti all'interno di un singolo ambiente Linux,
utilizzando la virtualizzazione leggera attraverso tecnologie implementate nel kernel Linux, come i network namespaces.
Questi permettono di creare istanze separate di interfacce di rete, tabelle di routing e tabelle ARP, che operano in modo indipendente \cite{tesiMininet}. 
Questo approccio consente di avviare numerosi host e switch (fino a 4096) su un singolo kernel del sistema operativo, simulando una rete completa su un'unica macchina \cite{MininetOv}.
Ciò consente di testare nuove applicazioni, protocolli e algoritmi in un ambiente controllato e modificabile prima di implementarli su reti reali.
\\Mininet mette a disposizione tre livelli differenti di API \cite{introMin}:
\begin{itemize}
\item \textbf{Low-level}: consiste nelle classi dei nodi e dei link istanziati individualmente e usati per creare una rete.
\item \textbf{Mid-level}: aggiunge un containter per nodi e link, l'oggetto Mininet, e fornisce metodi per la configurazione di rete.
\item \textbf{High-level}: aggiunge l'astrazione della topologia di rete, la classe Topo. Offre la possibilità di creare modelli di topologia riusabili passandoli al comando mn da linea di comando.
\end{itemize}
Si possono configurare i link come up o down e inserire metriche specifiche 
come quelle di banda, ritardo, perdita o massima lunghezza della coda di recezione per rendere la rete più realistica e adatta a esperimenti di test.
\\Gli host su Mininet condividono il filesystem root del server sottostante. 
Ciò significa che non è necessario trasferire file tra gli host virtuali perché tutti accedono agli stessi file direttamente.
Tuttavia, questa condivisione del filesystem può creare problemi se un programma ha bisogno di file di configurazione specifici per ogni host. 
In tal caso, è necessario creare un file di configurazione separato per ogni host e specificare quale file utilizzare quando si avvia il programma.
Un'altra limitazione riguarda la condivisione delle risorse del sistema su cui è in esecuzione che dovranno essere bilanciate tra tutti gli host della rete.
%Inoltre ci possono essere collisioni tra file se si prova a creare lo stesso file nella stessa directory di più hosts.
%Mininet mette a disposizione una GUI (miniedit) utile per visualizzare lo stato della rete durante gli esperimenti svolti.
\\Mininet è stato progettato per essere facilmente integrabile con altri software e sistemi di rete.
Consente anche di connettere un controller SDN remoto, quindi esterno alla rete, agli switch, indipendentemente dal PC su cui è installato, in modo da fornire un ambiente adatto allo sviluppo e al test.


\subsection{Alcuni comandi fondamentali}
\textbf{Linea di comando}
\\Inanzitutto è fondamentale creare una topologia di rete con il seguente comando\cite{walkmin}:
\\\textit{\$ sudo mn}
\\Di default viene inizializzata la topologia minimale (--topo=minimal) che consiste in uno switch connesso a due host e un controller OpenFlow.
All'interno di Mininet si possono trovare altre topologie disponibili e visualizzabili con il comando \\\textit{\$sudo mn -h} \\che si possono specificare tramite l'opzione $--topo$.
%scrivi le varie topologie
%--topo single, 3 uno switch con 3 host
\\Per avviare la topologia esistono diverse opzioni da poter applicare.Ad esempio, l'opzione $--controller$ seguito dall'indirizzo IP specifica il controller al quale gli switch dovranno collegarsi al posto 
del predefinito offerto da Mininet.
%riguarda
\\Una volta creata la topologia per avere informazioni su di essa esistono diversi comandi:
%mettere output/immagini
\begin{itemize}
    \item \textit{ nodes}: per visualizzare i nodi presenti.
    \item \textit{ net}: per visualizzare i nodi e i link presenti.
    \item \textit{ dump}: per visualizzare tutte le informazioni di dump dei nodi.
    \item \textit{h1 ifconfig}: per visualizzare le interfacce del nodo h1.
\end{itemize}
Alcuni comandi per interagire con la rete e fare dei test minimali sono:
\begin{itemize}
    \item \textit{ h1 ping -c 1 h2 }: verifica il corretto funzionamento del percorso tra h1 e h2.
    \item \textit{ pingall}: esegue il ping tra tutti gli host connessi alla rete.
    \item \textit{ iperf}: esegue un test di banda tra 2 degli host della rete.
    \item \textit{xterm h1}: permette di avviare il terminale relativo al nodo h1.
    \item \textit{exit}: esce dalla rete.
\end{itemize}
Per manipolare le metriche relative ai link invece vengono messi a disposizione i seguenti comandi:
\begin{itemize}
    \item \textit{ link s1 h1 down}: disabilita un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{ link s1 h1 up}: attiva un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem loss 50\% }: aggiunge una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem delay 200ms}: aggiunge un ritardo di 200ms sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem loss 50\% }: elimina una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem delay 200ms}: elimina un ritardo di 200ms sulla porta eth2 dello switch s2.
\end{itemize} 
\textbf{API Python}
\\Le API Python di Mininet permettono di creare e gestire topologie di rete in modo più flessibile e programmabile. 
Di seguito esponiamo alcune classi e comandi della Mid-level API:
\begin{itemize}
    \item \textit{Mininet}: classe per creare e gestire la rete. Il costruttore prende in input diversi parametri la topologia, gli host, gli switch,i controller, i link e ritorna un oggetto di rete.
    \item \textit{addSwitch()}: aggiunge uno switch alla topologia.
    \item \textit{addHost()}: aggiunge un host alla topologia.
    \item \textit{addLink()}: aggiunge un link alla topologia. Si possono specificare paramentri come la banda espressa in Mbit (bw=10 ), il ritardo (delay='5ms'), massima dimensione della coda espressa in numero di pacchetti (max\_queue\_size=1000), la loss espressa in percentuale (loss=10)
    \item \textit{start}: avvia la rete
    \item \textit{stop}: esce dalla rete
    \item \textit{pingall}: esegue il ping tra tutti gli host connessi alla rete
    \item \textit{h1.cmd('comando da eseguire')}: esegue un comando su h1 da CLI e prende l'output
\end{itemize}
Con le API in Python si può anche estendere il comando \textit{mn} usando l'opzione \textit{--custom} per invocare la topologia ricreata nello script.
\\\textit{sudo mn --your\_script.py --topo your\_topo}




