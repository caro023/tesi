\chapter{Gestione degli intenti in Teraflow}
\label{cap:teraflow}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{architetturatf.png}
    \caption{Architettura di TeraFlow \cite{archtfs}}
    \label{fig:tfs}
\end{figure}
Il controller SDN su cui ci focalizzeremo è TeraFlow \cite{TeraFlow}. 
\\TeraFlow è stato finanziato dall'unione europea nell'ambito del programma di ricerca e innovazione Horizon 2020\cite{Horizon} e supportato dal 5G PPP\cite{5GPPP}, un'iniziativa congiunta tra la Commissione Europea e l'industria europea delle telecomunicazioni.
%Nonostante la quantità di controller SDN i fondatori di TeraFlow hanno riscontrato che molte delle soluzioni attuali sono costituite da un software monolitoco
%in grado di sincronizzarsi con altri controller SDN distribuiti tramite protocolli specifici.
%Il problema principale dei controller SDN `e il basso numero di contribuzioni che negli ul-timi anni o mesi si stanno sempre di pi`u diradando
Nonostante la quantità di controller SDN i fondatori di TeraFlow hanno riscontrato come problema comune il declino delle contribuzioni negli ultimi anni. 
Di conseguenza, i nuovi bisogni e requisiti delle reti moderne che si stanno sviluppando in questo momento non saranno più supportati.
\\L'obiettivo di TeraFlow è implementare un controller che soddisfi i requisiti attuali ed eventualmente futuri, sia architetturali che infrastrutturali, per le reti.
Il controller mira a migliorare le capacità di elaborazione dei flussi consentendo la gestione di oltre un Tera di servizi di connettività, questo rappresenta una capacità cruciale per supportare le esigenze delle reti B5G. 
Per raggiungere questo scopo è necessaria una contribuzione attiva 
%(active develop) 
da parte degli utenti. A tal fine TeraFlow è un progetto OpenSource 
al quale tutti i membri della comunità ETSI (European Telecommunications Standard Institute) \cite{etsi} possono contribuire. 
\\Un ulteriore proposito è di riuscire a diminuire il gap tra ciò che le industrie richiedono e ciò che si può ricavare dagli standard SDN.
Questo controller, che è ancora in fase di sviluppo, sarà in grado di integrarsi con gli attuali framework NFV e MEC e fornire l'integrazione delle 
apparecchiature di rete ottica e a microonde. E' inoltre in grado di interfacciarsi con altri controller come ONOS, ma anche altre istanze di TeraFlow che gestiscono diversi domini, per poter sfruttare funzionalità non ancora implementate e comunicare con le altre reti.
\\A differenza dei controller presentati precedentemente con un'architettura monolitica, TeraFlow ha un'architettura nativa cloud basata su container, tecniche di virtualizzazione leggera che consentono di isolare le componenti utilizzando una minore quantità di risorse ripetto alle VM (Virtual Machine).
L'ambiente creato da un contenitore integra sia il codice di esecuzione del microservizio che le sue dipendenze. 
Questi container sono sviluppati sopra Kubernetes, un orchestratore responsabile dell'allocazione delle risorse in termini di capacità di calcolo, memoria e archiviazione\cite{arttfs}.
Kubernetes presenta diverse funzionalità che garantiscono al controller dinamicità, autoriparazione, integrità e bilanciamento del carico \cite{D53} \cite{D14}.
L'ambiente è sviluppato presso la sede del CTTC a Barcellona. 
\\Ogni container, o componente, ha delle responsabilità specifiche e definisce un microservizio che interagisce con gli altri attraverso la connessione di rete rendendo il controller disaggregato.
Le componenti principali sono implementate in Java (solo quelle di Automation e Policy) e Python.
\\Per ottimizzare l'accesso concorrente e gestire la configurazione di rete, TeraFlow utilizza una componente chiamata Context. Questa componente memorizza la configurazione di rete, inclusi topologie, dispositivi, 
collegamenti e servizi, in un database No-SQL\cite{D31}. La componente garantisce la coerenza dei dati gestiti dai vari componenti del controller SDN.
\\La componente di Monitoring gestisce invece le diverse metriche configurate per le apparecchiature e i servizi di rete memorizzando i dati di monitoraggio relativi alle Key Performance Indicators (KPI) selezionate.
I servizi sono semplici e dettagliati rendendo possibile l'uso di protocolli leggeri.
Dal punto di vista della sicurezza utilizza un sistema di Machine Learning per la prevenzione e la mitigazione di attacchi.
\\Altri elementi fondamentali del controller sono rappresentati da\cite{D22}:
\begin{itemize} 
    \item \textbf{usabilità}: realizzata grazie l'interfaccia utente webui che consente la configurazione di servizi predefiniti e la possibilità di visualizzare le metriche in modo personalizzabile in base alle esigenze.
    \item \textbf{scalabilità}: è intrinseca nel design del controller con la replicazione automatica di miscroservizi per gestire elevati volumi di richieste in ingresso.
    \item \textbf{affidabilità}: garantita attraverso robusti meccanismi di monitoraggio che supervisionano lo stato dei miscroservizi e dei flussi attivando automaticamente dei processi di ripristino se necessari.
\end{itemize}
TeraFlow si propone di affrontare le sfide delle reti moderne attraverso un'architettura innovativa e la collaborazione attiva della comunità. 
Il suo focus è sviluppare un controller SDN Carrier Grade (reti o infrastrutture ben testate con livelli estremamente elevati di affidabilità, ridondanza e sicurezza) per le reti B5G che automatizzi la gestione della rete e sia in grado di scalare per gestire miliardi di dispositivi.
\\Inoltre, TeraFlow mira a ottimizzare l'uso delle risorse di rete per migliorare l'efficienza energetica e ridurre i costi operativi.
\\Questo progetto riveste un ruolo chiave nel panorama delle tecnologie 5G, contribuendo a unire diverse università e istituti di ricerca per sviluppare soluzioni all'avanguardia
lavorando con organismi di standardizzazione per garantire l'adozione su scala globale.
%L'architettura è basata su microservizi interconnessi da un bus grpc.
\section{Componenti}
Le componenti TeraFlow sono classificate in due categorie; le componenti principali del sistema operativo e le netapp sovrapposte \cite{Component}. 
Implementano tutte delle interfacce di comunicazione all'avanguardia indipendenti dal linguaggio di programmazione 
per formare un potente sistema operativo di rete (Network Operating Sistem 'NOS') che fornisce servizi di connettività sulle infrastrutture di rete programmabili più avanzate tra cui P4, OpenConfig, TAPI...
\\TeraFlow utilizza un bus gRPC (Google Remote Procedure Call) come protocollo interno per la 
comunicazione tra di esse. Questo protocollo è stato preferito a REST per la sua capacità più concreta di descrivere le interazioni tra entità in modo efficiente e preciso.
gRPC sfrutta i Protocol Buffers per definire in modo preciso gli schemi dei messaggi condivisi e scambiati tra le componenti,  consentendo ai vari servizi di implementare le adeguate funzioni di comunicazione indipendentemente dalla piattaforma e dal linguaggio.
Ogni componente di TeraFlow implementa i propri Protocol Buffers. Questi si possono trovare nella pagina dedicata del sito \cite{ProtoBuf}.
\\TeraFlow fornisce, inoltre, funzionalità di automazione SDN avanzate basate su policy e parti interessate. Nello specifico, TeraFlow Automation sfrutta 
importanti eventi di sistema per realizzare la (ri)configurazione di servizi e dispositivi in modo zero-touch, contribuendo così notevolmente alla riduzione delle spese operative per gli operatori di rete\cite{Component}.

\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{componenti.png}
    \caption{Interazione tra alcune componenti in TeraFlow}
    \label{fig:componenti}
\end{figure}



\subsection{Device Level Abstraction}
Il Device Level Abstraction permette l'interazione con i dispositivi presenti all'interno della rete e corrisponde all'Infrastructure layer descritto nell'architettura SDN \ref{ch:SDN}.
Una componente fondamentale per questo livello di astrazione è la componente SBI.
Per far comunicare più tipi di device possibili con il controller la componente presenta eterogeneità offrendo supporto a differenti protocolli.
Il suo compito principale è stabilire una connessione con i dispositivi per integrarli nell'ecosistema del controller
e essere in grado di configurarli dinamicamente a tempo di esecuzione. 
Dopo aver stabilito la connessione con un determinato dispositivo e aver verificato la disponibilità,
la componente offre una API che permette di mandare in input le configurazioni scelte dal controller TeraFlow tramite un file JSON. 
Il controller provvederà a inserire le giuste entry nelle tabelle degli switch o router pertinenti.
%Per questo viene introdotta la componente di monitoring che servirà da supporto per il Management-level Abstraction.
%che interagisce con la il network equipment attraverso pluggable drivers.
%OpenFlowOpenConfig-based routers
\\Un'altra componente che fa parte di questo livello è quella di monitoring il cui compito è offrire supporto al Management-level.
E' essenziale per l'automazione dei servizi e per prendere le decisioni in tempo reale sulla base di eventi.
Questa componente interagisce con i dispositivi per catturare lo stato della rete attraverso delle KPI (Key Performance Indicators) persistendo le informazioni all'interno di un database. %necessari per l'esposizione e l'utilizzo da parte delle altre componenti.
Quando un valore KPI registrato supera determinate soglie predefinite, la componente di Monitoring utilizza il canale API degli eventi del controller per notificare le componenti interessate.
Ogni evento è composto da una KPI che identifica la regola a cui si riferisce, un timestamp e un KPIValue, che rappresenta il valore della metrica monitorata in tempo reale richiesta dalla KPI al momento specificato dal timestamp.
\\Per il corretto funzionamento la componente di Monitoring deve essere in grado di recuperare le metriche da tutti i diversi dispositivi monitorati.
Questi implementano spesso protocolli diversi per notificare le KPI, per questo motivo sono stati inclusi una serie di sottomoduli che si connettono 
agli elementi monitorati utilizzando i protocolli SBI necessari. I dati estratti vengono memorizzati nel Metrics Database così da poter fornire dati dimensionali 
con serie temporali visualizzabili su Grafana \cite{grafana}.
Tuttavia quanto si tratta di gestire topologie più complesse che coinvolgono molti dispositivi è necessario un livello di astrazione superiore per specificare la connessione tra vari end-points. A tal fine si introduce il Service-level. 

\subsection{Service Level Abstraction}
Il Service Level è responsabile della creazione e dell'aggiornamento dei servizi di rete.
Questo livello di astrazione permette agli utenti di definire intenti specifici per la connessione tra gli end-points attraverso la componente di Service.
%Un utente può selezionare un path scelto o chiedere al controller di determinarne uno che rispetti determinati requisiti. 
La componente di Service supporta diversi tipi di servizi ed è in grado di utilizzare vari protocolli per configurare i dispositivi di rete.
Implementa inoltre una Service Handler API che consente agli operatori di rete di definire i comportamenti necessari per ciascun tipo di servizio.
\begin{itemize}
    \item \textbf{L2-VPN}: servizio per dispositivi OpenConfig
    \item \textbf{L3-VPN}: servizio per dispositivi emulati o OpenConfig con supporto per ACLs
    \item \textbf{Connectivity}: servizio per dispositivi TAPI
    \item \textbf{L2 service Handler}: servizio per dispositivi P4
    \item \textbf{Microwave service Handler}
\end{itemize}
La componente ha al suo interno un blocco gRPC che espone una NBI al resto delle componenti del controller. 
Gestisce i vari servizi inviando richieste di calcolo del percorso alla PathComp e successivamente, grazie a uno scheduler, si occupa di
configurare i dispositivi in base alle connessioni restituite. Questo permette di astrarre la complessità del livello sottostante all'utente
in quanto la componente è in grado di tradurre l'intento in un insieme di regole che vengono propagate all'SBI attraverso dei file JSON.
\\La componente PathComp (path computation) si occupa di gestire la selezione del percorso tra gli end-points per i servizi di connettività di rete. 
Riceve richieste dalla componente di Service e, interagendo con la componente di Context, recupera le informazioni sulle topologie sottostanti al fine di creare
percorsi che soddisfino i requisiti del servizio di rete richiesto.
La PathComp rappresenta un'entità singola e specializzata dove possono essere ospitati diversi algoritmi. Questo permette che qualsiasi nuovo algoritmo utilizzato non impatti su altre componenti del controller.
Per confrontare i percorsi viene utilizzato inizialmente un algoritmo regolare K-SP dove i k percorsi sono ordinati per numero di passi (hop), ritardo end-to-end e larghezza di banda disponibile sul link più congestionato. 
%Una volta scelto il percorso viene calcolato il risultato della power path con l'approccio EAR.
Per automatizzare e rendere reattivo questo processo agli eventi che si verificano nello stato della rete, è essenziale introdurre un ulteriore livello di astrazione.
%Per astrarre la complessità del livello sottostante all'utente la componente di service è in grado di tradurre l'intento in un insieme di regole che propaga all'SBI (sempre attraverso file JSON)
%cosicchè venga configurato ogni device all'interno del percorso per stabilire la connessione.

\subsection{Management Level Abstraction}
Questo livello di astrazione è stato introdotto per consentire l'interazione dinamica con la componente di Service per la creazione, l'aggiornamento o l'eliminazione di un servizio in risposta agli eventi notificati.
Una delle componenti fondamentali che ne fanno parte è la componente di Policy.
Si occupa di definire condizioni di politica che possono essere aplicate sia a livello di singoli dispositivi che a livello di dominio della rete.
Le politiche possono includere più regole collegate tra loro tramite condizioni AND/OR.
\\Ogni regola è composta da una KPI che la identifica, un operatore numerico di paragone e un KPIValue, che rappresenta, insieme all'operatore numerico, l'insieme dei valori accettabili o non per quella determinata metrica.
Quando le condizioni definite dalla politica sono verificate (ad esempio, il KPIValue identificato per la latenza supera il valore massimo indicato), la componente di Monitoring
genera degli allarmi. Questi allarmi vengono inviati al componente di origine per poi eseguire l'azione predefinita specificata nella politica (ad esempio ricalcolo del percorso).
Questo meccanismo consente al controller di reagire agli eventi e di ripristinare un set di dispositivi ad uno stato desiderato.
\\Quando nella policy è specificato l'id del servizio, la componente interroga il database di contesto per recuperare l'insieme di dispositivi attraverso i quali transita il servizio.
In caso contrario la componente itera su una lista di dispositivi per identificare quali devono rispettare la politica di input.
Una regola di politica può avere vari stati:
\begin{itemize}
    \item \textbf{inserted} (inserita)
    \item \textbf{validated} (convalidata)
    \item \textbf{provisioned} (provvista)
    \item \textbf{actively enforced} (attivamente applicata)
    \item \textbf{failed} (fallita)
\end{itemize}
%La componente di policy utilizza inoltre il paradigma publish-subscribe per associare dinamicamente condizioni di politica agli eventi rilevati
%che richiedono azioni immediate.
L'integrazione della componente di Policy con quella di Monitoring ha reso quindi possibile l'associazione delle condizioni di politica con gli allarmi del sistema di Monitoring.

%Si occupa di creare un service level agreement (SLA) per uno specifico servizio identificato tramite un id.
%A questo punto verrà creato un evento e si eseguiranno delle azioni prestabilite dal controller fino ai dispositivi per gestire i problemi o cambiare il servizio. 
%Solitamente le azioni contemplano l'interazione con altre componenti al livello inferiore.
%Un'esempio può essere una notifica alla componente di Service che si occuperà di modificare il percorso identificato precedentemente per far rispettare nuovamente i requisiti.


\section{gRPC} \label{grpc}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{grpc.png}
    \caption{Funzionamento del protocollo gRPC \cite{librogrpc}}
    \label{fig:grpc}
\end{figure}
RPC (Remote Procedure Call) era un popolare protocollo inter-processo\cite{librogrpc}.
Esso utilizza uno scambio di messaggi con le caratteristiche di basso sovraccarico, semplicità e trasparenza.
Per questi motivi è stato ampiamente usato per diverso tempo in sistemi applicativi distribuiti.
Con RPC un client può invocare da remoto una funzione di un metodo.
Un utente di RPC non può distinguere dove viene eseguita la procedura chiamata, infatti viene chiamata come una procedura locale \cite{grpcArt2}.
Tuttavia, la maggior parte delle convenzionali implementazioni sono complesse dato che sono costruite sopra protocolli di comunicazione come TCP che ostacolano l'interoperabilità e richiedono specifiche eccessive.
\\Google Remote Procedure Calls (gRPC\cite{grpc}) è un framework OpenSource sviluppato da Google per facilitare la comunicazione tra applicazioni distribuite. Permette di
connettere, invocare, operare e fare debug di programmi eterogenei in modo semplice.
\\gRPC è basato sul protocollo di trasporto HTTP/2 che supporta la comunicazione bidirezionale. 
Include supporto per il bilanciamento del carico, il tracciamento, il controllo dello stato e l'autentificazione\cite{grpcArt3} \cite{grpcArt1}.
Inoltre, ha la capacità di supportare alte performance di streaming in modalità push da parte del server, consentendo l'indipendenza basata sul set di dati e definizioni, grazie ai metodi YANG,
e la compressione di dati binari.
\\Consente di definire i servizi, i loro metodi di comunicazione e trasportare messaggi attraverso dei file di descrizione dell'interfaccia detti Protocol Buffer, o più semplicemente file proto.
\\I file proto sono un meccanismo indipendente dal linguaggio e dalla piattaforma per la serializzazione delle strutture dati. Questo sistema di serializzazione è più efficiente rispetto a formati come JSON o XML
in termini sia di dimensione dei messaggi che di velocità di serializzazione e deserializzazione.
\\Per sviluppare un'applicazione gRPC è necessario definire un'interfaccia dei servizi. Questa contiene informazioni su come il servizio deve essere usato e quali sono i metodi, i parametri e 
il formato da utilizzare per i messaggi. Essa viene definita in un file proto, che a prima vista può sembrare un file di testo ordinario, ma in realtà specifica i metodi con i parametri di input e i valori di ritorno.
Tutti i metodi nella definizione di questa interfaccia possono essere invocati dal client da remoto.
Una volta definita si può generare il codice lato server (server skeleton) e il codice lato client (client stub) nel linguaggio desiderato usando il compilatore Protobuf \textit{protoc}\cite{librogrpc}.
\\Quando il client invoca un servizio gRPC, il lato client utilizza i protocol buffer per serializzare la chiamata di procedura remota nel formato appropriato. 
Successivamente, la richiesta viene mandata tramite HTTP/2. Sul lato server, viene deserializzata e viene invocata la relativa procedura usando i protocol buffers.
La risposta segue il flusso inverso da server a client\cite{librogrpc}.
\\Il framework gRPC sottostante gestisce tutta le complessità che normalmente sono associate all'imposizione di vincoli di servizio, serializzazione dati, comunicazioni di rete, autentificazione e molto altro.
gRPC è progettato per trasportare messaggi peer-to-peer in modo distribuito e non durevole, consentendo a più servizi di scambiarsi informazioni attraverso un bus condiviso.
Grazie all'uso di HTTP/2 e alla codifica orientata ai byte, gRPC riesce a introdurre bassa latenza, rendendolo adatto a sistemi altamente distribuiti e scalabili. 
Un'altra caratteristica importante è la sicurezza, infatti supporta nativamente il protocollo TLS (Transport Layer Security), garantendo che le comunicazioni tra i servizi siano cifrate e sicure.


\section{P4}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{p4.png}
    \caption{Workflow di P4 sul piano dati \cite{p4Article}}
    \label{fig:p4}
\end{figure}
P4 \cite{p4}, che sta per "Programming Protocol-indipendent Packet Processor",  è un linguaggio di programmazione ad alto livello che consente di descrivere il comportamento di un elemento di rete e del piano dati in modo flessibile.
Il linguaggio permette di personalizzare il modo in cui i dispositivi di rete elaborano i pacchetti, senza essere vincolati a un set predefinito di protocolli.
\\P4 è stato introdotto per l'esigenza di superare le limitazioni con cui si stava interfacciando OpenFlow.
Per quest'ultimo protocollo l'hardware e il software non erano più sincronizzati. In alcuni casi 
gli switch non riuscivano a supportare tutte le funzionalità che OpenFlow poteva offrire a causa delle limitazioni hardware in quanto progettati per supportare un insieme fisso di protocolli e funzionalità. 
Per introdurre un cambiamento dell'hardware sono richiesti vari sforzi per lo sviluppo e il finanziamento, inoltre richiederebbe mesi se non anni per essere portato a termine.
\\P4 è nato quindi con l'obiettivo di definire nuove astrazioni per programmare il piano dati senza dover andare incontro alle limitazioni riscontrate.
Offre un linguaggio di programmazione che permette di definire regole per cui i pacchetti vengono processati direttamente nei dispositivi di rete.
Di seguito vengono indicati i vari passi che si eseguono durante un flusso di lavoro di un programma P4 che si possono ritrovare anche nell'immagine \ref{fig:p4}.
%Permette di definire la tabella, le azioni e the counters per poi essere applicate a elementi hardware o software switch allo stesso modo.
%P4 ha un interfaccia per analizzare pacchetti e match campi nell'header. In questo modo abbiamo un accoppiamento tra hardware e software.
\\Un programma P4 è costituito da diverse sezioni \cite{p4Article}, ognuna delle quali descrive un aspetto specifico del trattamento dei pacchetti.
La dichiarazione degli header permette di specificare i protocolli che si vogliono analizzare e riconoscere o inventarne di nuovi per scopi di ricerca o esperimenti.
Questo include campi come indirizzi IP, numeri di porta e tutti i dati di protocollo.
Il Parser specifica come estrarre e interpretare i vari header dai pacchetti. Esso definisce uno stato macchina che determina, attraverso le informazioni in arrivo, come passare da uno stato all'altro. 
\\La Pipeline di Elaborazione comprende tabelle e azioni che definiscono come i pacchetti vengono processati dopo aver effettuato il parsing.
%guarda se mettere separato tabelle e azioni
I Controlli del flusso coordinano il parser, le tabelle di elaborazione e le azioni. Definiscono quindi i controlli necessari per il flusso corretto di un pacchetto.
\\Dopo aver definito il programma esso viene mandato al compilatore P4 che genera due tipi di output. 
Il primo è un file binario P4 ed è ciò che viene installato all'interno del dispositivo target.
Questo file binario è specifico per un target e quindi per uno specifico hardware (ad esempio Asics, fpga..).
Il secondo output è indipendente dal target ed è diretto verso la parte NorthBound del controller. Questo è chiamato P4 info e genera i metadati necessari per consentire al piano di controllo e al piano
dati di comunicare attraverso P4 Runtime.
\\P4Runtime è un API messa a disposizione da P4 che permette al controller di connettersi ai dispositivi, vedere cosa c'è attualmente nel pipeline 
e poter mandare le configurazioni rilevanti nella relativa tabella. Permette inoltre di definire il piano dati in modo dinamico collegandolo al piano di controllo. 
Per il piano di controllo, P4Runtime protegge i dettagli hardware del piano dati ed è indipendente dalle funzionalità e dal protocollo supportati.
P4Runtime riesce quindi a raggiunge l'indipendenza dal target, dal pipeline e dal protocollo.
\\I nodi programmabili che possono essere ottenuti tramite software o hardware sono definiti P4 target.
Essi hanno una pipeline di elaborazione dei pacchetti la cui struttura è specifica per il target ed è descritta da un determinato modello di architettura.

%E' lo schema del pipeline descritto al controller in modo da nascondere il tipo di dispositivi presenti nella rete.
I principali obiettivi quindi includono:
\begin{itemize}
    \item \textbf{Protocollo-indipendenza}: P4 non è vincolato a nessun protocollo specifico consentendo addirittura di definire nuovi protocolli o modificare quelli esistenti
    \item \textbf{Target-indipendenza}: Il codice può essere compilato per una varietà di target sia hardware che software
    \item \textbf{Riprogrammabilità}: permette di aggiornare e modificare il comportamento del dataplane in tempo reale rispondendo ai cambiamenti nei requisiti di rete.
\end{itemize}
P4 rappresenta un passo significativo verso reti più flessibili e programmabili, consentendo agli sviluppatori di adattare e innovare rapidamente in risposta ai cambiamenti nei requisiti di rete.
Si propone come una soluzione innovativa e versatile per superare le limitazioni degli attuali protocolli e dispositivi di rete offrendo un linguaggio dinamico e indipendente dall'hardware.

\section{Mininet}
\label{ch:Mininet}
\begin{figure}[h]
    \centering
   \includegraphics[width=1\textwidth]{mininet.png}
    \caption{Rete mininet}
    %https://telcomaglobal.com/p/what-is-mininet  fonte foto mininet
    \label{fig:mininet}
\end{figure}
Mininet \cite{mininet} è un sistema open source di orchestrazione per l'emulazione di reti su un unico ambiente Linux che permette di simulare un'intera rete su un singolo computer.
E' ampiamente utilizzato per la creazione, il test e la sperimentazione di reti virtuali realistiche.
Rispetto ad altri emulatori presenti in circolazione che emulano ogni dispositivo su una macchina virtuale, Mininet offre una serie di vantaggi. 
Inanzitutto, permette l'avviamento rapido di una rete, la capacità di eseguire test e programmi con tipologie ampie e personalizzate. 
Inoltre consente di personalizzare l'inoltro dei pacchetti per testare diverse funzionalità con la possibilità di condividere e replicare il codice.
A tal proposito Mininet presenta delle API e un interprete Python che consentono di definire e gestire facilmente delle topologie di rete.
Ciò è possibile anche tramite interfaccia a riga di comando (CLI). In entambi i casi possono essere sia predefinite che personalizzate con la possibilità di 
aggiungere e rimuovere switch, router, host, controller e link, tutti eseguiti su un unico computer.
Mininet utilizza infatti dei container di virtualizzazione leggera per creare nodi di rete ognuno con la propria pila di rete in modo tale che possano comunicare con gli 
altri nodi come farebbero in una rete fisica. 
Ciò consente di testare nuove applicazioni, protocolli e algoritmi in un ambiente controllato e modificabile prima di implementarli su reti reali.
\\Mininet mette a disposizione tre livelli differenti di API \cite{introMin}:
\begin{itemize}
\item \textbf{Low-level}: consiste nelle classi dei nodi e dei link istanziati individualmente e usati per creare una rete.
\item \textbf{Mid-level}: aggiunge un containter per nodi e link, l'oggetto Mininet, e fornisce metodi per la configurazione di rete.
\item \textbf{High-level}: aggiunge l'astrazione della topologia di rete, la classe Topo. Offre la possibilità di creare modelli di topologia riusabili passandoli al comando mn da linea di comando.
\end{itemize}
Si possono configurare i link come up o down e inserire metriche specifiche 
come quelle di banda, ritardo, perdita o massima lunghezza della coda di recezione per rendere la rete più realistica e adatta a esperimenti di test.
\\Gli host su Mininet condividono il filesystem root del server sottostante. 
Ciò significa che non è necessario trasferire file tra gli host virtuali perché tutti accedono agli stessi file direttamente.
Tuttavia, questa condivisione del filesystem può creare problemi se un programma ha bisogno di file di configurazione specifici per ogni host. 
In tal caso, è necessario creare un file di configurazione separato per ogni host e specificare quale file utilizzare quando si avvia il programma.
Un'altra limitazione riguarda la condivisione delle risorse del sistema su cui è in esecuzione che dovranno essere bilanciate tra tutti gli host della rete.
%Inoltre ci possono essere collisioni tra file se si prova a creare lo stesso file nella stessa directory di più hosts.
%Mininet mette a disposizione una GUI (miniedit) utile per visualizzare lo stato della rete durante gli esperimenti svolti.
\\Mininet è stato progettato per essere facilmente integrabile con altri software e sistemi di rete.
Consente anche di connettere un controller SDN remoto, quindi esterno alla rete, agli switch, indipendentemente dal PC su cui è installato, in modo da fornire un ambiente adatto allo sviluppo e al test.


\subsection{Alcuni comandi fondamentali}
\textbf{Linea di comando}
\\Inanzitutto è fondamentale creare una topologia di rete con il seguente comando\cite{walkmin}:
\\\textit{\$ sudo mn}
\\Di default viene inizializzata la topologia minimale (--topo=minimal) che consiste in uno switch connesso a due host e un controller OpenFlow.
All'interno di Mininet si possono trovare altre topologie disponibili e visualizzabili con il comando \\\textit{\$sudo mn -h} \\che si possono specificare tramite l'opzione $--topo$.
%scrivi le varie topologie
%--topo single, 3 uno switch con 3 host
\\Per avviare la topologia esistono diverse opzioni da poter applicare.Ad esempio, l'opzione $--controller$ seguito dall'indirizzo IP specifica il controller al quale gli switch dovranno collegarsi al posto 
del predefinito offerto da Mininet.
%riguarda
\\Una volta creata la topologia per avere informazioni su di essa esistono diversi comandi:
%mettere output/immagini
\begin{itemize}
    \item \textit{ nodes}: per visualizzare i nodi presenti.
    \item \textit{ net}: per visualizzare i nodi e i link presenti.
    \item \textit{ dump}: per visualizzare tutte le informazioni di dump dei nodi.
    \item \textit{h1 ifconfig}: per visualizzare le interfacce del nodo h1.
\end{itemize}
Alcuni comandi per interagire con la rete e fare dei test minimali sono:
\begin{itemize}
    \item \textit{ h1 ping -c 1 h2 }: verifica il corretto funzionamento del percorso tra h1 e h2.
    \item \textit{ pingall}: esegue il ping tra tutti gli host connessi alla rete.
    \item \textit{ iperf}: esegue un test di banda tra 2 degli host della rete.
    \item \textit{xterm h1}: permette di avviare il terminale relativo al nodo h1.
    \item \textit{exit}: esce dalla rete.
\end{itemize}
Per manipolare le metriche relative ai link invece vengono messi a disposizione i seguenti comandi:
\begin{itemize}
    \item \textit{ link s1 h1 down}: disabilita un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{ link s1 h1 up}: attiva un link, in questo caso quello tra lo switch s1 e l'host h1.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem loss 50\% }: aggiunge una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc add dev s2-eth2 root netem delay 200ms}: aggiunge un ritardo di 200ms sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem loss 50\% }: elimina una packet loss del 50\% sulla porta eth2 dello switch s2.
    \item \textit{s2 tc qdisc del dev s2-eth2 root netem delay 200ms}: elimina un ritardo di 200ms sulla porta eth2 dello switch s2.
\end{itemize} 
\textbf{API Python}
\\Le API Python di Mininet permettono di creare e gestire topologie di rete in modo più flessibile e programmabile. 
Di seguito esponiamo alcune classi e comandi della Mid-level API:
\begin{itemize}
    \item \textit{Mininet}: classe per creare e gestire la rete. Il costruttore prende in input diversi parametri la topologia, gli host, gli switch,i controller, i link e ritorna un oggetto di rete.
    \item \textit{addSwitch()}: aggiunge uno switch alla topologia.
    \item \textit{addHost()}: aggiunge un host alla topologia.
    \item \textit{addLink()}: aggiunge un link alla topologia. Si possono specificare paramentri come la banda espressa in Mbit (bw=10 ), il ritardo (delay='5ms'), massima dimensione della coda espressa in numero di pacchetti (max\_queue\_size=1000), la loss espressa in percentuale (loss=10)
    \item \textit{start}: avvia la rete
    \item \textit{stop}: esce dalla rete
    \item \textit{pingall}: esegue il ping tra tutti gli host connessi alla rete
    \item \textit{h1.cmd('comando da eseguire')}: esegue un comando su h1 da CLI e prende l'output
\end{itemize}
Con le API in Python si può anche estendere il comando \textit{mn} usando l'opzione \textit{--custom} per invocare la topologia ricreata nello script.
\\\textit{sudo mn --your\_script.py --topo your\_topo}




